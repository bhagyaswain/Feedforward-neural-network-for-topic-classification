{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'/Users/lilbwain/Documents/NLP/assignment/data_topic/train.csv', header=None, names=['Category','Text'])\n",
    "df_test = pd.read_csv(r'/Users/lilbwain/Documents/NLP/assignment/data_topic/test.csv', header=None, names=['Category','Text'])\n",
    "df_dev = pd.read_csv(r'/Users/lilbwain/Documents/NLP/assignment/data_topic/dev.csv', header=None, names=['Category','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.753874Z",
     "start_time": "2020-04-02T14:26:39.749647Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  2400 non-null   int64 \n",
      " 1   Text      2400 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:40.851926Z",
     "start_time": "2020-04-02T14:26:40.847500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1, 3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                   stop_words=[], vocab=set()):\n",
    "    tokens = re.findall(token_pattern, x_raw.lower())  # Lowercase and tokenize\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n",
    "    n_grams = []\n",
    "    for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i:i + n])  # Join tokens to form n-gram\n",
    "            if not vocab or ngram in vocab:  # Keep specific n-grams based on vocab (optional)\n",
    "                n_grams.append(ngram)\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1, 3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words=[]):\n",
    "    # Initialize Counter objects to store document frequencies and raw frequencies of n-grams\n",
    "    df_counter = Counter()\n",
    "    ngram_counter = Counter()\n",
    "    \n",
    "    # Iterate over each document in X_raw\n",
    "    for doc in X_raw:\n",
    "        # Extract n-grams from the current document using the specified token pattern\n",
    "        ngrams = extract_ngrams(doc, ngram_range=ngram_range, token_pattern=token_pattern, stop_words=stop_words)\n",
    "        # Count document frequencies of n-grams\n",
    "        df_counter.update(set(ngrams))\n",
    "        # Count raw frequencies of n-grams\n",
    "        ngram_counter.update(ngrams)\n",
    "    \n",
    "    # Filter n-grams based on min_df\n",
    "    if min_df > 0:\n",
    "        df_counter = Counter({ngram: freq for ngram, freq in df_counter.items() if freq >= min_df})\n",
    "    \n",
    "    # Filter n-grams based on keep_topN using unfiltered ngrams from ngram_counter\n",
    "    if keep_topN > 0:\n",
    "        top_ngrams = [ngram for ngram, _ in ngram_counter.most_common(keep_topN)]\n",
    "        df_counter = Counter({ngram: freq for ngram, freq in df_counter.items() if ngram in top_ngrams})\n",
    "    \n",
    "    # Create vocab of n-grams\n",
    "    vocab = set(df_counter.keys())\n",
    "    \n",
    "    # Return vocab, document frequencies, and raw frequencies\n",
    "    return vocab, df_counter, ngram_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 5000\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary, document frequencies (DF), and raw frequencies for unigrams (n-gram_range=(1, 1))\n",
    "vocab, df_unigrams, ngram_counts_unigrams = get_vocab(df_train['Text'].tolist(), ngram_range=(1, 1), stop_words=stop_words\n",
    "                                                     ,keep_topN=5000)\n",
    "\n",
    "print(\"Vocabulary Size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reuters', 694),\n",
       " ('said', 440),\n",
       " ('tuesday', 415),\n",
       " ('new', 365),\n",
       " ('wednesday', 346),\n",
       " ('after', 304),\n",
       " ('athens', 293),\n",
       " ('ap', 276),\n",
       " ('monday', 221),\n",
       " ('first', 219),\n",
       " ('us', 201),\n",
       " ('olympic', 196),\n",
       " ('york', 194),\n",
       " ('two', 192),\n",
       " ('over', 190),\n",
       " ('oil', 179),\n",
       " ('inc', 176),\n",
       " ('more', 166),\n",
       " ('prices', 163),\n",
       " ('year', 159)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counts_unigrams.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> vocabulary id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2id = {word: i for i, word in enumerate(vocab)}  # Map words to their indices (word -> id)\n",
    "id2word = [word for word in vocab]  # List of words in vocabulary order (id -> word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_document(doc):\n",
    "    \"\"\"\n",
    "    Tokenize documents into lists of words.\n",
    "\n",
    "    Args:\n",
    "    - doc (list of str): List of documents.\n",
    "\n",
    "    Returns:\n",
    "    - toklist (list of lists): List of lists of words for each document.\n",
    "    \"\"\"\n",
    "    toklist = []\n",
    "    for text in doc:\n",
    "        tokens = text.lower().split()  # Split text into words\n",
    "        toklist.append(tokens)\n",
    "    return toklist\n",
    "\n",
    "train_toklist = tokenize_document(df_train['Text'])\n",
    "dev_toklist = tokenize_document(df_dev['Text'])\n",
    "test_toklist = tokenize_document(df_test['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert them into lists of indices in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, word2id):\n",
    "   \n",
    "    indices = [word2id[word] for word in tokens if word in word2id]\n",
    "    return indices\n",
    "\n",
    "\n",
    "train_sequences = [tokens_to_indices(tokens, word2id) for tokens in train_toklist]\n",
    "dev_sequences = [tokens_to_indices(tokens, word2id) for tokens in dev_toklist]\n",
    "test_sequences = [tokens_to_indices(tokens, word2id) for tokens in test_toklist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:03:13.183996Z",
     "start_time": "2020-04-02T15:03:13.077575Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels shape: (2400,)\n",
      "Dev labels shape: (150,)\n",
      "Test labels shape: (900,)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "train_y = np.array(df_train['Category'])\n",
    "dev_y = np.array(df_dev['Category'])\n",
    "test_y = np.array(df_test['Category'])\n",
    "\n",
    "# Print shapes to verify conversion\n",
    "print(\"Train labels shape:\", train_y.shape)\n",
    "print(\"Dev labels shape:\", dev_y.shape)\n",
    "print(\"Test labels shape:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers. Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_classes`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:41:20.918617Z",
     "start_time": "2020-04-02T15:41:20.915597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300, \n",
    "                    hidden_dim=[], num_classes=3, init_val = 0.5):\n",
    "    np.random.seed(0)\n",
    "    W = {}\n",
    "    # Embedding layer weights\n",
    "    W[0] = np.random.uniform(low=-init_val, high=init_val, size=(vocab_size, embedding_dim)).astype(np.float32)\n",
    "\n",
    "  # Hidden layer weights (if any)\n",
    "    prev_dim = embedding_dim\n",
    "    for i, dim in enumerate(hidden_dim, start=1):  # Start indexing from 1 (after embedding layer)\n",
    "        W[i] = np.random.uniform(low=-init_val, high=init_val, size=(prev_dim, dim)).astype(np.float32)\n",
    "        prev_dim = dim\n",
    "\n",
    "        # Output layer weights\n",
    "    \n",
    "    W[len(hidden_dim)+1] = np.random.uniform(low=-init_val, high=init_val, size=(prev_dim, num_classes)).astype(np.float32)\n",
    "\n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = network_weights(vocab_size=3,embedding_dim=4,hidden_dim=[2], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[ 0.0488135 ,  0.21518937,  0.10276338,  0.04488318],\n",
       "        [-0.0763452 ,  0.14589411, -0.06241279,  0.39177302],\n",
       "        [ 0.46366277, -0.11655848,  0.29172504,  0.02889492]],\n",
       "       dtype=float32),\n",
       " 1: array([[ 0.06804456,  0.42559662],\n",
       "        [-0.42896393, -0.4128707 ],\n",
       "        [-0.4797816 ,  0.33261985],\n",
       "        [ 0.27815676,  0.37001213]], dtype=float32),\n",
       " 2: array([[ 0.47861835,  0.29915857],\n",
       "        [-0.03852064,  0.28052917]], dtype=float32)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. \n",
    "\n",
    "It takes as input `z` (array of real numbers) and returns `sig` (the softmax of `z`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Numerically stable implementation to avoid overflow\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    sig = exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    return sig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    # Clip predicted probabilities to avoid numerical instability (log(0))\n",
    "    #y_preds = np.clip(y_preds, 1e-15, 1 - 1e-15)\n",
    "    # Compute categorical cross-entropy loss\n",
    "    l = -np.log(y_preds[y-1] + 1e-10)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network \n",
    "(during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "  \n",
    "  relu_derivative($z_i$)=0, if $z_i$<=0, 1 otherwise.\n",
    "  \n",
    "\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    a = z.copy()  # Create a copy to avoid in-place modification\n",
    "    a[a < 0] = 0  # Set elements less than or equal to 0 to 0\n",
    "    return a\n",
    "    \n",
    "def relu_derivative(z):\n",
    "    dz = z.copy()  # Create a copy to avoid in-place modification\n",
    "    dz[dz <= 0] = 0  # Set derivative to 0 for elements less than or equal to 0\n",
    "    dz[dz > 0] = 1  # Set derivative to 1 for elements greater than 0\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    dropout_vec = np.random.binomial(1, 1-dropout_rate, size)  # Generate random numbers between 0 and 1\n",
    "   \n",
    "    return dropout_vec\n",
    "\n",
    "size=10\n",
    "dropout_rate=0.5\n",
    "dropout_vec = dropout_mask(size,dropout_rate)\n",
    "print(dropout_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    out_vals = {}\n",
    "    \n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    # Input layer\n",
    "    h = np.mean(W[0][x], axis=0)  # Embedding lookup\n",
    "    h_vecs.append(h)\n",
    "    a = relu(h)  # ReLU activation\n",
    "    a_vecs.append(a)\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i in range(1, len(W) - 1):\n",
    "        h = np.dot(a, W[i])  # Linear transformation\n",
    "        h_vecs.append(h)\n",
    "        a = relu(h)  # ReLU activation\n",
    "        dropout_vec = dropout_mask(a.shape[0], dropout_rate)  # Dropout mask\n",
    "        a *= dropout_vec  \n",
    "        a_vecs.append(a)\n",
    "        dropout_vecs.append(dropout_vec)\n",
    "    \n",
    "    # Output layer\n",
    "    h = np.dot(a, W[len(W) - 1])  # Linear transformation\n",
    "    h_vecs.append(h)\n",
    "    a = softmax(h)  # Softmax activation for output layer\n",
    "    a_vecs.append(a)\n",
    "    \n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['dropout'] = dropout_vecs\n",
    "    out_vals['pred'] = a\n",
    "    \n",
    "    return out_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and updates the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:24:13.732705Z",
     "start_time": "2020-05-11T08:24:13.729741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n",
    "    num_layers = len(W)\n",
    "    grads = {}\n",
    "    np.random.seed(0)\n",
    "    # Compute gradient from the loss function with respect to prediction\n",
    "    # For categorical cross-entropy and softmax, this simplifies to the prediction minus one-hot true label\n",
    "    pred = out_vals['pred']\n",
    "    grad_loss = pred.copy()\n",
    "    grad_loss[y-1] -= 1  # Subtract 1 from the true class probability\n",
    "\n",
    "    # Propagate the gradient backwards through the hidden layers\n",
    "    for i in range(num_layers - 1, 0, -1):\n",
    "        if i == num_layers - 1:\n",
    "            grads[i] = np.outer(out_vals['a'][i-1], grad_loss)\n",
    "            grad_next_layer = grad_loss\n",
    "        else:\n",
    "            # Compute gradient with respect to the activation (before dropout)\n",
    "            grad_next_layer = np.dot(grad_next_layer, W[i+1].T) * relu_derivative(out_vals['h'][i])\n",
    "\n",
    "            # Compute gradient with respect to the weights\n",
    "            a_prev = out_vals['a'][i-1]\n",
    "            grads[i] = np.outer(a_prev, grad_next_layer)\n",
    "\n",
    "        W[i] -= lr * grads[i]\n",
    "\n",
    "    # Optionally update embedding layer\n",
    "    if not freeze_emb:\n",
    "        # Sparse update for the embedding weights\n",
    "        grad_next_layer = np.dot(W[1], grad_next_layer)\n",
    "        for idx in x:\n",
    "            W[0][idx, :] -= lr * grad_next_layer / len(x)\n",
    "\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, \n",
    "        dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, \n",
    "        print_progress=True):\n",
    "    training_loss_history = []\n",
    "    best_dev_loss = np.inf  # Initialize with positive infinity for early stopping\n",
    "    validation_loss_history = []\n",
    "      # Training loop\n",
    "    np.random.seed(0)\n",
    "    shuffled_indices = np.random.permutation(len(X_tr))\n",
    "    X_tr = [X_tr[i] for i in shuffled_indices]\n",
    "    Y_tr = [Y_tr[i] for i in shuffled_indices]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_losses = 0\n",
    "        for x, y in zip(X_tr, Y_tr):\n",
    "            out_vals = forward_pass(x, W, dropout_rate)\n",
    "            loss = categorical_loss(y, out_vals['pred'])\n",
    "            train_losses += loss\n",
    "            W = backward_pass(x, y, W, out_vals, lr, freeze_emb)\n",
    "\n",
    "        # Calculate average training loss per epoch\n",
    "        avg_train_loss = train_losses / len(train_sequences)\n",
    "        training_loss_history.append(avg_train_loss)\n",
    "\n",
    "        # Validation pass (if development data provided)\n",
    "        dev_losses = 0\n",
    "        for x, y in zip(X_dev, Y_dev):\n",
    "            out_vals = forward_pass(x, W, dropout_rate)\n",
    "            loss = categorical_loss(y, out_vals['pred'])\n",
    "            dev_losses += loss\n",
    "            W = backward_pass(x, y, W, out_vals, lr, freeze_emb)\n",
    "            \n",
    "        avg_dev_loss = dev_losses / len(dev_sequences)\n",
    "        validation_loss_history.append(avg_dev_loss)\n",
    "\n",
    "        # Early stopping (check for significant improvement in validation loss)\n",
    "        if epoch > 1:\n",
    "            if print_progress and validation_loss_history[-2] - validation_loss_history[-1] < tolerance:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} due to validation loss stagnation.\")\n",
    "                break\n",
    "\n",
    "        # Print progress (optional)\n",
    "        if print_progress:\n",
    "            print(f\"Epoch: {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f} ({len(train_sequences)} samples)\")\n",
    "            #if dev_sequences and dev_y:\n",
    "            print(f\"Validation Loss: {avg_dev_loss:.4f} ({len(dev_sequences)} samples)\")\n",
    "\n",
    "    return W, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate your neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with learning rate: 0.001, dropout rate: 0.3, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 1.1057 (2400 samples)\n",
      "Validation Loss: 1.0955 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.1015 (2400 samples)\n",
      "Validation Loss: 1.0919 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0974 (2400 samples)\n",
      "Validation Loss: 1.0884 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0934 (2400 samples)\n",
      "Validation Loss: 1.0851 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0896 (2400 samples)\n",
      "Validation Loss: 1.0818 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0857 (2400 samples)\n",
      "Validation Loss: 1.0786 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0820 (2400 samples)\n",
      "Validation Loss: 1.0754 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0783 (2400 samples)\n",
      "Validation Loss: 1.0723 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0746 (2400 samples)\n",
      "Validation Loss: 1.0692 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0710 (2400 samples)\n",
      "Validation Loss: 1.0661 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0674 (2400 samples)\n",
      "Validation Loss: 1.0631 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0638 (2400 samples)\n",
      "Validation Loss: 1.0600 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 1.0602 (2400 samples)\n",
      "Validation Loss: 1.0570 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 1.0565 (2400 samples)\n",
      "Validation Loss: 1.0540 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 1.0529 (2400 samples)\n",
      "Validation Loss: 1.0510 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 1.0493 (2400 samples)\n",
      "Validation Loss: 1.0479 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 1.0457 (2400 samples)\n",
      "Validation Loss: 1.0448 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 1.0420 (2400 samples)\n",
      "Validation Loss: 1.0418 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 1.0383 (2400 samples)\n",
      "Validation Loss: 1.0387 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 1.0346 (2400 samples)\n",
      "Validation Loss: 1.0355 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 1.0308 (2400 samples)\n",
      "Validation Loss: 1.0324 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 1.0270 (2400 samples)\n",
      "Validation Loss: 1.0292 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 1.0232 (2400 samples)\n",
      "Validation Loss: 1.0260 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 1.0193 (2400 samples)\n",
      "Validation Loss: 1.0227 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 1.0154 (2400 samples)\n",
      "Validation Loss: 1.0194 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 1.0114 (2400 samples)\n",
      "Validation Loss: 1.0161 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 1.0074 (2400 samples)\n",
      "Validation Loss: 1.0127 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 1.0033 (2400 samples)\n",
      "Validation Loss: 1.0093 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.9992 (2400 samples)\n",
      "Validation Loss: 1.0059 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.9951 (2400 samples)\n",
      "Validation Loss: 1.0023 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.9908 (2400 samples)\n",
      "Validation Loss: 0.9988 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.9866 (2400 samples)\n",
      "Validation Loss: 0.9952 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.9823 (2400 samples)\n",
      "Validation Loss: 0.9915 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.9779 (2400 samples)\n",
      "Validation Loss: 0.9878 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.9735 (2400 samples)\n",
      "Validation Loss: 0.9840 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.9690 (2400 samples)\n",
      "Validation Loss: 0.9802 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.9644 (2400 samples)\n",
      "Validation Loss: 0.9764 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.9599 (2400 samples)\n",
      "Validation Loss: 0.9725 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.9552 (2400 samples)\n",
      "Validation Loss: 0.9685 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.9505 (2400 samples)\n",
      "Validation Loss: 0.9645 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.9458 (2400 samples)\n",
      "Validation Loss: 0.9604 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.9410 (2400 samples)\n",
      "Validation Loss: 0.9563 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.9361 (2400 samples)\n",
      "Validation Loss: 0.9522 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.9312 (2400 samples)\n",
      "Validation Loss: 0.9479 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.9263 (2400 samples)\n",
      "Validation Loss: 0.9437 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.9213 (2400 samples)\n",
      "Validation Loss: 0.9393 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.9163 (2400 samples)\n",
      "Validation Loss: 0.9350 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.9112 (2400 samples)\n",
      "Validation Loss: 0.9305 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.9061 (2400 samples)\n",
      "Validation Loss: 0.9261 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.9009 (2400 samples)\n",
      "Validation Loss: 0.9215 (150 samples)\n",
      "Training Loss: [1.1057206259438885, 1.1015068707171363, 1.0974224800865935, 1.0934451604018696, 1.08955861520457, 1.085748920926033, 1.0819990868364857, 1.0782961144207603, 1.0746287187012946, 1.0709889883886752, 1.0673675407116852, 1.0637588480126259, 1.060155449538258, 1.0565494518132164, 1.052934800945347, 1.04930681758877, 1.0456604825707274, 1.0419916693866829, 1.0382955188304646, 1.0345688704259124, 1.0308084543242038, 1.02701194785163, 1.0231758138926639, 1.0192987369127924, 1.0153791185932728, 1.0114114354410768, 1.0073954136521377, 1.0033306489914493, 0.9992169047877216, 0.9950528421502886, 0.9908379858276131, 0.986570695654248, 0.9822501548504757, 0.9778766198966082, 0.9734503927756085, 0.9689711811270623, 0.964439067132127, 0.9598511970595287, 0.9552107538318929, 0.9505157613308607, 0.9457687968724713, 0.9409692714891537, 0.9361202674438188, 0.9312242946713726, 0.9262820057817444, 0.9212935351515047, 0.9162599711699438, 0.9111834904755752, 0.90606763656734, 0.9009156657820402]\n",
      "Validation Losses: [1.0955081544838554, 1.0919199360997858, 1.0884467382803207, 1.0850736654577868, 1.0817860167111162, 1.0785646246502885, 1.075396756421921, 1.0722717691480275, 1.0691837745165398, 1.0661214891718183, 1.0630782874087905, 1.060042651497194, 1.057012102578924, 1.0539853510171218, 1.0509518437830214, 1.0479055042605594, 1.044844616292666, 1.0417664403256384, 1.0386667837245038, 1.0355414786084935, 1.032386334169755, 1.0292026003347086, 1.0259875093947846, 1.022733401618679, 1.0194389130546138, 1.0161066140352903, 1.012732896749927, 1.0093169670972781, 1.0058553516704167, 1.002348327013657, 0.9987917496674114, 0.9951829429158687, 0.9915207391345238, 0.987808078648464, 0.9840442743043015, 0.9802332009980378, 0.9763733868686575, 0.9724609205419567, 0.9685013931677122, 0.9644891397784431, 0.9604257645615378, 0.9563141603904313, 0.952150637379486, 0.9479352209463938, 0.9436662767748434, 0.9393443978739134, 0.9349689489363032, 0.9305406041287015, 0.9260647181254568, 0.9215410678586599]\n",
      "Hyperparameters: lr=0.001, dropout=0.3, embedding_dim=100\n",
      "Testing with learning rate: 0.001, dropout rate: 0.3, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 1.0980 (2400 samples)\n",
      "Validation Loss: 1.0927 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0906 (2400 samples)\n",
      "Validation Loss: 1.0866 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0836 (2400 samples)\n",
      "Validation Loss: 1.0808 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0768 (2400 samples)\n",
      "Validation Loss: 1.0753 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0703 (2400 samples)\n",
      "Validation Loss: 1.0699 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0640 (2400 samples)\n",
      "Validation Loss: 1.0647 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0577 (2400 samples)\n",
      "Validation Loss: 1.0595 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0516 (2400 samples)\n",
      "Validation Loss: 1.0545 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0456 (2400 samples)\n",
      "Validation Loss: 1.0495 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0396 (2400 samples)\n",
      "Validation Loss: 1.0445 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0336 (2400 samples)\n",
      "Validation Loss: 1.0395 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0277 (2400 samples)\n",
      "Validation Loss: 1.0346 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 1.0218 (2400 samples)\n",
      "Validation Loss: 1.0297 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 1.0159 (2400 samples)\n",
      "Validation Loss: 1.0248 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 1.0101 (2400 samples)\n",
      "Validation Loss: 1.0199 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 1.0042 (2400 samples)\n",
      "Validation Loss: 1.0150 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.9984 (2400 samples)\n",
      "Validation Loss: 1.0101 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.9925 (2400 samples)\n",
      "Validation Loss: 1.0052 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.9866 (2400 samples)\n",
      "Validation Loss: 1.0002 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.9808 (2400 samples)\n",
      "Validation Loss: 0.9953 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.9749 (2400 samples)\n",
      "Validation Loss: 0.9903 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.9690 (2400 samples)\n",
      "Validation Loss: 0.9853 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.9631 (2400 samples)\n",
      "Validation Loss: 0.9803 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.9572 (2400 samples)\n",
      "Validation Loss: 0.9752 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.9512 (2400 samples)\n",
      "Validation Loss: 0.9701 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.9453 (2400 samples)\n",
      "Validation Loss: 0.9650 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.9393 (2400 samples)\n",
      "Validation Loss: 0.9599 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.9333 (2400 samples)\n",
      "Validation Loss: 0.9547 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.9273 (2400 samples)\n",
      "Validation Loss: 0.9495 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.9212 (2400 samples)\n",
      "Validation Loss: 0.9442 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.9152 (2400 samples)\n",
      "Validation Loss: 0.9390 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.9091 (2400 samples)\n",
      "Validation Loss: 0.9337 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.9030 (2400 samples)\n",
      "Validation Loss: 0.9283 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.8969 (2400 samples)\n",
      "Validation Loss: 0.9229 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.8908 (2400 samples)\n",
      "Validation Loss: 0.9175 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.8846 (2400 samples)\n",
      "Validation Loss: 0.9121 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.8785 (2400 samples)\n",
      "Validation Loss: 0.9066 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.8724 (2400 samples)\n",
      "Validation Loss: 0.9012 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.8662 (2400 samples)\n",
      "Validation Loss: 0.8956 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.8601 (2400 samples)\n",
      "Validation Loss: 0.8901 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.8539 (2400 samples)\n",
      "Validation Loss: 0.8846 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.8477 (2400 samples)\n",
      "Validation Loss: 0.8790 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.8416 (2400 samples)\n",
      "Validation Loss: 0.8734 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.8354 (2400 samples)\n",
      "Validation Loss: 0.8678 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.8293 (2400 samples)\n",
      "Validation Loss: 0.8621 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.8231 (2400 samples)\n",
      "Validation Loss: 0.8565 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.8170 (2400 samples)\n",
      "Validation Loss: 0.8508 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.8109 (2400 samples)\n",
      "Validation Loss: 0.8451 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.8048 (2400 samples)\n",
      "Validation Loss: 0.8394 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.7988 (2400 samples)\n",
      "Validation Loss: 0.8337 (150 samples)\n",
      "Training Loss: [1.0979793720144584, 1.0905933830891614, 1.0835792351623101, 1.0768452576482321, 1.0703285778396991, 1.0639759953643388, 1.0577498266559875, 1.0516215331704934, 1.0455683109432132, 1.0395724702512155, 1.0336230253009655, 1.027707460223186, 1.0218151300270808, 1.0159376870215595, 1.0100729062964324, 1.0042155756486357, 0.998359055925418, 0.9925009546388188, 0.9866398572549305, 0.9807699842510148, 0.974889682786711, 0.968996457452525, 0.9630876556858683, 0.9571619617075616, 0.9512168399530401, 0.9452536412191087, 0.9392723210006517, 0.9332706470874174, 0.9272505124557872, 0.9212118552812695, 0.9151556317075371, 0.9090821487850878, 0.9029941577827831, 0.8968911220622222, 0.8907744595192196, 0.8846458109814, 0.8785072402338475, 0.8723615682006078, 0.8662083184794299, 0.860051484377366, 0.8538920179229266, 0.8477329014253496, 0.8415769304359547, 0.8354266969160231, 0.8292819978984295, 0.8231481808421744, 0.8170286198395056, 0.8109226241657925, 0.8048323466811301, 0.7987601083432472]\n",
      "Validation Losses: [1.0927237969513377, 1.086631468133392, 1.080846700826383, 1.0752925252327392, 1.0699165628274927, 1.064674380773498, 1.0595321041584527, 1.0544640511241894, 1.0494556839301243, 1.0444869184007293, 1.0395479016017102, 1.0346270017401213, 1.0297160610867655, 1.0248109302225727, 1.0199085110870056, 1.0150015143167734, 1.0100868282620534, 1.0051590441662481, 1.0002179137780323, 0.9952602297752009, 0.99028065883571, 0.9852778182905408, 0.980251695116728, 0.9751996931245284, 0.9701171406202473, 0.9650060580621848, 0.9598670506658031, 0.9546962446203077, 0.9494890862368502, 0.9442443097713129, 0.9389695752287373, 0.9336607506443634, 0.9283149829575058, 0.9229345120038969, 0.9175281660491915, 0.9120978506274887, 0.9066384462590386, 0.9011529819491784, 0.8956460212645262, 0.8901154509518127, 0.8845592319752459, 0.878983011670024, 0.8733850813862444, 0.8677690684093833, 0.8621355138552894, 0.856482122969623, 0.850808024290934, 0.8451167072994651, 0.8394120695106054, 0.8336981212808863]\n",
      "Hyperparameters: lr=0.001, dropout=0.3, embedding_dim=200\n",
      "Testing with learning rate: 0.001, dropout rate: 0.3, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 1.1267 (2400 samples)\n",
      "Validation Loss: 1.1401 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.1081 (2400 samples)\n",
      "Validation Loss: 1.1271 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0932 (2400 samples)\n",
      "Validation Loss: 1.1166 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0806 (2400 samples)\n",
      "Validation Loss: 1.1073 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0692 (2400 samples)\n",
      "Validation Loss: 1.0989 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0587 (2400 samples)\n",
      "Validation Loss: 1.0908 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0488 (2400 samples)\n",
      "Validation Loss: 1.0830 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0393 (2400 samples)\n",
      "Validation Loss: 1.0754 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0300 (2400 samples)\n",
      "Validation Loss: 1.0678 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0209 (2400 samples)\n",
      "Validation Loss: 1.0604 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0120 (2400 samples)\n",
      "Validation Loss: 1.0529 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0033 (2400 samples)\n",
      "Validation Loss: 1.0456 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.9946 (2400 samples)\n",
      "Validation Loss: 1.0382 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.9861 (2400 samples)\n",
      "Validation Loss: 1.0310 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.9777 (2400 samples)\n",
      "Validation Loss: 1.0237 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.9694 (2400 samples)\n",
      "Validation Loss: 1.0165 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.9612 (2400 samples)\n",
      "Validation Loss: 1.0093 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.9531 (2400 samples)\n",
      "Validation Loss: 1.0021 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.9451 (2400 samples)\n",
      "Validation Loss: 0.9950 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.9371 (2400 samples)\n",
      "Validation Loss: 0.9879 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.9292 (2400 samples)\n",
      "Validation Loss: 0.9809 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.9214 (2400 samples)\n",
      "Validation Loss: 0.9738 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.9136 (2400 samples)\n",
      "Validation Loss: 0.9668 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.9059 (2400 samples)\n",
      "Validation Loss: 0.9598 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.8983 (2400 samples)\n",
      "Validation Loss: 0.9528 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.8907 (2400 samples)\n",
      "Validation Loss: 0.9459 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.8832 (2400 samples)\n",
      "Validation Loss: 0.9389 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.8758 (2400 samples)\n",
      "Validation Loss: 0.9320 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.8684 (2400 samples)\n",
      "Validation Loss: 0.9251 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.8611 (2400 samples)\n",
      "Validation Loss: 0.9182 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.8539 (2400 samples)\n",
      "Validation Loss: 0.9114 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.8467 (2400 samples)\n",
      "Validation Loss: 0.9045 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.8396 (2400 samples)\n",
      "Validation Loss: 0.8977 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.8325 (2400 samples)\n",
      "Validation Loss: 0.8909 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.8255 (2400 samples)\n",
      "Validation Loss: 0.8842 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.8186 (2400 samples)\n",
      "Validation Loss: 0.8774 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.8117 (2400 samples)\n",
      "Validation Loss: 0.8707 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.8049 (2400 samples)\n",
      "Validation Loss: 0.8640 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.7981 (2400 samples)\n",
      "Validation Loss: 0.8573 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.7914 (2400 samples)\n",
      "Validation Loss: 0.8506 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.7848 (2400 samples)\n",
      "Validation Loss: 0.8440 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.7782 (2400 samples)\n",
      "Validation Loss: 0.8374 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.7717 (2400 samples)\n",
      "Validation Loss: 0.8308 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.7653 (2400 samples)\n",
      "Validation Loss: 0.8242 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.7589 (2400 samples)\n",
      "Validation Loss: 0.8177 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.7526 (2400 samples)\n",
      "Validation Loss: 0.8112 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.7463 (2400 samples)\n",
      "Validation Loss: 0.8048 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.7402 (2400 samples)\n",
      "Validation Loss: 0.7983 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.7340 (2400 samples)\n",
      "Validation Loss: 0.7919 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.7280 (2400 samples)\n",
      "Validation Loss: 0.7855 (150 samples)\n",
      "Training Loss: [1.1267331029068848, 1.1080910383200908, 1.0932191833009075, 1.080556472919933, 1.0692309662218362, 1.0587432303035373, 1.0488085849039495, 1.0392523301378354, 1.029970982652601, 1.0209044029027023, 1.0120106327524865, 1.0032651455500816, 0.9946477955507044, 0.9861468039026698, 0.9777485815127953, 0.9694466313271335, 0.9612341424870651, 0.9531056789227679, 0.9450551087390865, 0.9370818673529538, 0.9291816897647738, 0.9213539188376778, 0.9135950244835569, 0.9059039973906052, 0.8982784656853934, 0.8907170104512607, 0.8832186321582081, 0.8757837205139579, 0.8684136724933694, 0.8611069754670044, 0.8538632989056777, 0.8466792364110727, 0.8395558171412425, 0.8324935649670305, 0.8254934885917729, 0.8185536241057264, 0.8116741786533939, 0.8048588309394563, 0.7981057634336621, 0.7914142575362378, 0.7847844551040114, 0.7782177227110715, 0.7717153113421203, 0.7652771457353934, 0.7589028341625194, 0.7525916299074129, 0.7463454532805932, 0.7401636060001233, 0.7340463177563459, 0.7279933088842301]\n",
      "Validation Losses: [1.1400654572168705, 1.127119139130394, 1.1165610206312209, 1.107338700002015, 1.0988653190481177, 1.090816266153653, 1.083016491996316, 1.0753695399072782, 1.0678230459405436, 1.0603501909862876, 1.052937798779506, 1.0455718735003081, 1.0382482375224733, 1.0309581427560048, 1.0237051492142264, 1.0164856582639687, 1.0092992440994364, 1.0021491293310176, 0.9950285460372436, 0.9879311879747058, 0.980859389332794, 0.973812073304936, 0.9667887172658514, 0.959790650105643, 0.9528151884411771, 0.9458584034725146, 0.938919228246973, 0.9319993798202825, 0.9251037668478138, 0.9182299749584949, 0.911374592689903, 0.9045414579557287, 0.8977274157431157, 0.8909343469223298, 0.8841616903009649, 0.8774091416232469, 0.870678814674983, 0.8639711214720598, 0.8572877597837337, 0.850626393182825, 0.8439893635678236, 0.8373809081254235, 0.8307992217038453, 0.8242456400943122, 0.8177188848530011, 0.8112196943667204, 0.8047527894298757, 0.7983150865758436, 0.7919087403719939, 0.7855373955214056]\n",
      "Hyperparameters: lr=0.001, dropout=0.3, embedding_dim=300\n",
      "Testing with learning rate: 0.001, dropout rate: 0.5, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 1.1057 (2400 samples)\n",
      "Validation Loss: 1.0955 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.1015 (2400 samples)\n",
      "Validation Loss: 1.0919 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0974 (2400 samples)\n",
      "Validation Loss: 1.0884 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0934 (2400 samples)\n",
      "Validation Loss: 1.0851 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0896 (2400 samples)\n",
      "Validation Loss: 1.0818 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0857 (2400 samples)\n",
      "Validation Loss: 1.0786 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0820 (2400 samples)\n",
      "Validation Loss: 1.0754 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0783 (2400 samples)\n",
      "Validation Loss: 1.0723 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0746 (2400 samples)\n",
      "Validation Loss: 1.0692 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0710 (2400 samples)\n",
      "Validation Loss: 1.0661 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0674 (2400 samples)\n",
      "Validation Loss: 1.0631 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0638 (2400 samples)\n",
      "Validation Loss: 1.0600 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 1.0602 (2400 samples)\n",
      "Validation Loss: 1.0570 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 1.0565 (2400 samples)\n",
      "Validation Loss: 1.0540 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 1.0529 (2400 samples)\n",
      "Validation Loss: 1.0510 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 1.0493 (2400 samples)\n",
      "Validation Loss: 1.0479 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 1.0457 (2400 samples)\n",
      "Validation Loss: 1.0448 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 1.0420 (2400 samples)\n",
      "Validation Loss: 1.0418 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 1.0383 (2400 samples)\n",
      "Validation Loss: 1.0387 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 1.0346 (2400 samples)\n",
      "Validation Loss: 1.0355 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 1.0308 (2400 samples)\n",
      "Validation Loss: 1.0324 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 1.0270 (2400 samples)\n",
      "Validation Loss: 1.0292 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 1.0232 (2400 samples)\n",
      "Validation Loss: 1.0260 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 1.0193 (2400 samples)\n",
      "Validation Loss: 1.0227 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 1.0154 (2400 samples)\n",
      "Validation Loss: 1.0194 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 1.0114 (2400 samples)\n",
      "Validation Loss: 1.0161 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 1.0074 (2400 samples)\n",
      "Validation Loss: 1.0127 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 1.0033 (2400 samples)\n",
      "Validation Loss: 1.0093 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.9992 (2400 samples)\n",
      "Validation Loss: 1.0059 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.9951 (2400 samples)\n",
      "Validation Loss: 1.0023 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.9908 (2400 samples)\n",
      "Validation Loss: 0.9988 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.9866 (2400 samples)\n",
      "Validation Loss: 0.9952 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.9823 (2400 samples)\n",
      "Validation Loss: 0.9915 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.9779 (2400 samples)\n",
      "Validation Loss: 0.9878 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.9735 (2400 samples)\n",
      "Validation Loss: 0.9840 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.9690 (2400 samples)\n",
      "Validation Loss: 0.9802 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.9644 (2400 samples)\n",
      "Validation Loss: 0.9764 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.9599 (2400 samples)\n",
      "Validation Loss: 0.9725 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.9552 (2400 samples)\n",
      "Validation Loss: 0.9685 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.9505 (2400 samples)\n",
      "Validation Loss: 0.9645 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.9458 (2400 samples)\n",
      "Validation Loss: 0.9604 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.9410 (2400 samples)\n",
      "Validation Loss: 0.9563 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.9361 (2400 samples)\n",
      "Validation Loss: 0.9522 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.9312 (2400 samples)\n",
      "Validation Loss: 0.9479 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.9263 (2400 samples)\n",
      "Validation Loss: 0.9437 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.9213 (2400 samples)\n",
      "Validation Loss: 0.9393 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.9163 (2400 samples)\n",
      "Validation Loss: 0.9350 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.9112 (2400 samples)\n",
      "Validation Loss: 0.9305 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.9061 (2400 samples)\n",
      "Validation Loss: 0.9261 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.9009 (2400 samples)\n",
      "Validation Loss: 0.9215 (150 samples)\n",
      "Training Loss: [1.1057206259438885, 1.1015068707171363, 1.0974224800865935, 1.0934451604018696, 1.08955861520457, 1.085748920926033, 1.0819990868364857, 1.0782961144207603, 1.0746287187012946, 1.0709889883886752, 1.0673675407116852, 1.0637588480126259, 1.060155449538258, 1.0565494518132164, 1.052934800945347, 1.04930681758877, 1.0456604825707274, 1.0419916693866829, 1.0382955188304646, 1.0345688704259124, 1.0308084543242038, 1.02701194785163, 1.0231758138926639, 1.0192987369127924, 1.0153791185932728, 1.0114114354410768, 1.0073954136521377, 1.0033306489914493, 0.9992169047877216, 0.9950528421502886, 0.9908379858276131, 0.986570695654248, 0.9822501548504757, 0.9778766198966082, 0.9734503927756085, 0.9689711811270623, 0.964439067132127, 0.9598511970595287, 0.9552107538318929, 0.9505157613308607, 0.9457687968724713, 0.9409692714891537, 0.9361202674438188, 0.9312242946713726, 0.9262820057817444, 0.9212935351515047, 0.9162599711699438, 0.9111834904755752, 0.90606763656734, 0.9009156657820402]\n",
      "Validation Losses: [1.0955081544838554, 1.0919199360997858, 1.0884467382803207, 1.0850736654577868, 1.0817860167111162, 1.0785646246502885, 1.075396756421921, 1.0722717691480275, 1.0691837745165398, 1.0661214891718183, 1.0630782874087905, 1.060042651497194, 1.057012102578924, 1.0539853510171218, 1.0509518437830214, 1.0479055042605594, 1.044844616292666, 1.0417664403256384, 1.0386667837245038, 1.0355414786084935, 1.032386334169755, 1.0292026003347086, 1.0259875093947846, 1.022733401618679, 1.0194389130546138, 1.0161066140352903, 1.012732896749927, 1.0093169670972781, 1.0058553516704167, 1.002348327013657, 0.9987917496674114, 0.9951829429158687, 0.9915207391345238, 0.987808078648464, 0.9840442743043015, 0.9802332009980378, 0.9763733868686575, 0.9724609205419567, 0.9685013931677122, 0.9644891397784431, 0.9604257645615378, 0.9563141603904313, 0.952150637379486, 0.9479352209463938, 0.9436662767748434, 0.9393443978739134, 0.9349689489363032, 0.9305406041287015, 0.9260647181254568, 0.9215410678586599]\n",
      "Hyperparameters: lr=0.001, dropout=0.5, embedding_dim=100\n",
      "Testing with learning rate: 0.001, dropout rate: 0.5, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 1.0980 (2400 samples)\n",
      "Validation Loss: 1.0927 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0906 (2400 samples)\n",
      "Validation Loss: 1.0866 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0836 (2400 samples)\n",
      "Validation Loss: 1.0808 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0768 (2400 samples)\n",
      "Validation Loss: 1.0753 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0703 (2400 samples)\n",
      "Validation Loss: 1.0699 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0640 (2400 samples)\n",
      "Validation Loss: 1.0647 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0577 (2400 samples)\n",
      "Validation Loss: 1.0595 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0516 (2400 samples)\n",
      "Validation Loss: 1.0545 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0456 (2400 samples)\n",
      "Validation Loss: 1.0495 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0396 (2400 samples)\n",
      "Validation Loss: 1.0445 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0336 (2400 samples)\n",
      "Validation Loss: 1.0395 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0277 (2400 samples)\n",
      "Validation Loss: 1.0346 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 1.0218 (2400 samples)\n",
      "Validation Loss: 1.0297 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 1.0159 (2400 samples)\n",
      "Validation Loss: 1.0248 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 1.0101 (2400 samples)\n",
      "Validation Loss: 1.0199 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 1.0042 (2400 samples)\n",
      "Validation Loss: 1.0150 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.9984 (2400 samples)\n",
      "Validation Loss: 1.0101 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.9925 (2400 samples)\n",
      "Validation Loss: 1.0052 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.9866 (2400 samples)\n",
      "Validation Loss: 1.0002 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.9808 (2400 samples)\n",
      "Validation Loss: 0.9953 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.9749 (2400 samples)\n",
      "Validation Loss: 0.9903 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.9690 (2400 samples)\n",
      "Validation Loss: 0.9853 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.9631 (2400 samples)\n",
      "Validation Loss: 0.9803 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.9572 (2400 samples)\n",
      "Validation Loss: 0.9752 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.9512 (2400 samples)\n",
      "Validation Loss: 0.9701 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.9453 (2400 samples)\n",
      "Validation Loss: 0.9650 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.9393 (2400 samples)\n",
      "Validation Loss: 0.9599 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.9333 (2400 samples)\n",
      "Validation Loss: 0.9547 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.9273 (2400 samples)\n",
      "Validation Loss: 0.9495 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.9212 (2400 samples)\n",
      "Validation Loss: 0.9442 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.9152 (2400 samples)\n",
      "Validation Loss: 0.9390 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.9091 (2400 samples)\n",
      "Validation Loss: 0.9337 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.9030 (2400 samples)\n",
      "Validation Loss: 0.9283 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.8969 (2400 samples)\n",
      "Validation Loss: 0.9229 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.8908 (2400 samples)\n",
      "Validation Loss: 0.9175 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.8846 (2400 samples)\n",
      "Validation Loss: 0.9121 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.8785 (2400 samples)\n",
      "Validation Loss: 0.9066 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.8724 (2400 samples)\n",
      "Validation Loss: 0.9012 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.8662 (2400 samples)\n",
      "Validation Loss: 0.8956 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.8601 (2400 samples)\n",
      "Validation Loss: 0.8901 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.8539 (2400 samples)\n",
      "Validation Loss: 0.8846 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.8477 (2400 samples)\n",
      "Validation Loss: 0.8790 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.8416 (2400 samples)\n",
      "Validation Loss: 0.8734 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.8354 (2400 samples)\n",
      "Validation Loss: 0.8678 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.8293 (2400 samples)\n",
      "Validation Loss: 0.8621 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.8231 (2400 samples)\n",
      "Validation Loss: 0.8565 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.8170 (2400 samples)\n",
      "Validation Loss: 0.8508 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.8109 (2400 samples)\n",
      "Validation Loss: 0.8451 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.8048 (2400 samples)\n",
      "Validation Loss: 0.8394 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.7988 (2400 samples)\n",
      "Validation Loss: 0.8337 (150 samples)\n",
      "Training Loss: [1.0979793720144584, 1.0905933830891614, 1.0835792351623101, 1.0768452576482321, 1.0703285778396991, 1.0639759953643388, 1.0577498266559875, 1.0516215331704934, 1.0455683109432132, 1.0395724702512155, 1.0336230253009655, 1.027707460223186, 1.0218151300270808, 1.0159376870215595, 1.0100729062964324, 1.0042155756486357, 0.998359055925418, 0.9925009546388188, 0.9866398572549305, 0.9807699842510148, 0.974889682786711, 0.968996457452525, 0.9630876556858683, 0.9571619617075616, 0.9512168399530401, 0.9452536412191087, 0.9392723210006517, 0.9332706470874174, 0.9272505124557872, 0.9212118552812695, 0.9151556317075371, 0.9090821487850878, 0.9029941577827831, 0.8968911220622222, 0.8907744595192196, 0.8846458109814, 0.8785072402338475, 0.8723615682006078, 0.8662083184794299, 0.860051484377366, 0.8538920179229266, 0.8477329014253496, 0.8415769304359547, 0.8354266969160231, 0.8292819978984295, 0.8231481808421744, 0.8170286198395056, 0.8109226241657925, 0.8048323466811301, 0.7987601083432472]\n",
      "Validation Losses: [1.0927237969513377, 1.086631468133392, 1.080846700826383, 1.0752925252327392, 1.0699165628274927, 1.064674380773498, 1.0595321041584527, 1.0544640511241894, 1.0494556839301243, 1.0444869184007293, 1.0395479016017102, 1.0346270017401213, 1.0297160610867655, 1.0248109302225727, 1.0199085110870056, 1.0150015143167734, 1.0100868282620534, 1.0051590441662481, 1.0002179137780323, 0.9952602297752009, 0.99028065883571, 0.9852778182905408, 0.980251695116728, 0.9751996931245284, 0.9701171406202473, 0.9650060580621848, 0.9598670506658031, 0.9546962446203077, 0.9494890862368502, 0.9442443097713129, 0.9389695752287373, 0.9336607506443634, 0.9283149829575058, 0.9229345120038969, 0.9175281660491915, 0.9120978506274887, 0.9066384462590386, 0.9011529819491784, 0.8956460212645262, 0.8901154509518127, 0.8845592319752459, 0.878983011670024, 0.8733850813862444, 0.8677690684093833, 0.8621355138552894, 0.856482122969623, 0.850808024290934, 0.8451167072994651, 0.8394120695106054, 0.8336981212808863]\n",
      "Hyperparameters: lr=0.001, dropout=0.5, embedding_dim=200\n",
      "Testing with learning rate: 0.001, dropout rate: 0.5, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 1.1267 (2400 samples)\n",
      "Validation Loss: 1.1401 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.1081 (2400 samples)\n",
      "Validation Loss: 1.1271 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0932 (2400 samples)\n",
      "Validation Loss: 1.1166 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0806 (2400 samples)\n",
      "Validation Loss: 1.1073 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0692 (2400 samples)\n",
      "Validation Loss: 1.0989 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0587 (2400 samples)\n",
      "Validation Loss: 1.0908 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0488 (2400 samples)\n",
      "Validation Loss: 1.0830 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0393 (2400 samples)\n",
      "Validation Loss: 1.0754 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0300 (2400 samples)\n",
      "Validation Loss: 1.0678 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0209 (2400 samples)\n",
      "Validation Loss: 1.0604 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0120 (2400 samples)\n",
      "Validation Loss: 1.0529 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0033 (2400 samples)\n",
      "Validation Loss: 1.0456 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.9946 (2400 samples)\n",
      "Validation Loss: 1.0382 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.9861 (2400 samples)\n",
      "Validation Loss: 1.0310 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.9777 (2400 samples)\n",
      "Validation Loss: 1.0237 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.9694 (2400 samples)\n",
      "Validation Loss: 1.0165 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.9612 (2400 samples)\n",
      "Validation Loss: 1.0093 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.9531 (2400 samples)\n",
      "Validation Loss: 1.0021 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.9451 (2400 samples)\n",
      "Validation Loss: 0.9950 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.9371 (2400 samples)\n",
      "Validation Loss: 0.9879 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.9292 (2400 samples)\n",
      "Validation Loss: 0.9809 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.9214 (2400 samples)\n",
      "Validation Loss: 0.9738 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.9136 (2400 samples)\n",
      "Validation Loss: 0.9668 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.9059 (2400 samples)\n",
      "Validation Loss: 0.9598 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.8983 (2400 samples)\n",
      "Validation Loss: 0.9528 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.8907 (2400 samples)\n",
      "Validation Loss: 0.9459 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.8832 (2400 samples)\n",
      "Validation Loss: 0.9389 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.8758 (2400 samples)\n",
      "Validation Loss: 0.9320 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.8684 (2400 samples)\n",
      "Validation Loss: 0.9251 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.8611 (2400 samples)\n",
      "Validation Loss: 0.9182 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.8539 (2400 samples)\n",
      "Validation Loss: 0.9114 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.8467 (2400 samples)\n",
      "Validation Loss: 0.9045 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.8396 (2400 samples)\n",
      "Validation Loss: 0.8977 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.8325 (2400 samples)\n",
      "Validation Loss: 0.8909 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.8255 (2400 samples)\n",
      "Validation Loss: 0.8842 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.8186 (2400 samples)\n",
      "Validation Loss: 0.8774 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.8117 (2400 samples)\n",
      "Validation Loss: 0.8707 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.8049 (2400 samples)\n",
      "Validation Loss: 0.8640 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.7981 (2400 samples)\n",
      "Validation Loss: 0.8573 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.7914 (2400 samples)\n",
      "Validation Loss: 0.8506 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.7848 (2400 samples)\n",
      "Validation Loss: 0.8440 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.7782 (2400 samples)\n",
      "Validation Loss: 0.8374 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.7717 (2400 samples)\n",
      "Validation Loss: 0.8308 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.7653 (2400 samples)\n",
      "Validation Loss: 0.8242 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.7589 (2400 samples)\n",
      "Validation Loss: 0.8177 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.7526 (2400 samples)\n",
      "Validation Loss: 0.8112 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.7463 (2400 samples)\n",
      "Validation Loss: 0.8048 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.7402 (2400 samples)\n",
      "Validation Loss: 0.7983 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.7340 (2400 samples)\n",
      "Validation Loss: 0.7919 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.7280 (2400 samples)\n",
      "Validation Loss: 0.7855 (150 samples)\n",
      "Training Loss: [1.1267331029068848, 1.1080910383200908, 1.0932191833009075, 1.080556472919933, 1.0692309662218362, 1.0587432303035373, 1.0488085849039495, 1.0392523301378354, 1.029970982652601, 1.0209044029027023, 1.0120106327524865, 1.0032651455500816, 0.9946477955507044, 0.9861468039026698, 0.9777485815127953, 0.9694466313271335, 0.9612341424870651, 0.9531056789227679, 0.9450551087390865, 0.9370818673529538, 0.9291816897647738, 0.9213539188376778, 0.9135950244835569, 0.9059039973906052, 0.8982784656853934, 0.8907170104512607, 0.8832186321582081, 0.8757837205139579, 0.8684136724933694, 0.8611069754670044, 0.8538632989056777, 0.8466792364110727, 0.8395558171412425, 0.8324935649670305, 0.8254934885917729, 0.8185536241057264, 0.8116741786533939, 0.8048588309394563, 0.7981057634336621, 0.7914142575362378, 0.7847844551040114, 0.7782177227110715, 0.7717153113421203, 0.7652771457353934, 0.7589028341625194, 0.7525916299074129, 0.7463454532805932, 0.7401636060001233, 0.7340463177563459, 0.7279933088842301]\n",
      "Validation Losses: [1.1400654572168705, 1.127119139130394, 1.1165610206312209, 1.107338700002015, 1.0988653190481177, 1.090816266153653, 1.083016491996316, 1.0753695399072782, 1.0678230459405436, 1.0603501909862876, 1.052937798779506, 1.0455718735003081, 1.0382482375224733, 1.0309581427560048, 1.0237051492142264, 1.0164856582639687, 1.0092992440994364, 1.0021491293310176, 0.9950285460372436, 0.9879311879747058, 0.980859389332794, 0.973812073304936, 0.9667887172658514, 0.959790650105643, 0.9528151884411771, 0.9458584034725146, 0.938919228246973, 0.9319993798202825, 0.9251037668478138, 0.9182299749584949, 0.911374592689903, 0.9045414579557287, 0.8977274157431157, 0.8909343469223298, 0.8841616903009649, 0.8774091416232469, 0.870678814674983, 0.8639711214720598, 0.8572877597837337, 0.850626393182825, 0.8439893635678236, 0.8373809081254235, 0.8307992217038453, 0.8242456400943122, 0.8177188848530011, 0.8112196943667204, 0.8047527894298757, 0.7983150865758436, 0.7919087403719939, 0.7855373955214056]\n",
      "Hyperparameters: lr=0.001, dropout=0.5, embedding_dim=300\n",
      "Testing with learning rate: 0.001, dropout rate: 0.7, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 1.1057 (2400 samples)\n",
      "Validation Loss: 1.0955 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.1015 (2400 samples)\n",
      "Validation Loss: 1.0919 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0974 (2400 samples)\n",
      "Validation Loss: 1.0884 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0934 (2400 samples)\n",
      "Validation Loss: 1.0851 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0896 (2400 samples)\n",
      "Validation Loss: 1.0818 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0857 (2400 samples)\n",
      "Validation Loss: 1.0786 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0820 (2400 samples)\n",
      "Validation Loss: 1.0754 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0783 (2400 samples)\n",
      "Validation Loss: 1.0723 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0746 (2400 samples)\n",
      "Validation Loss: 1.0692 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0710 (2400 samples)\n",
      "Validation Loss: 1.0661 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0674 (2400 samples)\n",
      "Validation Loss: 1.0631 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0638 (2400 samples)\n",
      "Validation Loss: 1.0600 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 1.0602 (2400 samples)\n",
      "Validation Loss: 1.0570 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 1.0565 (2400 samples)\n",
      "Validation Loss: 1.0540 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 1.0529 (2400 samples)\n",
      "Validation Loss: 1.0510 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 1.0493 (2400 samples)\n",
      "Validation Loss: 1.0479 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 1.0457 (2400 samples)\n",
      "Validation Loss: 1.0448 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 1.0420 (2400 samples)\n",
      "Validation Loss: 1.0418 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 1.0383 (2400 samples)\n",
      "Validation Loss: 1.0387 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 1.0346 (2400 samples)\n",
      "Validation Loss: 1.0355 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 1.0308 (2400 samples)\n",
      "Validation Loss: 1.0324 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 1.0270 (2400 samples)\n",
      "Validation Loss: 1.0292 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 1.0232 (2400 samples)\n",
      "Validation Loss: 1.0260 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 1.0193 (2400 samples)\n",
      "Validation Loss: 1.0227 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 1.0154 (2400 samples)\n",
      "Validation Loss: 1.0194 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 1.0114 (2400 samples)\n",
      "Validation Loss: 1.0161 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 1.0074 (2400 samples)\n",
      "Validation Loss: 1.0127 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 1.0033 (2400 samples)\n",
      "Validation Loss: 1.0093 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.9992 (2400 samples)\n",
      "Validation Loss: 1.0059 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.9951 (2400 samples)\n",
      "Validation Loss: 1.0023 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.9908 (2400 samples)\n",
      "Validation Loss: 0.9988 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.9866 (2400 samples)\n",
      "Validation Loss: 0.9952 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.9823 (2400 samples)\n",
      "Validation Loss: 0.9915 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.9779 (2400 samples)\n",
      "Validation Loss: 0.9878 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.9735 (2400 samples)\n",
      "Validation Loss: 0.9840 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.9690 (2400 samples)\n",
      "Validation Loss: 0.9802 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.9644 (2400 samples)\n",
      "Validation Loss: 0.9764 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.9599 (2400 samples)\n",
      "Validation Loss: 0.9725 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.9552 (2400 samples)\n",
      "Validation Loss: 0.9685 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.9505 (2400 samples)\n",
      "Validation Loss: 0.9645 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.9458 (2400 samples)\n",
      "Validation Loss: 0.9604 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.9410 (2400 samples)\n",
      "Validation Loss: 0.9563 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.9361 (2400 samples)\n",
      "Validation Loss: 0.9522 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.9312 (2400 samples)\n",
      "Validation Loss: 0.9479 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.9263 (2400 samples)\n",
      "Validation Loss: 0.9437 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.9213 (2400 samples)\n",
      "Validation Loss: 0.9393 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.9163 (2400 samples)\n",
      "Validation Loss: 0.9350 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.9112 (2400 samples)\n",
      "Validation Loss: 0.9305 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.9061 (2400 samples)\n",
      "Validation Loss: 0.9261 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.9009 (2400 samples)\n",
      "Validation Loss: 0.9215 (150 samples)\n",
      "Training Loss: [1.1057206259438885, 1.1015068707171363, 1.0974224800865935, 1.0934451604018696, 1.08955861520457, 1.085748920926033, 1.0819990868364857, 1.0782961144207603, 1.0746287187012946, 1.0709889883886752, 1.0673675407116852, 1.0637588480126259, 1.060155449538258, 1.0565494518132164, 1.052934800945347, 1.04930681758877, 1.0456604825707274, 1.0419916693866829, 1.0382955188304646, 1.0345688704259124, 1.0308084543242038, 1.02701194785163, 1.0231758138926639, 1.0192987369127924, 1.0153791185932728, 1.0114114354410768, 1.0073954136521377, 1.0033306489914493, 0.9992169047877216, 0.9950528421502886, 0.9908379858276131, 0.986570695654248, 0.9822501548504757, 0.9778766198966082, 0.9734503927756085, 0.9689711811270623, 0.964439067132127, 0.9598511970595287, 0.9552107538318929, 0.9505157613308607, 0.9457687968724713, 0.9409692714891537, 0.9361202674438188, 0.9312242946713726, 0.9262820057817444, 0.9212935351515047, 0.9162599711699438, 0.9111834904755752, 0.90606763656734, 0.9009156657820402]\n",
      "Validation Losses: [1.0955081544838554, 1.0919199360997858, 1.0884467382803207, 1.0850736654577868, 1.0817860167111162, 1.0785646246502885, 1.075396756421921, 1.0722717691480275, 1.0691837745165398, 1.0661214891718183, 1.0630782874087905, 1.060042651497194, 1.057012102578924, 1.0539853510171218, 1.0509518437830214, 1.0479055042605594, 1.044844616292666, 1.0417664403256384, 1.0386667837245038, 1.0355414786084935, 1.032386334169755, 1.0292026003347086, 1.0259875093947846, 1.022733401618679, 1.0194389130546138, 1.0161066140352903, 1.012732896749927, 1.0093169670972781, 1.0058553516704167, 1.002348327013657, 0.9987917496674114, 0.9951829429158687, 0.9915207391345238, 0.987808078648464, 0.9840442743043015, 0.9802332009980378, 0.9763733868686575, 0.9724609205419567, 0.9685013931677122, 0.9644891397784431, 0.9604257645615378, 0.9563141603904313, 0.952150637379486, 0.9479352209463938, 0.9436662767748434, 0.9393443978739134, 0.9349689489363032, 0.9305406041287015, 0.9260647181254568, 0.9215410678586599]\n",
      "Hyperparameters: lr=0.001, dropout=0.7, embedding_dim=100\n",
      "Testing with learning rate: 0.001, dropout rate: 0.7, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 1.0980 (2400 samples)\n",
      "Validation Loss: 1.0927 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0906 (2400 samples)\n",
      "Validation Loss: 1.0866 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0836 (2400 samples)\n",
      "Validation Loss: 1.0808 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0768 (2400 samples)\n",
      "Validation Loss: 1.0753 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0703 (2400 samples)\n",
      "Validation Loss: 1.0699 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0640 (2400 samples)\n",
      "Validation Loss: 1.0647 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0577 (2400 samples)\n",
      "Validation Loss: 1.0595 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0516 (2400 samples)\n",
      "Validation Loss: 1.0545 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0456 (2400 samples)\n",
      "Validation Loss: 1.0495 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0396 (2400 samples)\n",
      "Validation Loss: 1.0445 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0336 (2400 samples)\n",
      "Validation Loss: 1.0395 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0277 (2400 samples)\n",
      "Validation Loss: 1.0346 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 1.0218 (2400 samples)\n",
      "Validation Loss: 1.0297 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 1.0159 (2400 samples)\n",
      "Validation Loss: 1.0248 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 1.0101 (2400 samples)\n",
      "Validation Loss: 1.0199 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 1.0042 (2400 samples)\n",
      "Validation Loss: 1.0150 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.9984 (2400 samples)\n",
      "Validation Loss: 1.0101 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.9925 (2400 samples)\n",
      "Validation Loss: 1.0052 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.9866 (2400 samples)\n",
      "Validation Loss: 1.0002 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.9808 (2400 samples)\n",
      "Validation Loss: 0.9953 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.9749 (2400 samples)\n",
      "Validation Loss: 0.9903 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.9690 (2400 samples)\n",
      "Validation Loss: 0.9853 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.9631 (2400 samples)\n",
      "Validation Loss: 0.9803 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.9572 (2400 samples)\n",
      "Validation Loss: 0.9752 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.9512 (2400 samples)\n",
      "Validation Loss: 0.9701 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.9453 (2400 samples)\n",
      "Validation Loss: 0.9650 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.9393 (2400 samples)\n",
      "Validation Loss: 0.9599 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.9333 (2400 samples)\n",
      "Validation Loss: 0.9547 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.9273 (2400 samples)\n",
      "Validation Loss: 0.9495 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.9212 (2400 samples)\n",
      "Validation Loss: 0.9442 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.9152 (2400 samples)\n",
      "Validation Loss: 0.9390 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.9091 (2400 samples)\n",
      "Validation Loss: 0.9337 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.9030 (2400 samples)\n",
      "Validation Loss: 0.9283 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.8969 (2400 samples)\n",
      "Validation Loss: 0.9229 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.8908 (2400 samples)\n",
      "Validation Loss: 0.9175 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.8846 (2400 samples)\n",
      "Validation Loss: 0.9121 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.8785 (2400 samples)\n",
      "Validation Loss: 0.9066 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.8724 (2400 samples)\n",
      "Validation Loss: 0.9012 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.8662 (2400 samples)\n",
      "Validation Loss: 0.8956 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.8601 (2400 samples)\n",
      "Validation Loss: 0.8901 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.8539 (2400 samples)\n",
      "Validation Loss: 0.8846 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.8477 (2400 samples)\n",
      "Validation Loss: 0.8790 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.8416 (2400 samples)\n",
      "Validation Loss: 0.8734 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.8354 (2400 samples)\n",
      "Validation Loss: 0.8678 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.8293 (2400 samples)\n",
      "Validation Loss: 0.8621 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.8231 (2400 samples)\n",
      "Validation Loss: 0.8565 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.8170 (2400 samples)\n",
      "Validation Loss: 0.8508 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.8109 (2400 samples)\n",
      "Validation Loss: 0.8451 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.8048 (2400 samples)\n",
      "Validation Loss: 0.8394 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.7988 (2400 samples)\n",
      "Validation Loss: 0.8337 (150 samples)\n",
      "Training Loss: [1.0979793720144584, 1.0905933830891614, 1.0835792351623101, 1.0768452576482321, 1.0703285778396991, 1.0639759953643388, 1.0577498266559875, 1.0516215331704934, 1.0455683109432132, 1.0395724702512155, 1.0336230253009655, 1.027707460223186, 1.0218151300270808, 1.0159376870215595, 1.0100729062964324, 1.0042155756486357, 0.998359055925418, 0.9925009546388188, 0.9866398572549305, 0.9807699842510148, 0.974889682786711, 0.968996457452525, 0.9630876556858683, 0.9571619617075616, 0.9512168399530401, 0.9452536412191087, 0.9392723210006517, 0.9332706470874174, 0.9272505124557872, 0.9212118552812695, 0.9151556317075371, 0.9090821487850878, 0.9029941577827831, 0.8968911220622222, 0.8907744595192196, 0.8846458109814, 0.8785072402338475, 0.8723615682006078, 0.8662083184794299, 0.860051484377366, 0.8538920179229266, 0.8477329014253496, 0.8415769304359547, 0.8354266969160231, 0.8292819978984295, 0.8231481808421744, 0.8170286198395056, 0.8109226241657925, 0.8048323466811301, 0.7987601083432472]\n",
      "Validation Losses: [1.0927237969513377, 1.086631468133392, 1.080846700826383, 1.0752925252327392, 1.0699165628274927, 1.064674380773498, 1.0595321041584527, 1.0544640511241894, 1.0494556839301243, 1.0444869184007293, 1.0395479016017102, 1.0346270017401213, 1.0297160610867655, 1.0248109302225727, 1.0199085110870056, 1.0150015143167734, 1.0100868282620534, 1.0051590441662481, 1.0002179137780323, 0.9952602297752009, 0.99028065883571, 0.9852778182905408, 0.980251695116728, 0.9751996931245284, 0.9701171406202473, 0.9650060580621848, 0.9598670506658031, 0.9546962446203077, 0.9494890862368502, 0.9442443097713129, 0.9389695752287373, 0.9336607506443634, 0.9283149829575058, 0.9229345120038969, 0.9175281660491915, 0.9120978506274887, 0.9066384462590386, 0.9011529819491784, 0.8956460212645262, 0.8901154509518127, 0.8845592319752459, 0.878983011670024, 0.8733850813862444, 0.8677690684093833, 0.8621355138552894, 0.856482122969623, 0.850808024290934, 0.8451167072994651, 0.8394120695106054, 0.8336981212808863]\n",
      "Hyperparameters: lr=0.001, dropout=0.7, embedding_dim=200\n",
      "Testing with learning rate: 0.001, dropout rate: 0.7, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 1.1267 (2400 samples)\n",
      "Validation Loss: 1.1401 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.1081 (2400 samples)\n",
      "Validation Loss: 1.1271 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0932 (2400 samples)\n",
      "Validation Loss: 1.1166 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 1.0806 (2400 samples)\n",
      "Validation Loss: 1.1073 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 1.0692 (2400 samples)\n",
      "Validation Loss: 1.0989 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 1.0587 (2400 samples)\n",
      "Validation Loss: 1.0908 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 1.0488 (2400 samples)\n",
      "Validation Loss: 1.0830 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 1.0393 (2400 samples)\n",
      "Validation Loss: 1.0754 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 1.0300 (2400 samples)\n",
      "Validation Loss: 1.0678 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 1.0209 (2400 samples)\n",
      "Validation Loss: 1.0604 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 1.0120 (2400 samples)\n",
      "Validation Loss: 1.0529 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 1.0033 (2400 samples)\n",
      "Validation Loss: 1.0456 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.9946 (2400 samples)\n",
      "Validation Loss: 1.0382 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.9861 (2400 samples)\n",
      "Validation Loss: 1.0310 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.9777 (2400 samples)\n",
      "Validation Loss: 1.0237 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.9694 (2400 samples)\n",
      "Validation Loss: 1.0165 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.9612 (2400 samples)\n",
      "Validation Loss: 1.0093 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.9531 (2400 samples)\n",
      "Validation Loss: 1.0021 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.9451 (2400 samples)\n",
      "Validation Loss: 0.9950 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.9371 (2400 samples)\n",
      "Validation Loss: 0.9879 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.9292 (2400 samples)\n",
      "Validation Loss: 0.9809 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.9214 (2400 samples)\n",
      "Validation Loss: 0.9738 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.9136 (2400 samples)\n",
      "Validation Loss: 0.9668 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.9059 (2400 samples)\n",
      "Validation Loss: 0.9598 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.8983 (2400 samples)\n",
      "Validation Loss: 0.9528 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.8907 (2400 samples)\n",
      "Validation Loss: 0.9459 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.8832 (2400 samples)\n",
      "Validation Loss: 0.9389 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.8758 (2400 samples)\n",
      "Validation Loss: 0.9320 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.8684 (2400 samples)\n",
      "Validation Loss: 0.9251 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.8611 (2400 samples)\n",
      "Validation Loss: 0.9182 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.8539 (2400 samples)\n",
      "Validation Loss: 0.9114 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.8467 (2400 samples)\n",
      "Validation Loss: 0.9045 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.8396 (2400 samples)\n",
      "Validation Loss: 0.8977 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.8325 (2400 samples)\n",
      "Validation Loss: 0.8909 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.8255 (2400 samples)\n",
      "Validation Loss: 0.8842 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.8186 (2400 samples)\n",
      "Validation Loss: 0.8774 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.8117 (2400 samples)\n",
      "Validation Loss: 0.8707 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.8049 (2400 samples)\n",
      "Validation Loss: 0.8640 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.7981 (2400 samples)\n",
      "Validation Loss: 0.8573 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.7914 (2400 samples)\n",
      "Validation Loss: 0.8506 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.7848 (2400 samples)\n",
      "Validation Loss: 0.8440 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.7782 (2400 samples)\n",
      "Validation Loss: 0.8374 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.7717 (2400 samples)\n",
      "Validation Loss: 0.8308 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.7653 (2400 samples)\n",
      "Validation Loss: 0.8242 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.7589 (2400 samples)\n",
      "Validation Loss: 0.8177 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.7526 (2400 samples)\n",
      "Validation Loss: 0.8112 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.7463 (2400 samples)\n",
      "Validation Loss: 0.8048 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.7402 (2400 samples)\n",
      "Validation Loss: 0.7983 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.7340 (2400 samples)\n",
      "Validation Loss: 0.7919 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.7280 (2400 samples)\n",
      "Validation Loss: 0.7855 (150 samples)\n",
      "Training Loss: [1.1267331029068848, 1.1080910383200908, 1.0932191833009075, 1.080556472919933, 1.0692309662218362, 1.0587432303035373, 1.0488085849039495, 1.0392523301378354, 1.029970982652601, 1.0209044029027023, 1.0120106327524865, 1.0032651455500816, 0.9946477955507044, 0.9861468039026698, 0.9777485815127953, 0.9694466313271335, 0.9612341424870651, 0.9531056789227679, 0.9450551087390865, 0.9370818673529538, 0.9291816897647738, 0.9213539188376778, 0.9135950244835569, 0.9059039973906052, 0.8982784656853934, 0.8907170104512607, 0.8832186321582081, 0.8757837205139579, 0.8684136724933694, 0.8611069754670044, 0.8538632989056777, 0.8466792364110727, 0.8395558171412425, 0.8324935649670305, 0.8254934885917729, 0.8185536241057264, 0.8116741786533939, 0.8048588309394563, 0.7981057634336621, 0.7914142575362378, 0.7847844551040114, 0.7782177227110715, 0.7717153113421203, 0.7652771457353934, 0.7589028341625194, 0.7525916299074129, 0.7463454532805932, 0.7401636060001233, 0.7340463177563459, 0.7279933088842301]\n",
      "Validation Losses: [1.1400654572168705, 1.127119139130394, 1.1165610206312209, 1.107338700002015, 1.0988653190481177, 1.090816266153653, 1.083016491996316, 1.0753695399072782, 1.0678230459405436, 1.0603501909862876, 1.052937798779506, 1.0455718735003081, 1.0382482375224733, 1.0309581427560048, 1.0237051492142264, 1.0164856582639687, 1.0092992440994364, 1.0021491293310176, 0.9950285460372436, 0.9879311879747058, 0.980859389332794, 0.973812073304936, 0.9667887172658514, 0.959790650105643, 0.9528151884411771, 0.9458584034725146, 0.938919228246973, 0.9319993798202825, 0.9251037668478138, 0.9182299749584949, 0.911374592689903, 0.9045414579557287, 0.8977274157431157, 0.8909343469223298, 0.8841616903009649, 0.8774091416232469, 0.870678814674983, 0.8639711214720598, 0.8572877597837337, 0.850626393182825, 0.8439893635678236, 0.8373809081254235, 0.8307992217038453, 0.8242456400943122, 0.8177188848530011, 0.8112196943667204, 0.8047527894298757, 0.7983150865758436, 0.7919087403719939, 0.7855373955214056]\n",
      "Hyperparameters: lr=0.001, dropout=0.7, embedding_dim=300\n",
      "Testing with learning rate: 0.01, dropout rate: 0.3, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 1.0904 (2400 samples)\n",
      "Validation Loss: 1.0703 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0535 (2400 samples)\n",
      "Validation Loss: 1.0398 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0160 (2400 samples)\n",
      "Validation Loss: 1.0069 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.9742 (2400 samples)\n",
      "Validation Loss: 0.9696 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.9273 (2400 samples)\n",
      "Validation Loss: 0.9272 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.8761 (2400 samples)\n",
      "Validation Loss: 0.8803 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.8227 (2400 samples)\n",
      "Validation Loss: 0.8305 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.7697 (2400 samples)\n",
      "Validation Loss: 0.7795 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.7186 (2400 samples)\n",
      "Validation Loss: 0.7290 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.6708 (2400 samples)\n",
      "Validation Loss: 0.6801 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.6266 (2400 samples)\n",
      "Validation Loss: 0.6338 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.5863 (2400 samples)\n",
      "Validation Loss: 0.5906 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.5497 (2400 samples)\n",
      "Validation Loss: 0.5507 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.5166 (2400 samples)\n",
      "Validation Loss: 0.5142 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.4866 (2400 samples)\n",
      "Validation Loss: 0.4809 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.4595 (2400 samples)\n",
      "Validation Loss: 0.4507 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.4349 (2400 samples)\n",
      "Validation Loss: 0.4233 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.4126 (2400 samples)\n",
      "Validation Loss: 0.3985 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3921 (2400 samples)\n",
      "Validation Loss: 0.3759 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.3733 (2400 samples)\n",
      "Validation Loss: 0.3553 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.3560 (2400 samples)\n",
      "Validation Loss: 0.3365 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.3399 (2400 samples)\n",
      "Validation Loss: 0.3193 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.3249 (2400 samples)\n",
      "Validation Loss: 0.3035 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.3110 (2400 samples)\n",
      "Validation Loss: 0.2888 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2978 (2400 samples)\n",
      "Validation Loss: 0.2753 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2855 (2400 samples)\n",
      "Validation Loss: 0.2628 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2739 (2400 samples)\n",
      "Validation Loss: 0.2510 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2629 (2400 samples)\n",
      "Validation Loss: 0.2401 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.2526 (2400 samples)\n",
      "Validation Loss: 0.2298 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.2427 (2400 samples)\n",
      "Validation Loss: 0.2201 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.2334 (2400 samples)\n",
      "Validation Loss: 0.2110 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.2246 (2400 samples)\n",
      "Validation Loss: 0.2024 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.2162 (2400 samples)\n",
      "Validation Loss: 0.1943 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.2082 (2400 samples)\n",
      "Validation Loss: 0.1867 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.2006 (2400 samples)\n",
      "Validation Loss: 0.1794 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1934 (2400 samples)\n",
      "Validation Loss: 0.1725 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1865 (2400 samples)\n",
      "Validation Loss: 0.1659 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1800 (2400 samples)\n",
      "Validation Loss: 0.1597 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1738 (2400 samples)\n",
      "Validation Loss: 0.1539 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1678 (2400 samples)\n",
      "Validation Loss: 0.1482 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1622 (2400 samples)\n",
      "Validation Loss: 0.1429 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1568 (2400 samples)\n",
      "Validation Loss: 0.1379 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1516 (2400 samples)\n",
      "Validation Loss: 0.1331 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1467 (2400 samples)\n",
      "Validation Loss: 0.1285 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1419 (2400 samples)\n",
      "Validation Loss: 0.1241 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1374 (2400 samples)\n",
      "Validation Loss: 0.1200 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1331 (2400 samples)\n",
      "Validation Loss: 0.1160 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1289 (2400 samples)\n",
      "Validation Loss: 0.1122 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.1249 (2400 samples)\n",
      "Validation Loss: 0.1086 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.1211 (2400 samples)\n",
      "Validation Loss: 0.1052 (150 samples)\n",
      "Training Loss: [1.0904044913771556, 1.0535131160188782, 1.0159720440148594, 0.9741820094441844, 0.9272646027224503, 0.8760814911616815, 0.8227448109466267, 0.7696642107734567, 0.7186145069294683, 0.6707539861631614, 0.6265804352678185, 0.5862679528822152, 0.5496826560115351, 0.5165593282976686, 0.4866063609148805, 0.45951347996444875, 0.4349378423876393, 0.41256107117459784, 0.39209730852409597, 0.37331603568180344, 0.3559797379686389, 0.33990420339544575, 0.3249428886795073, 0.3109547941730861, 0.29782626580660365, 0.2854937254455515, 0.27387732203142207, 0.2629121371037572, 0.252551903841847, 0.24273021360200858, 0.23342127374359176, 0.2245887389310188, 0.21619814581149835, 0.20821598340286285, 0.20061987694268432, 0.19340097553736876, 0.1865286793089676, 0.1799884479431183, 0.17376655903305147, 0.1678351617626998, 0.1621736713237832, 0.15676659446551358, 0.1515987750764579, 0.14665675681522683, 0.14192693710845977, 0.13739782971786288, 0.13306561170679088, 0.12891751644655552, 0.12494756023973312, 0.12114860866445]\n",
      "Validation Losses: [1.0702613647938866, 1.0397742114943909, 1.0069318762737185, 0.9695568492751043, 0.9271780336967929, 0.8803218519628202, 0.8304847565752117, 0.7795238948502634, 0.7290146729973306, 0.6801368146553097, 0.6337620769811639, 0.5905515711063877, 0.5507113159458729, 0.5141528714339755, 0.4808739854542578, 0.45070046815430015, 0.4233037048645494, 0.3984824460464512, 0.3759048087933548, 0.35534157898451224, 0.33652337087243767, 0.31928809476304515, 0.30345697275322353, 0.2888409455731436, 0.2753282119963002, 0.2627727721847101, 0.2510471379253546, 0.24006276886008407, 0.22977148727659394, 0.22007616112593945, 0.21097724864739925, 0.2024062043414197, 0.1943171230896613, 0.18665191053226018, 0.1793759242482061, 0.17248159805511634, 0.16594853256452047, 0.15974648224899296, 0.15385348533641446, 0.14824792088365418, 0.14293349734543428, 0.13788040702972884, 0.13306439463520062, 0.1284866825910633, 0.12411919168913926, 0.11998030312387037, 0.11602080224361554, 0.11223753360576415, 0.10862935368249661, 0.10517998018008719]\n",
      "Hyperparameters: lr=0.01, dropout=0.3, embedding_dim=100\n",
      "Testing with learning rate: 0.01, dropout rate: 0.3, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 1.0721 (2400 samples)\n",
      "Validation Loss: 1.0519 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0115 (2400 samples)\n",
      "Validation Loss: 1.0027 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.9528 (2400 samples)\n",
      "Validation Loss: 0.9520 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.8926 (2400 samples)\n",
      "Validation Loss: 0.8982 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.8315 (2400 samples)\n",
      "Validation Loss: 0.8421 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.7714 (2400 samples)\n",
      "Validation Loss: 0.7851 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.7142 (2400 samples)\n",
      "Validation Loss: 0.7294 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.6614 (2400 samples)\n",
      "Validation Loss: 0.6760 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.6136 (2400 samples)\n",
      "Validation Loss: 0.6262 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.5709 (2400 samples)\n",
      "Validation Loss: 0.5801 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.5327 (2400 samples)\n",
      "Validation Loss: 0.5381 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.4987 (2400 samples)\n",
      "Validation Loss: 0.5000 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.4683 (2400 samples)\n",
      "Validation Loss: 0.4654 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.4411 (2400 samples)\n",
      "Validation Loss: 0.4341 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.4165 (2400 samples)\n",
      "Validation Loss: 0.4057 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.3941 (2400 samples)\n",
      "Validation Loss: 0.3800 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.3738 (2400 samples)\n",
      "Validation Loss: 0.3566 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.3551 (2400 samples)\n",
      "Validation Loss: 0.3354 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3378 (2400 samples)\n",
      "Validation Loss: 0.3161 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.3218 (2400 samples)\n",
      "Validation Loss: 0.2984 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.3070 (2400 samples)\n",
      "Validation Loss: 0.2822 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.2931 (2400 samples)\n",
      "Validation Loss: 0.2673 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.2801 (2400 samples)\n",
      "Validation Loss: 0.2536 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.2679 (2400 samples)\n",
      "Validation Loss: 0.2409 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2565 (2400 samples)\n",
      "Validation Loss: 0.2291 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2457 (2400 samples)\n",
      "Validation Loss: 0.2182 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2356 (2400 samples)\n",
      "Validation Loss: 0.2080 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2260 (2400 samples)\n",
      "Validation Loss: 0.1985 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.2169 (2400 samples)\n",
      "Validation Loss: 0.1896 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.2082 (2400 samples)\n",
      "Validation Loss: 0.1813 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.2001 (2400 samples)\n",
      "Validation Loss: 0.1735 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.1923 (2400 samples)\n",
      "Validation Loss: 0.1661 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.1849 (2400 samples)\n",
      "Validation Loss: 0.1592 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.1779 (2400 samples)\n",
      "Validation Loss: 0.1527 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.1712 (2400 samples)\n",
      "Validation Loss: 0.1466 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1649 (2400 samples)\n",
      "Validation Loss: 0.1408 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1588 (2400 samples)\n",
      "Validation Loss: 0.1353 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1531 (2400 samples)\n",
      "Validation Loss: 0.1301 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1477 (2400 samples)\n",
      "Validation Loss: 0.1252 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1425 (2400 samples)\n",
      "Validation Loss: 0.1205 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1376 (2400 samples)\n",
      "Validation Loss: 0.1161 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1330 (2400 samples)\n",
      "Validation Loss: 0.1119 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1285 (2400 samples)\n",
      "Validation Loss: 0.1079 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1243 (2400 samples)\n",
      "Validation Loss: 0.1041 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1203 (2400 samples)\n",
      "Validation Loss: 0.1004 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1165 (2400 samples)\n",
      "Validation Loss: 0.0970 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1129 (2400 samples)\n",
      "Validation Loss: 0.0937 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1094 (2400 samples)\n",
      "Validation Loss: 0.0906 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.1061 (2400 samples)\n",
      "Validation Loss: 0.0876 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.1029 (2400 samples)\n",
      "Validation Loss: 0.0848 (150 samples)\n",
      "Training Loss: [1.0721025727215183, 1.0114910760785993, 0.9527667603052642, 0.892620047965431, 0.8315222906136024, 0.7713972128298888, 0.7142394915086456, 0.6614384714456057, 0.6136392482542955, 0.5708543696033461, 0.5327132668435904, 0.4987206560728415, 0.46833899865063927, 0.4410674876299865, 0.41646829043167305, 0.39414971962301676, 0.37377987291943743, 0.35507236378482226, 0.3378190853769261, 0.3218422568804909, 0.3069805851662543, 0.2931024412765911, 0.2801164095720379, 0.26794119052451476, 0.2564979049462237, 0.24572525823029234, 0.2355655402434108, 0.225959094997202, 0.21685978771089476, 0.20824374419719555, 0.20007538714835374, 0.1923225504848006, 0.18494781197208793, 0.17792978109172874, 0.17124815308607064, 0.16489031737003298, 0.158848748173855, 0.15311380469072697, 0.14767119343706092, 0.14250996238716404, 0.13761434156920094, 0.13296630998393663, 0.12854788162912056, 0.12434204280251704, 0.12033470368921238, 0.11651648625485853, 0.11287489449972796, 0.10939613671113402, 0.1060745343015949, 0.10290101518443373]\n",
      "Validation Losses: [1.0518786878784678, 1.0027177635168438, 0.9519656775683444, 0.8981564837658336, 0.8420686652973998, 0.7851377379004565, 0.7293550711870145, 0.6760354376320225, 0.6261525635339045, 0.5801378707294654, 0.538136592468789, 0.4999712656530981, 0.46538104788288187, 0.4340692591116224, 0.4057165069282305, 0.3799776364077216, 0.3566441017440523, 0.33543285968523395, 0.3160979005812784, 0.298383650847746, 0.2821738074571377, 0.2672877747089116, 0.25356343152013017, 0.24088092166419642, 0.22911434694448957, 0.21818122544440774, 0.20800623998583856, 0.19850635277452697, 0.18961154114888476, 0.18128471393543136, 0.17346781038589887, 0.16611967800660188, 0.15920821642393476, 0.15270097162732205, 0.14656285347118472, 0.1407585523700455, 0.13527517838450037, 0.13008675818131857, 0.1251673411361786, 0.1205002819910266, 0.11607343027124695, 0.11186413570077602, 0.10786561322386265, 0.10406431626607543, 0.10044485979685219, 0.09699743417585935, 0.09371442530032104, 0.09058985954315973, 0.08761455842809379, 0.08477958476099769]\n",
      "Hyperparameters: lr=0.01, dropout=0.3, embedding_dim=200\n",
      "Testing with learning rate: 0.01, dropout rate: 0.3, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 1.0746 (2400 samples)\n",
      "Validation Loss: 1.0716 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.9802 (2400 samples)\n",
      "Validation Loss: 0.9981 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.9006 (2400 samples)\n",
      "Validation Loss: 0.9280 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.8280 (2400 samples)\n",
      "Validation Loss: 0.8601 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.7617 (2400 samples)\n",
      "Validation Loss: 0.7948 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.7017 (2400 samples)\n",
      "Validation Loss: 0.7328 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.6480 (2400 samples)\n",
      "Validation Loss: 0.6749 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.6001 (2400 samples)\n",
      "Validation Loss: 0.6213 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.5575 (2400 samples)\n",
      "Validation Loss: 0.5725 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.5197 (2400 samples)\n",
      "Validation Loss: 0.5284 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.4859 (2400 samples)\n",
      "Validation Loss: 0.4887 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.4558 (2400 samples)\n",
      "Validation Loss: 0.4532 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.4287 (2400 samples)\n",
      "Validation Loss: 0.4213 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.4042 (2400 samples)\n",
      "Validation Loss: 0.3928 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.3820 (2400 samples)\n",
      "Validation Loss: 0.3672 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.3617 (2400 samples)\n",
      "Validation Loss: 0.3442 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.3430 (2400 samples)\n",
      "Validation Loss: 0.3235 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.3258 (2400 samples)\n",
      "Validation Loss: 0.3047 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3099 (2400 samples)\n",
      "Validation Loss: 0.2877 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.2952 (2400 samples)\n",
      "Validation Loss: 0.2721 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.2814 (2400 samples)\n",
      "Validation Loss: 0.2580 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.2685 (2400 samples)\n",
      "Validation Loss: 0.2450 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.2565 (2400 samples)\n",
      "Validation Loss: 0.2330 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.2452 (2400 samples)\n",
      "Validation Loss: 0.2220 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2346 (2400 samples)\n",
      "Validation Loss: 0.2118 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2246 (2400 samples)\n",
      "Validation Loss: 0.2023 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2151 (2400 samples)\n",
      "Validation Loss: 0.1934 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2063 (2400 samples)\n",
      "Validation Loss: 0.1851 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.1979 (2400 samples)\n",
      "Validation Loss: 0.1774 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.1900 (2400 samples)\n",
      "Validation Loss: 0.1701 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.1825 (2400 samples)\n",
      "Validation Loss: 0.1632 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.1754 (2400 samples)\n",
      "Validation Loss: 0.1567 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.1687 (2400 samples)\n",
      "Validation Loss: 0.1506 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.1624 (2400 samples)\n",
      "Validation Loss: 0.1448 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.1564 (2400 samples)\n",
      "Validation Loss: 0.1393 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1507 (2400 samples)\n",
      "Validation Loss: 0.1341 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1453 (2400 samples)\n",
      "Validation Loss: 0.1291 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1401 (2400 samples)\n",
      "Validation Loss: 0.1244 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1352 (2400 samples)\n",
      "Validation Loss: 0.1200 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1306 (2400 samples)\n",
      "Validation Loss: 0.1157 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1262 (2400 samples)\n",
      "Validation Loss: 0.1117 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1220 (2400 samples)\n",
      "Validation Loss: 0.1079 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1180 (2400 samples)\n",
      "Validation Loss: 0.1042 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1141 (2400 samples)\n",
      "Validation Loss: 0.1008 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1105 (2400 samples)\n",
      "Validation Loss: 0.0975 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1070 (2400 samples)\n",
      "Validation Loss: 0.0943 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1037 (2400 samples)\n",
      "Validation Loss: 0.0913 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1005 (2400 samples)\n",
      "Validation Loss: 0.0884 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0975 (2400 samples)\n",
      "Validation Loss: 0.0856 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0946 (2400 samples)\n",
      "Validation Loss: 0.0830 (150 samples)\n",
      "Training Loss: [1.0746493170483182, 0.9801602202651408, 0.9006061261472047, 0.8280042053061997, 0.7616692461942952, 0.7017027350815236, 0.6479588297789388, 0.6000744559063865, 0.5575147913232508, 0.5196657140242174, 0.48593438860959776, 0.4557661384963896, 0.42866577136653566, 0.40418203354230586, 0.3819519293414173, 0.3616525282441643, 0.3430207369065054, 0.3258380755264544, 0.3099304047783234, 0.29515650937407983, 0.2813913397848881, 0.2685269956044738, 0.2564808449368942, 0.24517577330261953, 0.23455009523191345, 0.22455353928237917, 0.2151324291298471, 0.20625024204332207, 0.19787028482428998, 0.18995826680751246, 0.1824805830750372, 0.17540965800660072, 0.16871662995489872, 0.16237668194127772, 0.15636734182745154, 0.15066923709884483, 0.14526092020889647, 0.14012665235368707, 0.13524959497879885, 0.13061163655909722, 0.12619197303865956, 0.12198123701941582, 0.11797106646871597, 0.1141489976767955, 0.11050430005517138, 0.10702678541519006, 0.10370619777524388, 0.10053269682719296, 0.09750010456237677, 0.09459898743072205]\n",
      "Validation Losses: [1.0715594929727454, 0.9981024154139063, 0.9279710257505301, 0.8600993377812409, 0.7947929850933728, 0.732827250151053, 0.67485012205703, 0.6212806360607266, 0.5724671626733733, 0.5283617648207302, 0.48873105293464597, 0.4531866033856706, 0.4213331477026942, 0.3928049190238619, 0.3672278270528792, 0.3442267286261588, 0.3234579206200379, 0.30468386403672865, 0.2876682810969875, 0.27214837227156335, 0.2579714140497427, 0.24497904650461114, 0.23302597328548297, 0.22199358647159018, 0.21177919730910136, 0.2022801856039154, 0.19341932850170587, 0.18513257448318013, 0.17737280389207918, 0.17006684035748884, 0.16318484810617703, 0.1566943851084582, 0.15056198674295293, 0.14476043781055184, 0.1392679466054313, 0.13405651486346762, 0.1291148507669905, 0.12442453227348788, 0.1199740087276522, 0.115747859353642, 0.11172512698926558, 0.10789441866826067, 0.1042484679109073, 0.1007757791558336, 0.09746272440259933, 0.09429486219613412, 0.09127387397809182, 0.08839028976261716, 0.08563636779889236, 0.08300400529861533]\n",
      "Hyperparameters: lr=0.01, dropout=0.3, embedding_dim=300\n",
      "Testing with learning rate: 0.01, dropout rate: 0.5, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 1.0904 (2400 samples)\n",
      "Validation Loss: 1.0703 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0535 (2400 samples)\n",
      "Validation Loss: 1.0398 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0160 (2400 samples)\n",
      "Validation Loss: 1.0069 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.9742 (2400 samples)\n",
      "Validation Loss: 0.9696 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.9273 (2400 samples)\n",
      "Validation Loss: 0.9272 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.8761 (2400 samples)\n",
      "Validation Loss: 0.8803 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.8227 (2400 samples)\n",
      "Validation Loss: 0.8305 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.7697 (2400 samples)\n",
      "Validation Loss: 0.7795 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.7186 (2400 samples)\n",
      "Validation Loss: 0.7290 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.6708 (2400 samples)\n",
      "Validation Loss: 0.6801 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.6266 (2400 samples)\n",
      "Validation Loss: 0.6338 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.5863 (2400 samples)\n",
      "Validation Loss: 0.5906 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.5497 (2400 samples)\n",
      "Validation Loss: 0.5507 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.5166 (2400 samples)\n",
      "Validation Loss: 0.5142 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.4866 (2400 samples)\n",
      "Validation Loss: 0.4809 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.4595 (2400 samples)\n",
      "Validation Loss: 0.4507 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.4349 (2400 samples)\n",
      "Validation Loss: 0.4233 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.4126 (2400 samples)\n",
      "Validation Loss: 0.3985 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3921 (2400 samples)\n",
      "Validation Loss: 0.3759 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.3733 (2400 samples)\n",
      "Validation Loss: 0.3553 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.3560 (2400 samples)\n",
      "Validation Loss: 0.3365 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.3399 (2400 samples)\n",
      "Validation Loss: 0.3193 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.3249 (2400 samples)\n",
      "Validation Loss: 0.3035 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.3110 (2400 samples)\n",
      "Validation Loss: 0.2888 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2978 (2400 samples)\n",
      "Validation Loss: 0.2753 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2855 (2400 samples)\n",
      "Validation Loss: 0.2628 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2739 (2400 samples)\n",
      "Validation Loss: 0.2510 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2629 (2400 samples)\n",
      "Validation Loss: 0.2401 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.2526 (2400 samples)\n",
      "Validation Loss: 0.2298 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.2427 (2400 samples)\n",
      "Validation Loss: 0.2201 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.2334 (2400 samples)\n",
      "Validation Loss: 0.2110 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.2246 (2400 samples)\n",
      "Validation Loss: 0.2024 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.2162 (2400 samples)\n",
      "Validation Loss: 0.1943 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.2082 (2400 samples)\n",
      "Validation Loss: 0.1867 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.2006 (2400 samples)\n",
      "Validation Loss: 0.1794 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1934 (2400 samples)\n",
      "Validation Loss: 0.1725 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1865 (2400 samples)\n",
      "Validation Loss: 0.1659 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1800 (2400 samples)\n",
      "Validation Loss: 0.1597 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1738 (2400 samples)\n",
      "Validation Loss: 0.1539 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1678 (2400 samples)\n",
      "Validation Loss: 0.1482 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1622 (2400 samples)\n",
      "Validation Loss: 0.1429 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1568 (2400 samples)\n",
      "Validation Loss: 0.1379 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1516 (2400 samples)\n",
      "Validation Loss: 0.1331 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1467 (2400 samples)\n",
      "Validation Loss: 0.1285 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1419 (2400 samples)\n",
      "Validation Loss: 0.1241 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1374 (2400 samples)\n",
      "Validation Loss: 0.1200 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1331 (2400 samples)\n",
      "Validation Loss: 0.1160 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1289 (2400 samples)\n",
      "Validation Loss: 0.1122 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.1249 (2400 samples)\n",
      "Validation Loss: 0.1086 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.1211 (2400 samples)\n",
      "Validation Loss: 0.1052 (150 samples)\n",
      "Training Loss: [1.0904044913771556, 1.0535131160188782, 1.0159720440148594, 0.9741820094441844, 0.9272646027224503, 0.8760814911616815, 0.8227448109466267, 0.7696642107734567, 0.7186145069294683, 0.6707539861631614, 0.6265804352678185, 0.5862679528822152, 0.5496826560115351, 0.5165593282976686, 0.4866063609148805, 0.45951347996444875, 0.4349378423876393, 0.41256107117459784, 0.39209730852409597, 0.37331603568180344, 0.3559797379686389, 0.33990420339544575, 0.3249428886795073, 0.3109547941730861, 0.29782626580660365, 0.2854937254455515, 0.27387732203142207, 0.2629121371037572, 0.252551903841847, 0.24273021360200858, 0.23342127374359176, 0.2245887389310188, 0.21619814581149835, 0.20821598340286285, 0.20061987694268432, 0.19340097553736876, 0.1865286793089676, 0.1799884479431183, 0.17376655903305147, 0.1678351617626998, 0.1621736713237832, 0.15676659446551358, 0.1515987750764579, 0.14665675681522683, 0.14192693710845977, 0.13739782971786288, 0.13306561170679088, 0.12891751644655552, 0.12494756023973312, 0.12114860866445]\n",
      "Validation Losses: [1.0702613647938866, 1.0397742114943909, 1.0069318762737185, 0.9695568492751043, 0.9271780336967929, 0.8803218519628202, 0.8304847565752117, 0.7795238948502634, 0.7290146729973306, 0.6801368146553097, 0.6337620769811639, 0.5905515711063877, 0.5507113159458729, 0.5141528714339755, 0.4808739854542578, 0.45070046815430015, 0.4233037048645494, 0.3984824460464512, 0.3759048087933548, 0.35534157898451224, 0.33652337087243767, 0.31928809476304515, 0.30345697275322353, 0.2888409455731436, 0.2753282119963002, 0.2627727721847101, 0.2510471379253546, 0.24006276886008407, 0.22977148727659394, 0.22007616112593945, 0.21097724864739925, 0.2024062043414197, 0.1943171230896613, 0.18665191053226018, 0.1793759242482061, 0.17248159805511634, 0.16594853256452047, 0.15974648224899296, 0.15385348533641446, 0.14824792088365418, 0.14293349734543428, 0.13788040702972884, 0.13306439463520062, 0.1284866825910633, 0.12411919168913926, 0.11998030312387037, 0.11602080224361554, 0.11223753360576415, 0.10862935368249661, 0.10517998018008719]\n",
      "Hyperparameters: lr=0.01, dropout=0.5, embedding_dim=100\n",
      "Testing with learning rate: 0.01, dropout rate: 0.5, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 1.0721 (2400 samples)\n",
      "Validation Loss: 1.0519 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0115 (2400 samples)\n",
      "Validation Loss: 1.0027 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.9528 (2400 samples)\n",
      "Validation Loss: 0.9520 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.8926 (2400 samples)\n",
      "Validation Loss: 0.8982 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.8315 (2400 samples)\n",
      "Validation Loss: 0.8421 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.7714 (2400 samples)\n",
      "Validation Loss: 0.7851 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.7142 (2400 samples)\n",
      "Validation Loss: 0.7294 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.6614 (2400 samples)\n",
      "Validation Loss: 0.6760 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.6136 (2400 samples)\n",
      "Validation Loss: 0.6262 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.5709 (2400 samples)\n",
      "Validation Loss: 0.5801 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.5327 (2400 samples)\n",
      "Validation Loss: 0.5381 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.4987 (2400 samples)\n",
      "Validation Loss: 0.5000 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.4683 (2400 samples)\n",
      "Validation Loss: 0.4654 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.4411 (2400 samples)\n",
      "Validation Loss: 0.4341 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.4165 (2400 samples)\n",
      "Validation Loss: 0.4057 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.3941 (2400 samples)\n",
      "Validation Loss: 0.3800 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.3738 (2400 samples)\n",
      "Validation Loss: 0.3566 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.3551 (2400 samples)\n",
      "Validation Loss: 0.3354 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3378 (2400 samples)\n",
      "Validation Loss: 0.3161 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.3218 (2400 samples)\n",
      "Validation Loss: 0.2984 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.3070 (2400 samples)\n",
      "Validation Loss: 0.2822 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.2931 (2400 samples)\n",
      "Validation Loss: 0.2673 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.2801 (2400 samples)\n",
      "Validation Loss: 0.2536 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.2679 (2400 samples)\n",
      "Validation Loss: 0.2409 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2565 (2400 samples)\n",
      "Validation Loss: 0.2291 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2457 (2400 samples)\n",
      "Validation Loss: 0.2182 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2356 (2400 samples)\n",
      "Validation Loss: 0.2080 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2260 (2400 samples)\n",
      "Validation Loss: 0.1985 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.2169 (2400 samples)\n",
      "Validation Loss: 0.1896 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.2082 (2400 samples)\n",
      "Validation Loss: 0.1813 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.2001 (2400 samples)\n",
      "Validation Loss: 0.1735 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.1923 (2400 samples)\n",
      "Validation Loss: 0.1661 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.1849 (2400 samples)\n",
      "Validation Loss: 0.1592 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.1779 (2400 samples)\n",
      "Validation Loss: 0.1527 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.1712 (2400 samples)\n",
      "Validation Loss: 0.1466 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1649 (2400 samples)\n",
      "Validation Loss: 0.1408 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1588 (2400 samples)\n",
      "Validation Loss: 0.1353 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1531 (2400 samples)\n",
      "Validation Loss: 0.1301 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1477 (2400 samples)\n",
      "Validation Loss: 0.1252 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1425 (2400 samples)\n",
      "Validation Loss: 0.1205 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1376 (2400 samples)\n",
      "Validation Loss: 0.1161 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1330 (2400 samples)\n",
      "Validation Loss: 0.1119 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1285 (2400 samples)\n",
      "Validation Loss: 0.1079 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1243 (2400 samples)\n",
      "Validation Loss: 0.1041 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1203 (2400 samples)\n",
      "Validation Loss: 0.1004 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1165 (2400 samples)\n",
      "Validation Loss: 0.0970 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1129 (2400 samples)\n",
      "Validation Loss: 0.0937 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1094 (2400 samples)\n",
      "Validation Loss: 0.0906 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.1061 (2400 samples)\n",
      "Validation Loss: 0.0876 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.1029 (2400 samples)\n",
      "Validation Loss: 0.0848 (150 samples)\n",
      "Training Loss: [1.0721025727215183, 1.0114910760785993, 0.9527667603052642, 0.892620047965431, 0.8315222906136024, 0.7713972128298888, 0.7142394915086456, 0.6614384714456057, 0.6136392482542955, 0.5708543696033461, 0.5327132668435904, 0.4987206560728415, 0.46833899865063927, 0.4410674876299865, 0.41646829043167305, 0.39414971962301676, 0.37377987291943743, 0.35507236378482226, 0.3378190853769261, 0.3218422568804909, 0.3069805851662543, 0.2931024412765911, 0.2801164095720379, 0.26794119052451476, 0.2564979049462237, 0.24572525823029234, 0.2355655402434108, 0.225959094997202, 0.21685978771089476, 0.20824374419719555, 0.20007538714835374, 0.1923225504848006, 0.18494781197208793, 0.17792978109172874, 0.17124815308607064, 0.16489031737003298, 0.158848748173855, 0.15311380469072697, 0.14767119343706092, 0.14250996238716404, 0.13761434156920094, 0.13296630998393663, 0.12854788162912056, 0.12434204280251704, 0.12033470368921238, 0.11651648625485853, 0.11287489449972796, 0.10939613671113402, 0.1060745343015949, 0.10290101518443373]\n",
      "Validation Losses: [1.0518786878784678, 1.0027177635168438, 0.9519656775683444, 0.8981564837658336, 0.8420686652973998, 0.7851377379004565, 0.7293550711870145, 0.6760354376320225, 0.6261525635339045, 0.5801378707294654, 0.538136592468789, 0.4999712656530981, 0.46538104788288187, 0.4340692591116224, 0.4057165069282305, 0.3799776364077216, 0.3566441017440523, 0.33543285968523395, 0.3160979005812784, 0.298383650847746, 0.2821738074571377, 0.2672877747089116, 0.25356343152013017, 0.24088092166419642, 0.22911434694448957, 0.21818122544440774, 0.20800623998583856, 0.19850635277452697, 0.18961154114888476, 0.18128471393543136, 0.17346781038589887, 0.16611967800660188, 0.15920821642393476, 0.15270097162732205, 0.14656285347118472, 0.1407585523700455, 0.13527517838450037, 0.13008675818131857, 0.1251673411361786, 0.1205002819910266, 0.11607343027124695, 0.11186413570077602, 0.10786561322386265, 0.10406431626607543, 0.10044485979685219, 0.09699743417585935, 0.09371442530032104, 0.09058985954315973, 0.08761455842809379, 0.08477958476099769]\n",
      "Hyperparameters: lr=0.01, dropout=0.5, embedding_dim=200\n",
      "Testing with learning rate: 0.01, dropout rate: 0.5, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 1.0746 (2400 samples)\n",
      "Validation Loss: 1.0716 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.9802 (2400 samples)\n",
      "Validation Loss: 0.9981 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.9006 (2400 samples)\n",
      "Validation Loss: 0.9280 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.8280 (2400 samples)\n",
      "Validation Loss: 0.8601 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.7617 (2400 samples)\n",
      "Validation Loss: 0.7948 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.7017 (2400 samples)\n",
      "Validation Loss: 0.7328 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.6480 (2400 samples)\n",
      "Validation Loss: 0.6749 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.6001 (2400 samples)\n",
      "Validation Loss: 0.6213 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.5575 (2400 samples)\n",
      "Validation Loss: 0.5725 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.5197 (2400 samples)\n",
      "Validation Loss: 0.5284 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.4859 (2400 samples)\n",
      "Validation Loss: 0.4887 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.4558 (2400 samples)\n",
      "Validation Loss: 0.4532 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.4287 (2400 samples)\n",
      "Validation Loss: 0.4213 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.4042 (2400 samples)\n",
      "Validation Loss: 0.3928 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.3820 (2400 samples)\n",
      "Validation Loss: 0.3672 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.3617 (2400 samples)\n",
      "Validation Loss: 0.3442 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.3430 (2400 samples)\n",
      "Validation Loss: 0.3235 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.3258 (2400 samples)\n",
      "Validation Loss: 0.3047 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3099 (2400 samples)\n",
      "Validation Loss: 0.2877 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.2952 (2400 samples)\n",
      "Validation Loss: 0.2721 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.2814 (2400 samples)\n",
      "Validation Loss: 0.2580 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.2685 (2400 samples)\n",
      "Validation Loss: 0.2450 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.2565 (2400 samples)\n",
      "Validation Loss: 0.2330 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.2452 (2400 samples)\n",
      "Validation Loss: 0.2220 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2346 (2400 samples)\n",
      "Validation Loss: 0.2118 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2246 (2400 samples)\n",
      "Validation Loss: 0.2023 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2151 (2400 samples)\n",
      "Validation Loss: 0.1934 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2063 (2400 samples)\n",
      "Validation Loss: 0.1851 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.1979 (2400 samples)\n",
      "Validation Loss: 0.1774 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.1900 (2400 samples)\n",
      "Validation Loss: 0.1701 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.1825 (2400 samples)\n",
      "Validation Loss: 0.1632 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.1754 (2400 samples)\n",
      "Validation Loss: 0.1567 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.1687 (2400 samples)\n",
      "Validation Loss: 0.1506 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.1624 (2400 samples)\n",
      "Validation Loss: 0.1448 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.1564 (2400 samples)\n",
      "Validation Loss: 0.1393 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1507 (2400 samples)\n",
      "Validation Loss: 0.1341 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1453 (2400 samples)\n",
      "Validation Loss: 0.1291 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1401 (2400 samples)\n",
      "Validation Loss: 0.1244 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1352 (2400 samples)\n",
      "Validation Loss: 0.1200 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1306 (2400 samples)\n",
      "Validation Loss: 0.1157 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1262 (2400 samples)\n",
      "Validation Loss: 0.1117 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1220 (2400 samples)\n",
      "Validation Loss: 0.1079 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1180 (2400 samples)\n",
      "Validation Loss: 0.1042 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1141 (2400 samples)\n",
      "Validation Loss: 0.1008 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1105 (2400 samples)\n",
      "Validation Loss: 0.0975 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1070 (2400 samples)\n",
      "Validation Loss: 0.0943 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1037 (2400 samples)\n",
      "Validation Loss: 0.0913 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1005 (2400 samples)\n",
      "Validation Loss: 0.0884 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0975 (2400 samples)\n",
      "Validation Loss: 0.0856 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0946 (2400 samples)\n",
      "Validation Loss: 0.0830 (150 samples)\n",
      "Training Loss: [1.0746493170483182, 0.9801602202651408, 0.9006061261472047, 0.8280042053061997, 0.7616692461942952, 0.7017027350815236, 0.6479588297789388, 0.6000744559063865, 0.5575147913232508, 0.5196657140242174, 0.48593438860959776, 0.4557661384963896, 0.42866577136653566, 0.40418203354230586, 0.3819519293414173, 0.3616525282441643, 0.3430207369065054, 0.3258380755264544, 0.3099304047783234, 0.29515650937407983, 0.2813913397848881, 0.2685269956044738, 0.2564808449368942, 0.24517577330261953, 0.23455009523191345, 0.22455353928237917, 0.2151324291298471, 0.20625024204332207, 0.19787028482428998, 0.18995826680751246, 0.1824805830750372, 0.17540965800660072, 0.16871662995489872, 0.16237668194127772, 0.15636734182745154, 0.15066923709884483, 0.14526092020889647, 0.14012665235368707, 0.13524959497879885, 0.13061163655909722, 0.12619197303865956, 0.12198123701941582, 0.11797106646871597, 0.1141489976767955, 0.11050430005517138, 0.10702678541519006, 0.10370619777524388, 0.10053269682719296, 0.09750010456237677, 0.09459898743072205]\n",
      "Validation Losses: [1.0715594929727454, 0.9981024154139063, 0.9279710257505301, 0.8600993377812409, 0.7947929850933728, 0.732827250151053, 0.67485012205703, 0.6212806360607266, 0.5724671626733733, 0.5283617648207302, 0.48873105293464597, 0.4531866033856706, 0.4213331477026942, 0.3928049190238619, 0.3672278270528792, 0.3442267286261588, 0.3234579206200379, 0.30468386403672865, 0.2876682810969875, 0.27214837227156335, 0.2579714140497427, 0.24497904650461114, 0.23302597328548297, 0.22199358647159018, 0.21177919730910136, 0.2022801856039154, 0.19341932850170587, 0.18513257448318013, 0.17737280389207918, 0.17006684035748884, 0.16318484810617703, 0.1566943851084582, 0.15056198674295293, 0.14476043781055184, 0.1392679466054313, 0.13405651486346762, 0.1291148507669905, 0.12442453227348788, 0.1199740087276522, 0.115747859353642, 0.11172512698926558, 0.10789441866826067, 0.1042484679109073, 0.1007757791558336, 0.09746272440259933, 0.09429486219613412, 0.09127387397809182, 0.08839028976261716, 0.08563636779889236, 0.08300400529861533]\n",
      "Hyperparameters: lr=0.01, dropout=0.5, embedding_dim=300\n",
      "Testing with learning rate: 0.01, dropout rate: 0.7, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 1.0904 (2400 samples)\n",
      "Validation Loss: 1.0703 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0535 (2400 samples)\n",
      "Validation Loss: 1.0398 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 1.0160 (2400 samples)\n",
      "Validation Loss: 1.0069 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.9742 (2400 samples)\n",
      "Validation Loss: 0.9696 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.9273 (2400 samples)\n",
      "Validation Loss: 0.9272 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.8761 (2400 samples)\n",
      "Validation Loss: 0.8803 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.8227 (2400 samples)\n",
      "Validation Loss: 0.8305 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.7697 (2400 samples)\n",
      "Validation Loss: 0.7795 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.7186 (2400 samples)\n",
      "Validation Loss: 0.7290 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.6708 (2400 samples)\n",
      "Validation Loss: 0.6801 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.6266 (2400 samples)\n",
      "Validation Loss: 0.6338 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.5863 (2400 samples)\n",
      "Validation Loss: 0.5906 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.5497 (2400 samples)\n",
      "Validation Loss: 0.5507 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.5166 (2400 samples)\n",
      "Validation Loss: 0.5142 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.4866 (2400 samples)\n",
      "Validation Loss: 0.4809 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.4595 (2400 samples)\n",
      "Validation Loss: 0.4507 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.4349 (2400 samples)\n",
      "Validation Loss: 0.4233 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.4126 (2400 samples)\n",
      "Validation Loss: 0.3985 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3921 (2400 samples)\n",
      "Validation Loss: 0.3759 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.3733 (2400 samples)\n",
      "Validation Loss: 0.3553 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.3560 (2400 samples)\n",
      "Validation Loss: 0.3365 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.3399 (2400 samples)\n",
      "Validation Loss: 0.3193 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.3249 (2400 samples)\n",
      "Validation Loss: 0.3035 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.3110 (2400 samples)\n",
      "Validation Loss: 0.2888 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2978 (2400 samples)\n",
      "Validation Loss: 0.2753 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2855 (2400 samples)\n",
      "Validation Loss: 0.2628 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2739 (2400 samples)\n",
      "Validation Loss: 0.2510 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2629 (2400 samples)\n",
      "Validation Loss: 0.2401 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.2526 (2400 samples)\n",
      "Validation Loss: 0.2298 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.2427 (2400 samples)\n",
      "Validation Loss: 0.2201 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.2334 (2400 samples)\n",
      "Validation Loss: 0.2110 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.2246 (2400 samples)\n",
      "Validation Loss: 0.2024 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.2162 (2400 samples)\n",
      "Validation Loss: 0.1943 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.2082 (2400 samples)\n",
      "Validation Loss: 0.1867 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.2006 (2400 samples)\n",
      "Validation Loss: 0.1794 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1934 (2400 samples)\n",
      "Validation Loss: 0.1725 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1865 (2400 samples)\n",
      "Validation Loss: 0.1659 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1800 (2400 samples)\n",
      "Validation Loss: 0.1597 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1738 (2400 samples)\n",
      "Validation Loss: 0.1539 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1678 (2400 samples)\n",
      "Validation Loss: 0.1482 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1622 (2400 samples)\n",
      "Validation Loss: 0.1429 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1568 (2400 samples)\n",
      "Validation Loss: 0.1379 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1516 (2400 samples)\n",
      "Validation Loss: 0.1331 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1467 (2400 samples)\n",
      "Validation Loss: 0.1285 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1419 (2400 samples)\n",
      "Validation Loss: 0.1241 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1374 (2400 samples)\n",
      "Validation Loss: 0.1200 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1331 (2400 samples)\n",
      "Validation Loss: 0.1160 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1289 (2400 samples)\n",
      "Validation Loss: 0.1122 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.1249 (2400 samples)\n",
      "Validation Loss: 0.1086 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.1211 (2400 samples)\n",
      "Validation Loss: 0.1052 (150 samples)\n",
      "Training Loss: [1.0904044913771556, 1.0535131160188782, 1.0159720440148594, 0.9741820094441844, 0.9272646027224503, 0.8760814911616815, 0.8227448109466267, 0.7696642107734567, 0.7186145069294683, 0.6707539861631614, 0.6265804352678185, 0.5862679528822152, 0.5496826560115351, 0.5165593282976686, 0.4866063609148805, 0.45951347996444875, 0.4349378423876393, 0.41256107117459784, 0.39209730852409597, 0.37331603568180344, 0.3559797379686389, 0.33990420339544575, 0.3249428886795073, 0.3109547941730861, 0.29782626580660365, 0.2854937254455515, 0.27387732203142207, 0.2629121371037572, 0.252551903841847, 0.24273021360200858, 0.23342127374359176, 0.2245887389310188, 0.21619814581149835, 0.20821598340286285, 0.20061987694268432, 0.19340097553736876, 0.1865286793089676, 0.1799884479431183, 0.17376655903305147, 0.1678351617626998, 0.1621736713237832, 0.15676659446551358, 0.1515987750764579, 0.14665675681522683, 0.14192693710845977, 0.13739782971786288, 0.13306561170679088, 0.12891751644655552, 0.12494756023973312, 0.12114860866445]\n",
      "Validation Losses: [1.0702613647938866, 1.0397742114943909, 1.0069318762737185, 0.9695568492751043, 0.9271780336967929, 0.8803218519628202, 0.8304847565752117, 0.7795238948502634, 0.7290146729973306, 0.6801368146553097, 0.6337620769811639, 0.5905515711063877, 0.5507113159458729, 0.5141528714339755, 0.4808739854542578, 0.45070046815430015, 0.4233037048645494, 0.3984824460464512, 0.3759048087933548, 0.35534157898451224, 0.33652337087243767, 0.31928809476304515, 0.30345697275322353, 0.2888409455731436, 0.2753282119963002, 0.2627727721847101, 0.2510471379253546, 0.24006276886008407, 0.22977148727659394, 0.22007616112593945, 0.21097724864739925, 0.2024062043414197, 0.1943171230896613, 0.18665191053226018, 0.1793759242482061, 0.17248159805511634, 0.16594853256452047, 0.15974648224899296, 0.15385348533641446, 0.14824792088365418, 0.14293349734543428, 0.13788040702972884, 0.13306439463520062, 0.1284866825910633, 0.12411919168913926, 0.11998030312387037, 0.11602080224361554, 0.11223753360576415, 0.10862935368249661, 0.10517998018008719]\n",
      "Hyperparameters: lr=0.01, dropout=0.7, embedding_dim=100\n",
      "Testing with learning rate: 0.01, dropout rate: 0.7, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 1.0721 (2400 samples)\n",
      "Validation Loss: 1.0519 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 1.0115 (2400 samples)\n",
      "Validation Loss: 1.0027 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.9528 (2400 samples)\n",
      "Validation Loss: 0.9520 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.8926 (2400 samples)\n",
      "Validation Loss: 0.8982 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.8315 (2400 samples)\n",
      "Validation Loss: 0.8421 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.7714 (2400 samples)\n",
      "Validation Loss: 0.7851 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.7142 (2400 samples)\n",
      "Validation Loss: 0.7294 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.6614 (2400 samples)\n",
      "Validation Loss: 0.6760 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.6136 (2400 samples)\n",
      "Validation Loss: 0.6262 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.5709 (2400 samples)\n",
      "Validation Loss: 0.5801 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.5327 (2400 samples)\n",
      "Validation Loss: 0.5381 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.4987 (2400 samples)\n",
      "Validation Loss: 0.5000 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.4683 (2400 samples)\n",
      "Validation Loss: 0.4654 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.4411 (2400 samples)\n",
      "Validation Loss: 0.4341 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.4165 (2400 samples)\n",
      "Validation Loss: 0.4057 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.3941 (2400 samples)\n",
      "Validation Loss: 0.3800 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.3738 (2400 samples)\n",
      "Validation Loss: 0.3566 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.3551 (2400 samples)\n",
      "Validation Loss: 0.3354 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3378 (2400 samples)\n",
      "Validation Loss: 0.3161 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.3218 (2400 samples)\n",
      "Validation Loss: 0.2984 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.3070 (2400 samples)\n",
      "Validation Loss: 0.2822 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.2931 (2400 samples)\n",
      "Validation Loss: 0.2673 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.2801 (2400 samples)\n",
      "Validation Loss: 0.2536 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.2679 (2400 samples)\n",
      "Validation Loss: 0.2409 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2565 (2400 samples)\n",
      "Validation Loss: 0.2291 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2457 (2400 samples)\n",
      "Validation Loss: 0.2182 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2356 (2400 samples)\n",
      "Validation Loss: 0.2080 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2260 (2400 samples)\n",
      "Validation Loss: 0.1985 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.2169 (2400 samples)\n",
      "Validation Loss: 0.1896 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.2082 (2400 samples)\n",
      "Validation Loss: 0.1813 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.2001 (2400 samples)\n",
      "Validation Loss: 0.1735 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.1923 (2400 samples)\n",
      "Validation Loss: 0.1661 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.1849 (2400 samples)\n",
      "Validation Loss: 0.1592 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.1779 (2400 samples)\n",
      "Validation Loss: 0.1527 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.1712 (2400 samples)\n",
      "Validation Loss: 0.1466 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1649 (2400 samples)\n",
      "Validation Loss: 0.1408 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1588 (2400 samples)\n",
      "Validation Loss: 0.1353 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1531 (2400 samples)\n",
      "Validation Loss: 0.1301 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1477 (2400 samples)\n",
      "Validation Loss: 0.1252 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1425 (2400 samples)\n",
      "Validation Loss: 0.1205 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1376 (2400 samples)\n",
      "Validation Loss: 0.1161 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1330 (2400 samples)\n",
      "Validation Loss: 0.1119 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1285 (2400 samples)\n",
      "Validation Loss: 0.1079 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1243 (2400 samples)\n",
      "Validation Loss: 0.1041 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1203 (2400 samples)\n",
      "Validation Loss: 0.1004 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1165 (2400 samples)\n",
      "Validation Loss: 0.0970 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1129 (2400 samples)\n",
      "Validation Loss: 0.0937 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1094 (2400 samples)\n",
      "Validation Loss: 0.0906 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.1061 (2400 samples)\n",
      "Validation Loss: 0.0876 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.1029 (2400 samples)\n",
      "Validation Loss: 0.0848 (150 samples)\n",
      "Training Loss: [1.0721025727215183, 1.0114910760785993, 0.9527667603052642, 0.892620047965431, 0.8315222906136024, 0.7713972128298888, 0.7142394915086456, 0.6614384714456057, 0.6136392482542955, 0.5708543696033461, 0.5327132668435904, 0.4987206560728415, 0.46833899865063927, 0.4410674876299865, 0.41646829043167305, 0.39414971962301676, 0.37377987291943743, 0.35507236378482226, 0.3378190853769261, 0.3218422568804909, 0.3069805851662543, 0.2931024412765911, 0.2801164095720379, 0.26794119052451476, 0.2564979049462237, 0.24572525823029234, 0.2355655402434108, 0.225959094997202, 0.21685978771089476, 0.20824374419719555, 0.20007538714835374, 0.1923225504848006, 0.18494781197208793, 0.17792978109172874, 0.17124815308607064, 0.16489031737003298, 0.158848748173855, 0.15311380469072697, 0.14767119343706092, 0.14250996238716404, 0.13761434156920094, 0.13296630998393663, 0.12854788162912056, 0.12434204280251704, 0.12033470368921238, 0.11651648625485853, 0.11287489449972796, 0.10939613671113402, 0.1060745343015949, 0.10290101518443373]\n",
      "Validation Losses: [1.0518786878784678, 1.0027177635168438, 0.9519656775683444, 0.8981564837658336, 0.8420686652973998, 0.7851377379004565, 0.7293550711870145, 0.6760354376320225, 0.6261525635339045, 0.5801378707294654, 0.538136592468789, 0.4999712656530981, 0.46538104788288187, 0.4340692591116224, 0.4057165069282305, 0.3799776364077216, 0.3566441017440523, 0.33543285968523395, 0.3160979005812784, 0.298383650847746, 0.2821738074571377, 0.2672877747089116, 0.25356343152013017, 0.24088092166419642, 0.22911434694448957, 0.21818122544440774, 0.20800623998583856, 0.19850635277452697, 0.18961154114888476, 0.18128471393543136, 0.17346781038589887, 0.16611967800660188, 0.15920821642393476, 0.15270097162732205, 0.14656285347118472, 0.1407585523700455, 0.13527517838450037, 0.13008675818131857, 0.1251673411361786, 0.1205002819910266, 0.11607343027124695, 0.11186413570077602, 0.10786561322386265, 0.10406431626607543, 0.10044485979685219, 0.09699743417585935, 0.09371442530032104, 0.09058985954315973, 0.08761455842809379, 0.08477958476099769]\n",
      "Hyperparameters: lr=0.01, dropout=0.7, embedding_dim=200\n",
      "Testing with learning rate: 0.01, dropout rate: 0.7, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 1.0746 (2400 samples)\n",
      "Validation Loss: 1.0716 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.9802 (2400 samples)\n",
      "Validation Loss: 0.9981 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.9006 (2400 samples)\n",
      "Validation Loss: 0.9280 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.8280 (2400 samples)\n",
      "Validation Loss: 0.8601 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.7617 (2400 samples)\n",
      "Validation Loss: 0.7948 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.7017 (2400 samples)\n",
      "Validation Loss: 0.7328 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.6480 (2400 samples)\n",
      "Validation Loss: 0.6749 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.6001 (2400 samples)\n",
      "Validation Loss: 0.6213 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.5575 (2400 samples)\n",
      "Validation Loss: 0.5725 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.5197 (2400 samples)\n",
      "Validation Loss: 0.5284 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.4859 (2400 samples)\n",
      "Validation Loss: 0.4887 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.4558 (2400 samples)\n",
      "Validation Loss: 0.4532 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.4287 (2400 samples)\n",
      "Validation Loss: 0.4213 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.4042 (2400 samples)\n",
      "Validation Loss: 0.3928 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.3820 (2400 samples)\n",
      "Validation Loss: 0.3672 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.3617 (2400 samples)\n",
      "Validation Loss: 0.3442 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.3430 (2400 samples)\n",
      "Validation Loss: 0.3235 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.3258 (2400 samples)\n",
      "Validation Loss: 0.3047 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.3099 (2400 samples)\n",
      "Validation Loss: 0.2877 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.2952 (2400 samples)\n",
      "Validation Loss: 0.2721 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.2814 (2400 samples)\n",
      "Validation Loss: 0.2580 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.2685 (2400 samples)\n",
      "Validation Loss: 0.2450 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.2565 (2400 samples)\n",
      "Validation Loss: 0.2330 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.2452 (2400 samples)\n",
      "Validation Loss: 0.2220 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.2346 (2400 samples)\n",
      "Validation Loss: 0.2118 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.2246 (2400 samples)\n",
      "Validation Loss: 0.2023 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.2151 (2400 samples)\n",
      "Validation Loss: 0.1934 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.2063 (2400 samples)\n",
      "Validation Loss: 0.1851 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.1979 (2400 samples)\n",
      "Validation Loss: 0.1774 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.1900 (2400 samples)\n",
      "Validation Loss: 0.1701 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.1825 (2400 samples)\n",
      "Validation Loss: 0.1632 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.1754 (2400 samples)\n",
      "Validation Loss: 0.1567 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.1687 (2400 samples)\n",
      "Validation Loss: 0.1506 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.1624 (2400 samples)\n",
      "Validation Loss: 0.1448 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.1564 (2400 samples)\n",
      "Validation Loss: 0.1393 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.1507 (2400 samples)\n",
      "Validation Loss: 0.1341 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.1453 (2400 samples)\n",
      "Validation Loss: 0.1291 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.1401 (2400 samples)\n",
      "Validation Loss: 0.1244 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.1352 (2400 samples)\n",
      "Validation Loss: 0.1200 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.1306 (2400 samples)\n",
      "Validation Loss: 0.1157 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.1262 (2400 samples)\n",
      "Validation Loss: 0.1117 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.1220 (2400 samples)\n",
      "Validation Loss: 0.1079 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.1180 (2400 samples)\n",
      "Validation Loss: 0.1042 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.1141 (2400 samples)\n",
      "Validation Loss: 0.1008 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.1105 (2400 samples)\n",
      "Validation Loss: 0.0975 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.1070 (2400 samples)\n",
      "Validation Loss: 0.0943 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.1037 (2400 samples)\n",
      "Validation Loss: 0.0913 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.1005 (2400 samples)\n",
      "Validation Loss: 0.0884 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0975 (2400 samples)\n",
      "Validation Loss: 0.0856 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0946 (2400 samples)\n",
      "Validation Loss: 0.0830 (150 samples)\n",
      "Training Loss: [1.0746493170483182, 0.9801602202651408, 0.9006061261472047, 0.8280042053061997, 0.7616692461942952, 0.7017027350815236, 0.6479588297789388, 0.6000744559063865, 0.5575147913232508, 0.5196657140242174, 0.48593438860959776, 0.4557661384963896, 0.42866577136653566, 0.40418203354230586, 0.3819519293414173, 0.3616525282441643, 0.3430207369065054, 0.3258380755264544, 0.3099304047783234, 0.29515650937407983, 0.2813913397848881, 0.2685269956044738, 0.2564808449368942, 0.24517577330261953, 0.23455009523191345, 0.22455353928237917, 0.2151324291298471, 0.20625024204332207, 0.19787028482428998, 0.18995826680751246, 0.1824805830750372, 0.17540965800660072, 0.16871662995489872, 0.16237668194127772, 0.15636734182745154, 0.15066923709884483, 0.14526092020889647, 0.14012665235368707, 0.13524959497879885, 0.13061163655909722, 0.12619197303865956, 0.12198123701941582, 0.11797106646871597, 0.1141489976767955, 0.11050430005517138, 0.10702678541519006, 0.10370619777524388, 0.10053269682719296, 0.09750010456237677, 0.09459898743072205]\n",
      "Validation Losses: [1.0715594929727454, 0.9981024154139063, 0.9279710257505301, 0.8600993377812409, 0.7947929850933728, 0.732827250151053, 0.67485012205703, 0.6212806360607266, 0.5724671626733733, 0.5283617648207302, 0.48873105293464597, 0.4531866033856706, 0.4213331477026942, 0.3928049190238619, 0.3672278270528792, 0.3442267286261588, 0.3234579206200379, 0.30468386403672865, 0.2876682810969875, 0.27214837227156335, 0.2579714140497427, 0.24497904650461114, 0.23302597328548297, 0.22199358647159018, 0.21177919730910136, 0.2022801856039154, 0.19341932850170587, 0.18513257448318013, 0.17737280389207918, 0.17006684035748884, 0.16318484810617703, 0.1566943851084582, 0.15056198674295293, 0.14476043781055184, 0.1392679466054313, 0.13405651486346762, 0.1291148507669905, 0.12442453227348788, 0.1199740087276522, 0.115747859353642, 0.11172512698926558, 0.10789441866826067, 0.1042484679109073, 0.1007757791558336, 0.09746272440259933, 0.09429486219613412, 0.09127387397809182, 0.08839028976261716, 0.08563636779889236, 0.08300400529861533]\n",
      "Hyperparameters: lr=0.01, dropout=0.7, embedding_dim=300\n",
      "Testing with learning rate: 0.1, dropout rate: 0.3, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 0.9234 (2400 samples)\n",
      "Validation Loss: 0.7375 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.5280 (2400 samples)\n",
      "Validation Loss: 0.4052 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.3344 (2400 samples)\n",
      "Validation Loss: 0.2576 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.2304 (2400 samples)\n",
      "Validation Loss: 0.1751 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1645 (2400 samples)\n",
      "Validation Loss: 0.1231 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.1208 (2400 samples)\n",
      "Validation Loss: 0.0900 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0915 (2400 samples)\n",
      "Validation Loss: 0.0681 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0713 (2400 samples)\n",
      "Validation Loss: 0.0526 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0571 (2400 samples)\n",
      "Validation Loss: 0.0409 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0468 (2400 samples)\n",
      "Validation Loss: 0.0319 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0391 (2400 samples)\n",
      "Validation Loss: 0.0253 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0333 (2400 samples)\n",
      "Validation Loss: 0.0207 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0289 (2400 samples)\n",
      "Validation Loss: 0.0174 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0254 (2400 samples)\n",
      "Validation Loss: 0.0149 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0226 (2400 samples)\n",
      "Validation Loss: 0.0130 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0203 (2400 samples)\n",
      "Validation Loss: 0.0114 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0184 (2400 samples)\n",
      "Validation Loss: 0.0102 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0168 (2400 samples)\n",
      "Validation Loss: 0.0092 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0154 (2400 samples)\n",
      "Validation Loss: 0.0083 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0142 (2400 samples)\n",
      "Validation Loss: 0.0076 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0132 (2400 samples)\n",
      "Validation Loss: 0.0070 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0123 (2400 samples)\n",
      "Validation Loss: 0.0065 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0115 (2400 samples)\n",
      "Validation Loss: 0.0060 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0107 (2400 samples)\n",
      "Validation Loss: 0.0056 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0101 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0095 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0090 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0085 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0080 (2400 samples)\n",
      "Validation Loss: 0.0041 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0076 (2400 samples)\n",
      "Validation Loss: 0.0039 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0073 (2400 samples)\n",
      "Validation Loss: 0.0037 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0069 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0066 (2400 samples)\n",
      "Validation Loss: 0.0034 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0064 (2400 samples)\n",
      "Validation Loss: 0.0032 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0061 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0058 (2400 samples)\n",
      "Validation Loss: 0.0030 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0045 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0041 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0037 (2400 samples)\n",
      "Validation Loss: 0.0018 (150 samples)\n",
      "Training Loss: [0.9233531191617823, 0.5279854764286411, 0.3343940155868618, 0.23037959092243063, 0.16448739011601043, 0.12081039646408426, 0.09150883751772924, 0.07133441685705177, 0.057132020001334036, 0.0467828548515092, 0.039087812130494344, 0.03329362252402053, 0.028855054189233068, 0.02538130104297453, 0.0225986054613068, 0.02032625160801664, 0.01842877240560706, 0.01682082230514478, 0.015441365846607998, 0.01424451724124545, 0.013198181821513279, 0.01227708698451981, 0.011460732708499208, 0.010731641959360006, 0.010078035252227299, 0.00949037772048119, 0.008960695263831413, 0.008480408651155315, 0.008043926694968654, 0.007645943728187704, 0.007281786279508428, 0.006947209003940225, 0.006639350200927513, 0.006355367327259595, 0.006092124920349054, 0.005847945773080245, 0.005620759049070298, 0.005408811521864925, 0.00521083813554425, 0.005025629113043397, 0.0048524762720977485, 0.004690018169606894, 0.004536725425601026, 0.0043923365316097165, 0.0042561252133033415, 0.00412740851958921, 0.004005613197757125, 0.003890212528250902, 0.0037807593152037328, 0.00367678170042292]\n",
      "Validation Losses: [0.7374641801451686, 0.405200719927367, 0.25764692394398975, 0.17508924483799454, 0.12306185480775696, 0.09000862570601258, 0.0680791507344097, 0.052593282202203516, 0.04088505165822568, 0.03188334077533065, 0.02526586292004013, 0.020674432104327824, 0.017364117729804528, 0.014876820962349742, 0.012956470649006721, 0.011436341567258855, 0.010206158558952513, 0.009192506697559201, 0.008344446010488058, 0.007625884488263714, 0.007009987908797554, 0.006481401683254721, 0.006018208854204024, 0.005610123930356884, 0.005248299498008998, 0.00492560004548665, 0.0046362542196133364, 0.004375612530175335, 0.004139653802025274, 0.003925272832631543, 0.0037297737724063793, 0.0035509999048720423, 0.0033869918364738036, 0.003236088824016062, 0.0030968966349319573, 0.0029679212972912577, 0.002848270021019465, 0.00273700976331485, 0.002633317678088973, 0.0025364788626743226, 0.0024457850701204917, 0.002360603987882211, 0.00228057462754799, 0.002205354951819158, 0.0021345061434510436, 0.0020676914729955546, 0.0020045685585637686, 0.0019448434516041021, 0.0018882655718248345, 0.0018345811906625336]\n",
      "Hyperparameters: lr=0.1, dropout=0.3, embedding_dim=100\n",
      "Testing with learning rate: 0.1, dropout rate: 0.3, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 0.8543 (2400 samples)\n",
      "Validation Loss: 0.6419 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.4607 (2400 samples)\n",
      "Validation Loss: 0.3488 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.2928 (2400 samples)\n",
      "Validation Loss: 0.2165 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.1989 (2400 samples)\n",
      "Validation Loss: 0.1442 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1397 (2400 samples)\n",
      "Validation Loss: 0.1008 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.1026 (2400 samples)\n",
      "Validation Loss: 0.0723 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0778 (2400 samples)\n",
      "Validation Loss: 0.0544 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0613 (2400 samples)\n",
      "Validation Loss: 0.0424 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0496 (2400 samples)\n",
      "Validation Loss: 0.0338 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0411 (2400 samples)\n",
      "Validation Loss: 0.0276 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0349 (2400 samples)\n",
      "Validation Loss: 0.0230 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0301 (2400 samples)\n",
      "Validation Loss: 0.0195 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0265 (2400 samples)\n",
      "Validation Loss: 0.0168 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0235 (2400 samples)\n",
      "Validation Loss: 0.0147 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0212 (2400 samples)\n",
      "Validation Loss: 0.0129 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0192 (2400 samples)\n",
      "Validation Loss: 0.0116 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0176 (2400 samples)\n",
      "Validation Loss: 0.0104 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0161 (2400 samples)\n",
      "Validation Loss: 0.0094 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0149 (2400 samples)\n",
      "Validation Loss: 0.0086 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0139 (2400 samples)\n",
      "Validation Loss: 0.0079 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0129 (2400 samples)\n",
      "Validation Loss: 0.0073 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0121 (2400 samples)\n",
      "Validation Loss: 0.0068 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0113 (2400 samples)\n",
      "Validation Loss: 0.0063 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0107 (2400 samples)\n",
      "Validation Loss: 0.0059 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0101 (2400 samples)\n",
      "Validation Loss: 0.0055 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0095 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0090 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0086 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0082 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0078 (2400 samples)\n",
      "Validation Loss: 0.0042 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0074 (2400 samples)\n",
      "Validation Loss: 0.0040 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0071 (2400 samples)\n",
      "Validation Loss: 0.0038 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0068 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0065 (2400 samples)\n",
      "Validation Loss: 0.0035 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0063 (2400 samples)\n",
      "Validation Loss: 0.0033 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0060 (2400 samples)\n",
      "Validation Loss: 0.0032 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0058 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0029 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0046 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0042 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Training Loss: [0.854281985545311, 0.4607366005124219, 0.2928228812268111, 0.19893922022405544, 0.13971697912040948, 0.10263784821061735, 0.07781053239995185, 0.0613020834046727, 0.04959746062233629, 0.041115743340696426, 0.03487354079678537, 0.030144711613692435, 0.026467808256892125, 0.02354544799629622, 0.02117291622026689, 0.01920807874453711, 0.017553651443514225, 0.01614028495194123, 0.014918311978358148, 0.013851555243961994, 0.012911660746932311, 0.012078076296994877, 0.011334535639934877, 0.010667546592519716, 0.010066290513736215, 0.009521641706945104, 0.009026017632631868, 0.008572892331269434, 0.008157598338627097, 0.007776124617148859, 0.007424659670022315, 0.007099966022228124, 0.006799061533152479, 0.00651992549036062, 0.00626039614303591, 0.006018484142647863, 0.005792527130712348, 0.0055810126777948565, 0.005382546320313811, 0.005196007216521931, 0.0050204461889890644, 0.0048550126037023855, 0.004698991681893182, 0.004551638996578317, 0.004412266694218002, 0.004280298488700972, 0.0041551691260958215, 0.004036428093249813, 0.003923526785746067, 0.003816148433356654]\n",
      "Validation Losses: [0.6418970606343735, 0.34876291876149673, 0.21647708664886675, 0.14424842661034176, 0.1007683233586255, 0.07229881364785548, 0.05443625255105267, 0.04237144944494639, 0.03377903633817761, 0.027569862236936065, 0.022975199231555946, 0.019496015744255694, 0.016787101841005448, 0.01465339790966653, 0.012947466061842274, 0.011556075620606154, 0.010406655795619925, 0.009442943239084546, 0.008625962607011898, 0.00792598796439164, 0.00732022821845239, 0.006790177716972679, 0.006323613729925276, 0.005909320396365513, 0.0055410665011462225, 0.00521160655398979, 0.004915162642374274, 0.004647067066227142, 0.004403927384276045, 0.0041827041764655685, 0.00398055880160202, 0.0037950632882236077, 0.003624442714601719, 0.003466962409478307, 0.003321337153339214, 0.0031863227205090397, 0.0030607309051907815, 0.002943656221777342, 0.002834331304274833, 0.002732053227721606, 0.00263624244921441, 0.002546273492963086, 0.0024616585125175716, 0.0023819196310269535, 0.0023066544760246407, 0.002235541338822472, 0.002168272887551775, 0.0021045620505337854, 0.002044140451020384, 0.0019867816733254856]\n",
      "Hyperparameters: lr=0.1, dropout=0.3, embedding_dim=200\n",
      "Testing with learning rate: 0.1, dropout rate: 0.3, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 0.8119 (2400 samples)\n",
      "Validation Loss: 0.5826 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.4240 (2400 samples)\n",
      "Validation Loss: 0.3222 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.2669 (2400 samples)\n",
      "Validation Loss: 0.2077 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.1792 (2400 samples)\n",
      "Validation Loss: 0.1420 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1262 (2400 samples)\n",
      "Validation Loss: 0.0993 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.0927 (2400 samples)\n",
      "Validation Loss: 0.0726 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0709 (2400 samples)\n",
      "Validation Loss: 0.0547 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0559 (2400 samples)\n",
      "Validation Loss: 0.0423 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0454 (2400 samples)\n",
      "Validation Loss: 0.0334 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0379 (2400 samples)\n",
      "Validation Loss: 0.0270 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0323 (2400 samples)\n",
      "Validation Loss: 0.0223 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0280 (2400 samples)\n",
      "Validation Loss: 0.0188 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0247 (2400 samples)\n",
      "Validation Loss: 0.0161 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0220 (2400 samples)\n",
      "Validation Loss: 0.0141 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0198 (2400 samples)\n",
      "Validation Loss: 0.0124 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0180 (2400 samples)\n",
      "Validation Loss: 0.0110 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0165 (2400 samples)\n",
      "Validation Loss: 0.0099 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0151 (2400 samples)\n",
      "Validation Loss: 0.0090 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0140 (2400 samples)\n",
      "Validation Loss: 0.0082 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0130 (2400 samples)\n",
      "Validation Loss: 0.0075 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0121 (2400 samples)\n",
      "Validation Loss: 0.0069 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0113 (2400 samples)\n",
      "Validation Loss: 0.0064 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0106 (2400 samples)\n",
      "Validation Loss: 0.0060 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0100 (2400 samples)\n",
      "Validation Loss: 0.0056 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0094 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0089 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0084 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0080 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0076 (2400 samples)\n",
      "Validation Loss: 0.0042 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0073 (2400 samples)\n",
      "Validation Loss: 0.0039 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0070 (2400 samples)\n",
      "Validation Loss: 0.0037 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0066 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0064 (2400 samples)\n",
      "Validation Loss: 0.0034 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0061 (2400 samples)\n",
      "Validation Loss: 0.0033 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0059 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0030 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0029 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0046 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0042 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0037 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0036 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Training Loss: [0.8118913906748335, 0.42395818705039157, 0.2668560977902136, 0.1791931058924279, 0.1262391458317742, 0.09265945336928359, 0.07086475650572673, 0.055873864130512534, 0.045420118860825975, 0.03787647533486543, 0.03228376157101213, 0.028019071931114917, 0.024679675581181632, 0.02200672511398424, 0.019819643267078132, 0.01799597598977858, 0.016451091582457583, 0.015126745338414885, 0.013979155187245558, 0.012976855977827521, 0.012094515976982323, 0.011312143332475811, 0.010614300418256048, 0.009988201920438294, 0.009423850568263666, 0.008913310427522924, 0.00844901565665763, 0.008025511479719449, 0.0076381017836789885, 0.007282440307549995, 0.006953939536446659, 0.0066499990436732995, 0.006368736447095148, 0.006107870035552055, 0.00586544486788397, 0.005639945355951379, 0.0054296555060107604, 0.00523324879438739, 0.005049217116792522, 0.004876572470330487, 0.004714515880427312, 0.004562035196357594, 0.0044182894221976654, 0.004282599733761833, 0.004154342619159129, 0.0040328982973436055, 0.003917671640383952, 0.0038080969586928052, 0.0037039545629681124, 0.0036048494404376363]\n",
      "Validation Losses: [0.5826063440265196, 0.32221540122090037, 0.20774829251763352, 0.14196930971794905, 0.09934137345169264, 0.07261095874131177, 0.05472735191878166, 0.04234939671181471, 0.0334228086645119, 0.02700066815624902, 0.02230537125210602, 0.018816631954590813, 0.01614600064210178, 0.014061682233457178, 0.012400764310600849, 0.011048926014377696, 0.009929342869897228, 0.008992607646056822, 0.008200223737571056, 0.007522769630045871, 0.006937481983138866, 0.006427631425243872, 0.005980400894475637, 0.00558526038940495, 0.00523388161768219, 0.0049196205663321925, 0.004637146637429795, 0.004382196690396195, 0.004151184769938181, 0.003941001216719806, 0.0037489272603180267, 0.0035729427768227302, 0.003411129777300435, 0.003261986645691328, 0.0031241472695379365, 0.0029963853838039472, 0.002877603667235935, 0.002766761876331419, 0.002663351134796778, 0.0025667179367636402, 0.002476222358058262, 0.002391309649393074, 0.0023114790640510834, 0.0022362997701354135, 0.0021654000724523194, 0.0020985035425938304, 0.0020352507211202756, 0.0019753761006900206, 0.001918606630631948, 0.0018647406756828104]\n",
      "Hyperparameters: lr=0.1, dropout=0.3, embedding_dim=300\n",
      "Testing with learning rate: 0.1, dropout rate: 0.5, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 0.9234 (2400 samples)\n",
      "Validation Loss: 0.7375 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.5280 (2400 samples)\n",
      "Validation Loss: 0.4052 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.3344 (2400 samples)\n",
      "Validation Loss: 0.2576 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.2304 (2400 samples)\n",
      "Validation Loss: 0.1751 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1645 (2400 samples)\n",
      "Validation Loss: 0.1231 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.1208 (2400 samples)\n",
      "Validation Loss: 0.0900 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0915 (2400 samples)\n",
      "Validation Loss: 0.0681 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0713 (2400 samples)\n",
      "Validation Loss: 0.0526 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0571 (2400 samples)\n",
      "Validation Loss: 0.0409 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0468 (2400 samples)\n",
      "Validation Loss: 0.0319 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0391 (2400 samples)\n",
      "Validation Loss: 0.0253 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0333 (2400 samples)\n",
      "Validation Loss: 0.0207 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0289 (2400 samples)\n",
      "Validation Loss: 0.0174 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0254 (2400 samples)\n",
      "Validation Loss: 0.0149 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0226 (2400 samples)\n",
      "Validation Loss: 0.0130 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0203 (2400 samples)\n",
      "Validation Loss: 0.0114 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0184 (2400 samples)\n",
      "Validation Loss: 0.0102 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0168 (2400 samples)\n",
      "Validation Loss: 0.0092 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0154 (2400 samples)\n",
      "Validation Loss: 0.0083 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0142 (2400 samples)\n",
      "Validation Loss: 0.0076 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0132 (2400 samples)\n",
      "Validation Loss: 0.0070 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0123 (2400 samples)\n",
      "Validation Loss: 0.0065 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0115 (2400 samples)\n",
      "Validation Loss: 0.0060 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0107 (2400 samples)\n",
      "Validation Loss: 0.0056 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0101 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0095 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0090 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0085 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0080 (2400 samples)\n",
      "Validation Loss: 0.0041 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0076 (2400 samples)\n",
      "Validation Loss: 0.0039 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0073 (2400 samples)\n",
      "Validation Loss: 0.0037 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0069 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0066 (2400 samples)\n",
      "Validation Loss: 0.0034 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0064 (2400 samples)\n",
      "Validation Loss: 0.0032 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0061 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0058 (2400 samples)\n",
      "Validation Loss: 0.0030 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0045 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0041 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0037 (2400 samples)\n",
      "Validation Loss: 0.0018 (150 samples)\n",
      "Training Loss: [0.9233531191617823, 0.5279854764286411, 0.3343940155868618, 0.23037959092243063, 0.16448739011601043, 0.12081039646408426, 0.09150883751772924, 0.07133441685705177, 0.057132020001334036, 0.0467828548515092, 0.039087812130494344, 0.03329362252402053, 0.028855054189233068, 0.02538130104297453, 0.0225986054613068, 0.02032625160801664, 0.01842877240560706, 0.01682082230514478, 0.015441365846607998, 0.01424451724124545, 0.013198181821513279, 0.01227708698451981, 0.011460732708499208, 0.010731641959360006, 0.010078035252227299, 0.00949037772048119, 0.008960695263831413, 0.008480408651155315, 0.008043926694968654, 0.007645943728187704, 0.007281786279508428, 0.006947209003940225, 0.006639350200927513, 0.006355367327259595, 0.006092124920349054, 0.005847945773080245, 0.005620759049070298, 0.005408811521864925, 0.00521083813554425, 0.005025629113043397, 0.0048524762720977485, 0.004690018169606894, 0.004536725425601026, 0.0043923365316097165, 0.0042561252133033415, 0.00412740851958921, 0.004005613197757125, 0.003890212528250902, 0.0037807593152037328, 0.00367678170042292]\n",
      "Validation Losses: [0.7374641801451686, 0.405200719927367, 0.25764692394398975, 0.17508924483799454, 0.12306185480775696, 0.09000862570601258, 0.0680791507344097, 0.052593282202203516, 0.04088505165822568, 0.03188334077533065, 0.02526586292004013, 0.020674432104327824, 0.017364117729804528, 0.014876820962349742, 0.012956470649006721, 0.011436341567258855, 0.010206158558952513, 0.009192506697559201, 0.008344446010488058, 0.007625884488263714, 0.007009987908797554, 0.006481401683254721, 0.006018208854204024, 0.005610123930356884, 0.005248299498008998, 0.00492560004548665, 0.0046362542196133364, 0.004375612530175335, 0.004139653802025274, 0.003925272832631543, 0.0037297737724063793, 0.0035509999048720423, 0.0033869918364738036, 0.003236088824016062, 0.0030968966349319573, 0.0029679212972912577, 0.002848270021019465, 0.00273700976331485, 0.002633317678088973, 0.0025364788626743226, 0.0024457850701204917, 0.002360603987882211, 0.00228057462754799, 0.002205354951819158, 0.0021345061434510436, 0.0020676914729955546, 0.0020045685585637686, 0.0019448434516041021, 0.0018882655718248345, 0.0018345811906625336]\n",
      "Hyperparameters: lr=0.1, dropout=0.5, embedding_dim=100\n",
      "Testing with learning rate: 0.1, dropout rate: 0.5, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 0.8543 (2400 samples)\n",
      "Validation Loss: 0.6419 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.4607 (2400 samples)\n",
      "Validation Loss: 0.3488 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.2928 (2400 samples)\n",
      "Validation Loss: 0.2165 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.1989 (2400 samples)\n",
      "Validation Loss: 0.1442 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1397 (2400 samples)\n",
      "Validation Loss: 0.1008 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.1026 (2400 samples)\n",
      "Validation Loss: 0.0723 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0778 (2400 samples)\n",
      "Validation Loss: 0.0544 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0613 (2400 samples)\n",
      "Validation Loss: 0.0424 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0496 (2400 samples)\n",
      "Validation Loss: 0.0338 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0411 (2400 samples)\n",
      "Validation Loss: 0.0276 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0349 (2400 samples)\n",
      "Validation Loss: 0.0230 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0301 (2400 samples)\n",
      "Validation Loss: 0.0195 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0265 (2400 samples)\n",
      "Validation Loss: 0.0168 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0235 (2400 samples)\n",
      "Validation Loss: 0.0147 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0212 (2400 samples)\n",
      "Validation Loss: 0.0129 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0192 (2400 samples)\n",
      "Validation Loss: 0.0116 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0176 (2400 samples)\n",
      "Validation Loss: 0.0104 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0161 (2400 samples)\n",
      "Validation Loss: 0.0094 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0149 (2400 samples)\n",
      "Validation Loss: 0.0086 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0139 (2400 samples)\n",
      "Validation Loss: 0.0079 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0129 (2400 samples)\n",
      "Validation Loss: 0.0073 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0121 (2400 samples)\n",
      "Validation Loss: 0.0068 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0113 (2400 samples)\n",
      "Validation Loss: 0.0063 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0107 (2400 samples)\n",
      "Validation Loss: 0.0059 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0101 (2400 samples)\n",
      "Validation Loss: 0.0055 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0095 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0090 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0086 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0082 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0078 (2400 samples)\n",
      "Validation Loss: 0.0042 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0074 (2400 samples)\n",
      "Validation Loss: 0.0040 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0071 (2400 samples)\n",
      "Validation Loss: 0.0038 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0068 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0065 (2400 samples)\n",
      "Validation Loss: 0.0035 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0063 (2400 samples)\n",
      "Validation Loss: 0.0033 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0060 (2400 samples)\n",
      "Validation Loss: 0.0032 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0058 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0029 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0046 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0042 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Training Loss: [0.854281985545311, 0.4607366005124219, 0.2928228812268111, 0.19893922022405544, 0.13971697912040948, 0.10263784821061735, 0.07781053239995185, 0.0613020834046727, 0.04959746062233629, 0.041115743340696426, 0.03487354079678537, 0.030144711613692435, 0.026467808256892125, 0.02354544799629622, 0.02117291622026689, 0.01920807874453711, 0.017553651443514225, 0.01614028495194123, 0.014918311978358148, 0.013851555243961994, 0.012911660746932311, 0.012078076296994877, 0.011334535639934877, 0.010667546592519716, 0.010066290513736215, 0.009521641706945104, 0.009026017632631868, 0.008572892331269434, 0.008157598338627097, 0.007776124617148859, 0.007424659670022315, 0.007099966022228124, 0.006799061533152479, 0.00651992549036062, 0.00626039614303591, 0.006018484142647863, 0.005792527130712348, 0.0055810126777948565, 0.005382546320313811, 0.005196007216521931, 0.0050204461889890644, 0.0048550126037023855, 0.004698991681893182, 0.004551638996578317, 0.004412266694218002, 0.004280298488700972, 0.0041551691260958215, 0.004036428093249813, 0.003923526785746067, 0.003816148433356654]\n",
      "Validation Losses: [0.6418970606343735, 0.34876291876149673, 0.21647708664886675, 0.14424842661034176, 0.1007683233586255, 0.07229881364785548, 0.05443625255105267, 0.04237144944494639, 0.03377903633817761, 0.027569862236936065, 0.022975199231555946, 0.019496015744255694, 0.016787101841005448, 0.01465339790966653, 0.012947466061842274, 0.011556075620606154, 0.010406655795619925, 0.009442943239084546, 0.008625962607011898, 0.00792598796439164, 0.00732022821845239, 0.006790177716972679, 0.006323613729925276, 0.005909320396365513, 0.0055410665011462225, 0.00521160655398979, 0.004915162642374274, 0.004647067066227142, 0.004403927384276045, 0.0041827041764655685, 0.00398055880160202, 0.0037950632882236077, 0.003624442714601719, 0.003466962409478307, 0.003321337153339214, 0.0031863227205090397, 0.0030607309051907815, 0.002943656221777342, 0.002834331304274833, 0.002732053227721606, 0.00263624244921441, 0.002546273492963086, 0.0024616585125175716, 0.0023819196310269535, 0.0023066544760246407, 0.002235541338822472, 0.002168272887551775, 0.0021045620505337854, 0.002044140451020384, 0.0019867816733254856]\n",
      "Hyperparameters: lr=0.1, dropout=0.5, embedding_dim=200\n",
      "Testing with learning rate: 0.1, dropout rate: 0.5, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 0.8119 (2400 samples)\n",
      "Validation Loss: 0.5826 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.4240 (2400 samples)\n",
      "Validation Loss: 0.3222 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.2669 (2400 samples)\n",
      "Validation Loss: 0.2077 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.1792 (2400 samples)\n",
      "Validation Loss: 0.1420 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1262 (2400 samples)\n",
      "Validation Loss: 0.0993 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.0927 (2400 samples)\n",
      "Validation Loss: 0.0726 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0709 (2400 samples)\n",
      "Validation Loss: 0.0547 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0559 (2400 samples)\n",
      "Validation Loss: 0.0423 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0454 (2400 samples)\n",
      "Validation Loss: 0.0334 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0379 (2400 samples)\n",
      "Validation Loss: 0.0270 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0323 (2400 samples)\n",
      "Validation Loss: 0.0223 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0280 (2400 samples)\n",
      "Validation Loss: 0.0188 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0247 (2400 samples)\n",
      "Validation Loss: 0.0161 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0220 (2400 samples)\n",
      "Validation Loss: 0.0141 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0198 (2400 samples)\n",
      "Validation Loss: 0.0124 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0180 (2400 samples)\n",
      "Validation Loss: 0.0110 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0165 (2400 samples)\n",
      "Validation Loss: 0.0099 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0151 (2400 samples)\n",
      "Validation Loss: 0.0090 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0140 (2400 samples)\n",
      "Validation Loss: 0.0082 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0130 (2400 samples)\n",
      "Validation Loss: 0.0075 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0121 (2400 samples)\n",
      "Validation Loss: 0.0069 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0113 (2400 samples)\n",
      "Validation Loss: 0.0064 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0106 (2400 samples)\n",
      "Validation Loss: 0.0060 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0100 (2400 samples)\n",
      "Validation Loss: 0.0056 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0094 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0089 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0084 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0080 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0076 (2400 samples)\n",
      "Validation Loss: 0.0042 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0073 (2400 samples)\n",
      "Validation Loss: 0.0039 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0070 (2400 samples)\n",
      "Validation Loss: 0.0037 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0066 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0064 (2400 samples)\n",
      "Validation Loss: 0.0034 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0061 (2400 samples)\n",
      "Validation Loss: 0.0033 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0059 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0030 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0029 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0046 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0042 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0037 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0036 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Training Loss: [0.8118913906748335, 0.42395818705039157, 0.2668560977902136, 0.1791931058924279, 0.1262391458317742, 0.09265945336928359, 0.07086475650572673, 0.055873864130512534, 0.045420118860825975, 0.03787647533486543, 0.03228376157101213, 0.028019071931114917, 0.024679675581181632, 0.02200672511398424, 0.019819643267078132, 0.01799597598977858, 0.016451091582457583, 0.015126745338414885, 0.013979155187245558, 0.012976855977827521, 0.012094515976982323, 0.011312143332475811, 0.010614300418256048, 0.009988201920438294, 0.009423850568263666, 0.008913310427522924, 0.00844901565665763, 0.008025511479719449, 0.0076381017836789885, 0.007282440307549995, 0.006953939536446659, 0.0066499990436732995, 0.006368736447095148, 0.006107870035552055, 0.00586544486788397, 0.005639945355951379, 0.0054296555060107604, 0.00523324879438739, 0.005049217116792522, 0.004876572470330487, 0.004714515880427312, 0.004562035196357594, 0.0044182894221976654, 0.004282599733761833, 0.004154342619159129, 0.0040328982973436055, 0.003917671640383952, 0.0038080969586928052, 0.0037039545629681124, 0.0036048494404376363]\n",
      "Validation Losses: [0.5826063440265196, 0.32221540122090037, 0.20774829251763352, 0.14196930971794905, 0.09934137345169264, 0.07261095874131177, 0.05472735191878166, 0.04234939671181471, 0.0334228086645119, 0.02700066815624902, 0.02230537125210602, 0.018816631954590813, 0.01614600064210178, 0.014061682233457178, 0.012400764310600849, 0.011048926014377696, 0.009929342869897228, 0.008992607646056822, 0.008200223737571056, 0.007522769630045871, 0.006937481983138866, 0.006427631425243872, 0.005980400894475637, 0.00558526038940495, 0.00523388161768219, 0.0049196205663321925, 0.004637146637429795, 0.004382196690396195, 0.004151184769938181, 0.003941001216719806, 0.0037489272603180267, 0.0035729427768227302, 0.003411129777300435, 0.003261986645691328, 0.0031241472695379365, 0.0029963853838039472, 0.002877603667235935, 0.002766761876331419, 0.002663351134796778, 0.0025667179367636402, 0.002476222358058262, 0.002391309649393074, 0.0023114790640510834, 0.0022362997701354135, 0.0021654000724523194, 0.0020985035425938304, 0.0020352507211202756, 0.0019753761006900206, 0.001918606630631948, 0.0018647406756828104]\n",
      "Hyperparameters: lr=0.1, dropout=0.5, embedding_dim=300\n",
      "Testing with learning rate: 0.1, dropout rate: 0.7, embedding dim: 100\n",
      "Epoch: 1/50, Train Loss: 0.9234 (2400 samples)\n",
      "Validation Loss: 0.7375 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.5280 (2400 samples)\n",
      "Validation Loss: 0.4052 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.3344 (2400 samples)\n",
      "Validation Loss: 0.2576 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.2304 (2400 samples)\n",
      "Validation Loss: 0.1751 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1645 (2400 samples)\n",
      "Validation Loss: 0.1231 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.1208 (2400 samples)\n",
      "Validation Loss: 0.0900 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0915 (2400 samples)\n",
      "Validation Loss: 0.0681 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0713 (2400 samples)\n",
      "Validation Loss: 0.0526 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0571 (2400 samples)\n",
      "Validation Loss: 0.0409 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0468 (2400 samples)\n",
      "Validation Loss: 0.0319 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0391 (2400 samples)\n",
      "Validation Loss: 0.0253 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0333 (2400 samples)\n",
      "Validation Loss: 0.0207 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0289 (2400 samples)\n",
      "Validation Loss: 0.0174 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0254 (2400 samples)\n",
      "Validation Loss: 0.0149 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0226 (2400 samples)\n",
      "Validation Loss: 0.0130 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0203 (2400 samples)\n",
      "Validation Loss: 0.0114 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0184 (2400 samples)\n",
      "Validation Loss: 0.0102 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0168 (2400 samples)\n",
      "Validation Loss: 0.0092 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0154 (2400 samples)\n",
      "Validation Loss: 0.0083 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0142 (2400 samples)\n",
      "Validation Loss: 0.0076 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0132 (2400 samples)\n",
      "Validation Loss: 0.0070 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0123 (2400 samples)\n",
      "Validation Loss: 0.0065 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0115 (2400 samples)\n",
      "Validation Loss: 0.0060 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0107 (2400 samples)\n",
      "Validation Loss: 0.0056 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0101 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0095 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0090 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0085 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0080 (2400 samples)\n",
      "Validation Loss: 0.0041 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0076 (2400 samples)\n",
      "Validation Loss: 0.0039 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0073 (2400 samples)\n",
      "Validation Loss: 0.0037 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0069 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0066 (2400 samples)\n",
      "Validation Loss: 0.0034 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0064 (2400 samples)\n",
      "Validation Loss: 0.0032 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0061 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0058 (2400 samples)\n",
      "Validation Loss: 0.0030 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0045 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0041 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0037 (2400 samples)\n",
      "Validation Loss: 0.0018 (150 samples)\n",
      "Training Loss: [0.9233531191617823, 0.5279854764286411, 0.3343940155868618, 0.23037959092243063, 0.16448739011601043, 0.12081039646408426, 0.09150883751772924, 0.07133441685705177, 0.057132020001334036, 0.0467828548515092, 0.039087812130494344, 0.03329362252402053, 0.028855054189233068, 0.02538130104297453, 0.0225986054613068, 0.02032625160801664, 0.01842877240560706, 0.01682082230514478, 0.015441365846607998, 0.01424451724124545, 0.013198181821513279, 0.01227708698451981, 0.011460732708499208, 0.010731641959360006, 0.010078035252227299, 0.00949037772048119, 0.008960695263831413, 0.008480408651155315, 0.008043926694968654, 0.007645943728187704, 0.007281786279508428, 0.006947209003940225, 0.006639350200927513, 0.006355367327259595, 0.006092124920349054, 0.005847945773080245, 0.005620759049070298, 0.005408811521864925, 0.00521083813554425, 0.005025629113043397, 0.0048524762720977485, 0.004690018169606894, 0.004536725425601026, 0.0043923365316097165, 0.0042561252133033415, 0.00412740851958921, 0.004005613197757125, 0.003890212528250902, 0.0037807593152037328, 0.00367678170042292]\n",
      "Validation Losses: [0.7374641801451686, 0.405200719927367, 0.25764692394398975, 0.17508924483799454, 0.12306185480775696, 0.09000862570601258, 0.0680791507344097, 0.052593282202203516, 0.04088505165822568, 0.03188334077533065, 0.02526586292004013, 0.020674432104327824, 0.017364117729804528, 0.014876820962349742, 0.012956470649006721, 0.011436341567258855, 0.010206158558952513, 0.009192506697559201, 0.008344446010488058, 0.007625884488263714, 0.007009987908797554, 0.006481401683254721, 0.006018208854204024, 0.005610123930356884, 0.005248299498008998, 0.00492560004548665, 0.0046362542196133364, 0.004375612530175335, 0.004139653802025274, 0.003925272832631543, 0.0037297737724063793, 0.0035509999048720423, 0.0033869918364738036, 0.003236088824016062, 0.0030968966349319573, 0.0029679212972912577, 0.002848270021019465, 0.00273700976331485, 0.002633317678088973, 0.0025364788626743226, 0.0024457850701204917, 0.002360603987882211, 0.00228057462754799, 0.002205354951819158, 0.0021345061434510436, 0.0020676914729955546, 0.0020045685585637686, 0.0019448434516041021, 0.0018882655718248345, 0.0018345811906625336]\n",
      "Hyperparameters: lr=0.1, dropout=0.7, embedding_dim=100\n",
      "Testing with learning rate: 0.1, dropout rate: 0.7, embedding dim: 200\n",
      "Epoch: 1/50, Train Loss: 0.8543 (2400 samples)\n",
      "Validation Loss: 0.6419 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.4607 (2400 samples)\n",
      "Validation Loss: 0.3488 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.2928 (2400 samples)\n",
      "Validation Loss: 0.2165 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.1989 (2400 samples)\n",
      "Validation Loss: 0.1442 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1397 (2400 samples)\n",
      "Validation Loss: 0.1008 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.1026 (2400 samples)\n",
      "Validation Loss: 0.0723 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0778 (2400 samples)\n",
      "Validation Loss: 0.0544 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0613 (2400 samples)\n",
      "Validation Loss: 0.0424 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0496 (2400 samples)\n",
      "Validation Loss: 0.0338 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0411 (2400 samples)\n",
      "Validation Loss: 0.0276 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0349 (2400 samples)\n",
      "Validation Loss: 0.0230 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0301 (2400 samples)\n",
      "Validation Loss: 0.0195 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0265 (2400 samples)\n",
      "Validation Loss: 0.0168 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0235 (2400 samples)\n",
      "Validation Loss: 0.0147 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0212 (2400 samples)\n",
      "Validation Loss: 0.0129 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0192 (2400 samples)\n",
      "Validation Loss: 0.0116 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0176 (2400 samples)\n",
      "Validation Loss: 0.0104 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0161 (2400 samples)\n",
      "Validation Loss: 0.0094 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0149 (2400 samples)\n",
      "Validation Loss: 0.0086 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0139 (2400 samples)\n",
      "Validation Loss: 0.0079 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0129 (2400 samples)\n",
      "Validation Loss: 0.0073 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0121 (2400 samples)\n",
      "Validation Loss: 0.0068 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0113 (2400 samples)\n",
      "Validation Loss: 0.0063 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0107 (2400 samples)\n",
      "Validation Loss: 0.0059 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0101 (2400 samples)\n",
      "Validation Loss: 0.0055 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0095 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0090 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0086 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0082 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0078 (2400 samples)\n",
      "Validation Loss: 0.0042 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0074 (2400 samples)\n",
      "Validation Loss: 0.0040 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0071 (2400 samples)\n",
      "Validation Loss: 0.0038 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0068 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0065 (2400 samples)\n",
      "Validation Loss: 0.0035 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0063 (2400 samples)\n",
      "Validation Loss: 0.0033 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0060 (2400 samples)\n",
      "Validation Loss: 0.0032 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0058 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0029 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0046 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0042 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Training Loss: [0.854281985545311, 0.4607366005124219, 0.2928228812268111, 0.19893922022405544, 0.13971697912040948, 0.10263784821061735, 0.07781053239995185, 0.0613020834046727, 0.04959746062233629, 0.041115743340696426, 0.03487354079678537, 0.030144711613692435, 0.026467808256892125, 0.02354544799629622, 0.02117291622026689, 0.01920807874453711, 0.017553651443514225, 0.01614028495194123, 0.014918311978358148, 0.013851555243961994, 0.012911660746932311, 0.012078076296994877, 0.011334535639934877, 0.010667546592519716, 0.010066290513736215, 0.009521641706945104, 0.009026017632631868, 0.008572892331269434, 0.008157598338627097, 0.007776124617148859, 0.007424659670022315, 0.007099966022228124, 0.006799061533152479, 0.00651992549036062, 0.00626039614303591, 0.006018484142647863, 0.005792527130712348, 0.0055810126777948565, 0.005382546320313811, 0.005196007216521931, 0.0050204461889890644, 0.0048550126037023855, 0.004698991681893182, 0.004551638996578317, 0.004412266694218002, 0.004280298488700972, 0.0041551691260958215, 0.004036428093249813, 0.003923526785746067, 0.003816148433356654]\n",
      "Validation Losses: [0.6418970606343735, 0.34876291876149673, 0.21647708664886675, 0.14424842661034176, 0.1007683233586255, 0.07229881364785548, 0.05443625255105267, 0.04237144944494639, 0.03377903633817761, 0.027569862236936065, 0.022975199231555946, 0.019496015744255694, 0.016787101841005448, 0.01465339790966653, 0.012947466061842274, 0.011556075620606154, 0.010406655795619925, 0.009442943239084546, 0.008625962607011898, 0.00792598796439164, 0.00732022821845239, 0.006790177716972679, 0.006323613729925276, 0.005909320396365513, 0.0055410665011462225, 0.00521160655398979, 0.004915162642374274, 0.004647067066227142, 0.004403927384276045, 0.0041827041764655685, 0.00398055880160202, 0.0037950632882236077, 0.003624442714601719, 0.003466962409478307, 0.003321337153339214, 0.0031863227205090397, 0.0030607309051907815, 0.002943656221777342, 0.002834331304274833, 0.002732053227721606, 0.00263624244921441, 0.002546273492963086, 0.0024616585125175716, 0.0023819196310269535, 0.0023066544760246407, 0.002235541338822472, 0.002168272887551775, 0.0021045620505337854, 0.002044140451020384, 0.0019867816733254856]\n",
      "Hyperparameters: lr=0.1, dropout=0.7, embedding_dim=200\n",
      "Testing with learning rate: 0.1, dropout rate: 0.7, embedding dim: 300\n",
      "Epoch: 1/50, Train Loss: 0.8119 (2400 samples)\n",
      "Validation Loss: 0.5826 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.4240 (2400 samples)\n",
      "Validation Loss: 0.3222 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.2669 (2400 samples)\n",
      "Validation Loss: 0.2077 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.1792 (2400 samples)\n",
      "Validation Loss: 0.1420 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1262 (2400 samples)\n",
      "Validation Loss: 0.0993 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.0927 (2400 samples)\n",
      "Validation Loss: 0.0726 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0709 (2400 samples)\n",
      "Validation Loss: 0.0547 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0559 (2400 samples)\n",
      "Validation Loss: 0.0423 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0454 (2400 samples)\n",
      "Validation Loss: 0.0334 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0379 (2400 samples)\n",
      "Validation Loss: 0.0270 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0323 (2400 samples)\n",
      "Validation Loss: 0.0223 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0280 (2400 samples)\n",
      "Validation Loss: 0.0188 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0247 (2400 samples)\n",
      "Validation Loss: 0.0161 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0220 (2400 samples)\n",
      "Validation Loss: 0.0141 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0198 (2400 samples)\n",
      "Validation Loss: 0.0124 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0180 (2400 samples)\n",
      "Validation Loss: 0.0110 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0165 (2400 samples)\n",
      "Validation Loss: 0.0099 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0151 (2400 samples)\n",
      "Validation Loss: 0.0090 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0140 (2400 samples)\n",
      "Validation Loss: 0.0082 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0130 (2400 samples)\n",
      "Validation Loss: 0.0075 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0121 (2400 samples)\n",
      "Validation Loss: 0.0069 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0113 (2400 samples)\n",
      "Validation Loss: 0.0064 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0106 (2400 samples)\n",
      "Validation Loss: 0.0060 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0100 (2400 samples)\n",
      "Validation Loss: 0.0056 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0094 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0089 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0084 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0080 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0076 (2400 samples)\n",
      "Validation Loss: 0.0042 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0073 (2400 samples)\n",
      "Validation Loss: 0.0039 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0070 (2400 samples)\n",
      "Validation Loss: 0.0037 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0066 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0064 (2400 samples)\n",
      "Validation Loss: 0.0034 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0061 (2400 samples)\n",
      "Validation Loss: 0.0033 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0059 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0030 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0029 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0046 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0042 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0037 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0036 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Training Loss: [0.8118913906748335, 0.42395818705039157, 0.2668560977902136, 0.1791931058924279, 0.1262391458317742, 0.09265945336928359, 0.07086475650572673, 0.055873864130512534, 0.045420118860825975, 0.03787647533486543, 0.03228376157101213, 0.028019071931114917, 0.024679675581181632, 0.02200672511398424, 0.019819643267078132, 0.01799597598977858, 0.016451091582457583, 0.015126745338414885, 0.013979155187245558, 0.012976855977827521, 0.012094515976982323, 0.011312143332475811, 0.010614300418256048, 0.009988201920438294, 0.009423850568263666, 0.008913310427522924, 0.00844901565665763, 0.008025511479719449, 0.0076381017836789885, 0.007282440307549995, 0.006953939536446659, 0.0066499990436732995, 0.006368736447095148, 0.006107870035552055, 0.00586544486788397, 0.005639945355951379, 0.0054296555060107604, 0.00523324879438739, 0.005049217116792522, 0.004876572470330487, 0.004714515880427312, 0.004562035196357594, 0.0044182894221976654, 0.004282599733761833, 0.004154342619159129, 0.0040328982973436055, 0.003917671640383952, 0.0038080969586928052, 0.0037039545629681124, 0.0036048494404376363]\n",
      "Validation Losses: [0.5826063440265196, 0.32221540122090037, 0.20774829251763352, 0.14196930971794905, 0.09934137345169264, 0.07261095874131177, 0.05472735191878166, 0.04234939671181471, 0.0334228086645119, 0.02700066815624902, 0.02230537125210602, 0.018816631954590813, 0.01614600064210178, 0.014061682233457178, 0.012400764310600849, 0.011048926014377696, 0.009929342869897228, 0.008992607646056822, 0.008200223737571056, 0.007522769630045871, 0.006937481983138866, 0.006427631425243872, 0.005980400894475637, 0.00558526038940495, 0.00523388161768219, 0.0049196205663321925, 0.004637146637429795, 0.004382196690396195, 0.004151184769938181, 0.003941001216719806, 0.0037489272603180267, 0.0035729427768227302, 0.003411129777300435, 0.003261986645691328, 0.0031241472695379365, 0.0029963853838039472, 0.002877603667235935, 0.002766761876331419, 0.002663351134796778, 0.0025667179367636402, 0.002476222358058262, 0.002391309649393074, 0.0023114790640510834, 0.0022362997701354135, 0.0021654000724523194, 0.0020985035425938304, 0.0020352507211202756, 0.0019753761006900206, 0.001918606630631948, 0.0018647406756828104]\n",
      "Hyperparameters: lr=0.1, dropout=0.7, embedding_dim=300\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define lists of hyperparameters to test\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "dropout_rates = [0.3, 0.5, 0.7]\n",
    "embedding_dims = [100, 200, 300]  # Different embedding dimensions to test\n",
    "\n",
    "# Initialize dictionaries to store losses for each hyperparameter combination\n",
    "train_loss_dict = {}\n",
    "dev_loss_dict = {}\n",
    "\n",
    "# Iterate over different combinations of hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for embedding_dim in embedding_dims:\n",
    "            print(f\"Testing with learning rate: {lr}, dropout rate: {dropout_rate}, embedding dim: {embedding_dim}\")\n",
    "\n",
    "            # Initialize network weights\n",
    "            W = network_weights(vocab_size=len(word2id), embedding_dim=embedding_dim, hidden_dim=[], num_classes=3)\n",
    "\n",
    "            # Train the network\n",
    "            W, loss_tr, dev_losses = SGD(X_tr=train_sequences, Y_tr=train_y,\n",
    "                                        W=W,\n",
    "                                        X_dev=dev_sequences,\n",
    "                                        Y_dev=dev_y,\n",
    "                                        lr=lr,\n",
    "                                        dropout=dropout_rate,\n",
    "                                        freeze_emb=False,\n",
    "                                        tolerance=1e-12,\n",
    "                                        epochs=50)\n",
    "\n",
    "            # Store losses for this hyperparameter combination\n",
    "            key = (lr, dropout_rate, embedding_dim)\n",
    "            train_loss_dict[key] = loss_tr\n",
    "            dev_loss_dict[key] = dev_losses\n",
    "\n",
    "            # Optionally, you can print or store the results for each combination\n",
    "            print(\"Training Loss:\", loss_tr)\n",
    "            print(\"Validation Losses:\", dev_losses)\n",
    "            print(\"Hyperparameters:\", f\"lr={lr}, dropout={dropout_rate}, embedding_dim={embedding_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (5000, 300)\n",
      "Shape W1 (300, 3)\n",
      "Epoch: 1/50, Train Loss: 0.8119 (2400 samples)\n",
      "Validation Loss: 0.5826 (150 samples)\n",
      "Epoch: 2/50, Train Loss: 0.4240 (2400 samples)\n",
      "Validation Loss: 0.3222 (150 samples)\n",
      "Epoch: 3/50, Train Loss: 0.2669 (2400 samples)\n",
      "Validation Loss: 0.2077 (150 samples)\n",
      "Epoch: 4/50, Train Loss: 0.1792 (2400 samples)\n",
      "Validation Loss: 0.1420 (150 samples)\n",
      "Epoch: 5/50, Train Loss: 0.1262 (2400 samples)\n",
      "Validation Loss: 0.0993 (150 samples)\n",
      "Epoch: 6/50, Train Loss: 0.0927 (2400 samples)\n",
      "Validation Loss: 0.0726 (150 samples)\n",
      "Epoch: 7/50, Train Loss: 0.0709 (2400 samples)\n",
      "Validation Loss: 0.0547 (150 samples)\n",
      "Epoch: 8/50, Train Loss: 0.0559 (2400 samples)\n",
      "Validation Loss: 0.0423 (150 samples)\n",
      "Epoch: 9/50, Train Loss: 0.0454 (2400 samples)\n",
      "Validation Loss: 0.0334 (150 samples)\n",
      "Epoch: 10/50, Train Loss: 0.0379 (2400 samples)\n",
      "Validation Loss: 0.0270 (150 samples)\n",
      "Epoch: 11/50, Train Loss: 0.0323 (2400 samples)\n",
      "Validation Loss: 0.0223 (150 samples)\n",
      "Epoch: 12/50, Train Loss: 0.0280 (2400 samples)\n",
      "Validation Loss: 0.0188 (150 samples)\n",
      "Epoch: 13/50, Train Loss: 0.0247 (2400 samples)\n",
      "Validation Loss: 0.0161 (150 samples)\n",
      "Epoch: 14/50, Train Loss: 0.0220 (2400 samples)\n",
      "Validation Loss: 0.0141 (150 samples)\n",
      "Epoch: 15/50, Train Loss: 0.0198 (2400 samples)\n",
      "Validation Loss: 0.0124 (150 samples)\n",
      "Epoch: 16/50, Train Loss: 0.0180 (2400 samples)\n",
      "Validation Loss: 0.0110 (150 samples)\n",
      "Epoch: 17/50, Train Loss: 0.0165 (2400 samples)\n",
      "Validation Loss: 0.0099 (150 samples)\n",
      "Epoch: 18/50, Train Loss: 0.0151 (2400 samples)\n",
      "Validation Loss: 0.0090 (150 samples)\n",
      "Epoch: 19/50, Train Loss: 0.0140 (2400 samples)\n",
      "Validation Loss: 0.0082 (150 samples)\n",
      "Epoch: 20/50, Train Loss: 0.0130 (2400 samples)\n",
      "Validation Loss: 0.0075 (150 samples)\n",
      "Epoch: 21/50, Train Loss: 0.0121 (2400 samples)\n",
      "Validation Loss: 0.0069 (150 samples)\n",
      "Epoch: 22/50, Train Loss: 0.0113 (2400 samples)\n",
      "Validation Loss: 0.0064 (150 samples)\n",
      "Epoch: 23/50, Train Loss: 0.0106 (2400 samples)\n",
      "Validation Loss: 0.0060 (150 samples)\n",
      "Epoch: 24/50, Train Loss: 0.0100 (2400 samples)\n",
      "Validation Loss: 0.0056 (150 samples)\n",
      "Epoch: 25/50, Train Loss: 0.0094 (2400 samples)\n",
      "Validation Loss: 0.0052 (150 samples)\n",
      "Epoch: 26/50, Train Loss: 0.0089 (2400 samples)\n",
      "Validation Loss: 0.0049 (150 samples)\n",
      "Epoch: 27/50, Train Loss: 0.0084 (2400 samples)\n",
      "Validation Loss: 0.0046 (150 samples)\n",
      "Epoch: 28/50, Train Loss: 0.0080 (2400 samples)\n",
      "Validation Loss: 0.0044 (150 samples)\n",
      "Epoch: 29/50, Train Loss: 0.0076 (2400 samples)\n",
      "Validation Loss: 0.0042 (150 samples)\n",
      "Epoch: 30/50, Train Loss: 0.0073 (2400 samples)\n",
      "Validation Loss: 0.0039 (150 samples)\n",
      "Epoch: 31/50, Train Loss: 0.0070 (2400 samples)\n",
      "Validation Loss: 0.0037 (150 samples)\n",
      "Epoch: 32/50, Train Loss: 0.0066 (2400 samples)\n",
      "Validation Loss: 0.0036 (150 samples)\n",
      "Epoch: 33/50, Train Loss: 0.0064 (2400 samples)\n",
      "Validation Loss: 0.0034 (150 samples)\n",
      "Epoch: 34/50, Train Loss: 0.0061 (2400 samples)\n",
      "Validation Loss: 0.0033 (150 samples)\n",
      "Epoch: 35/50, Train Loss: 0.0059 (2400 samples)\n",
      "Validation Loss: 0.0031 (150 samples)\n",
      "Epoch: 36/50, Train Loss: 0.0056 (2400 samples)\n",
      "Validation Loss: 0.0030 (150 samples)\n",
      "Epoch: 37/50, Train Loss: 0.0054 (2400 samples)\n",
      "Validation Loss: 0.0029 (150 samples)\n",
      "Epoch: 38/50, Train Loss: 0.0052 (2400 samples)\n",
      "Validation Loss: 0.0028 (150 samples)\n",
      "Epoch: 39/50, Train Loss: 0.0050 (2400 samples)\n",
      "Validation Loss: 0.0027 (150 samples)\n",
      "Epoch: 40/50, Train Loss: 0.0049 (2400 samples)\n",
      "Validation Loss: 0.0026 (150 samples)\n",
      "Epoch: 41/50, Train Loss: 0.0047 (2400 samples)\n",
      "Validation Loss: 0.0025 (150 samples)\n",
      "Epoch: 42/50, Train Loss: 0.0046 (2400 samples)\n",
      "Validation Loss: 0.0024 (150 samples)\n",
      "Epoch: 43/50, Train Loss: 0.0044 (2400 samples)\n",
      "Validation Loss: 0.0023 (150 samples)\n",
      "Epoch: 44/50, Train Loss: 0.0043 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 45/50, Train Loss: 0.0042 (2400 samples)\n",
      "Validation Loss: 0.0022 (150 samples)\n",
      "Epoch: 46/50, Train Loss: 0.0040 (2400 samples)\n",
      "Validation Loss: 0.0021 (150 samples)\n",
      "Epoch: 47/50, Train Loss: 0.0039 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 48/50, Train Loss: 0.0038 (2400 samples)\n",
      "Validation Loss: 0.0020 (150 samples)\n",
      "Epoch: 49/50, Train Loss: 0.0037 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n",
      "Epoch: 50/50, Train Loss: 0.0036 (2400 samples)\n",
      "Validation Loss: 0.0019 (150 samples)\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(word2id), embedding_dim=300, hidden_dim=[], num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr=train_sequences, Y_tr=train_y,\n",
    "                            W=W,\n",
    "                            X_dev=dev_sequences,\n",
    "                            Y_dev=dev_y,\n",
    "                            lr=0.1,\n",
    "                            dropout=0.5,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance= 1e-12,\n",
    "                            epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning process\n",
    "plt.plot(range(1, len(loss_tr) + 1), loss_tr, label='Training Loss')\n",
    "plt.plot(range(1, len(dev_loss) + 1), dev_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Process')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['pred']) \n",
    "            for x,y in zip(test_sequences,test_y)]\n",
    "preds_te_adjusted = [pred + 1 for pred in preds_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8322222222222222\n",
      "Precision: 0.833867180052918\n",
      "Recall: 0.8322222222222223\n",
      "F1-Score: 0.831693546343287\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(test_y, preds_te_adjusted))\n",
    "print('Precision:', precision_score(test_y, preds_te_adjusted, average='macro'))\n",
    "print('Recall:', recall_score(test_y, preds_te_adjusted, average='macro'))\n",
    "print('F1-Score:', f1_score(test_y, preds_te_adjusted, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Learning Rate (lr): The learning rate determines the step size during the optimization process. In the code, a learning rate of 0.001 was chosen (lr=0.001). This value is commonly used as a starting point for many optimization algorithms and was likely chosen based on prior experience or experimentation.\n",
    "\n",
    "Dropout Rate (dropout): Dropout is a regularization technique used to prevent overfitting by randomly dropping units (along with their connections) from the neural network during training. In the code, a dropout rate of 0.5 was used (dropout=0.5). This value is a common default choice for dropout rates and was likely selected based on best practices or previous experimentation.\n",
    "\n",
    "Tolerance for Early Stopping (tolerance): Early stopping is a regularization technique used to prevent overfitting by stopping the training process early if the performance on a validation dataset stops improving. In the code, a tolerance of 1e-12 was used (tolerance=1e-12). This very small value ensures that even minor improvements in validation loss are considered significant enough to continue training.\n",
    "\n",
    "Freeze Embeddings (freeze_emb): Freezing embeddings refers to the practice of preventing the weights of the embedding layer from being updated during training. In the code, freeze_emb is set to False, indicating that the embeddings are not frozen. Depending on the dataset and task, freezing embeddings may or may not be beneficial, and this decision may have been based on experimentation or prior knowledge about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    np.random.seed(0)\n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in vocab:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29683     0.016691    0.0067694  ...  0.12887999 -0.10487\n",
      "   0.43740001]\n",
      " [-0.32328999 -0.092565   -0.059932   ... -0.48166999 -0.26572001\n",
      "   0.41001001]\n",
      " [ 0.24941    -0.32334    -0.23853    ... -0.22702999 -0.35901999\n",
      "  -0.28477001]\n",
      " ...\n",
      " [ 0.28319001 -0.41251001  0.17455    ...  0.17354999 -0.30254999\n",
      "  -0.32628   ]\n",
      " [ 0.34018001  0.26451999 -0.44622001 ... -0.55971998 -0.51005\n",
      "   0.14749999]\n",
      " [ 0.1301     -0.30665001 -0.197      ...  0.22492     0.38260001\n",
      "   0.24495   ]]\n"
     ]
    }
   ],
   "source": [
    "w_glove = get_glove_embeddings(f_zip=\"glove.840B.300d.zip\", f_txt=\"glove.840B.300d.txt\", word2id=word2id)\n",
    "print(w_glove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with learning rate: 0.001, dropout rate: 0.3\n",
      "Epoch: 1/100, Train Loss: 1.1070 (2400 samples)\n",
      "Validation Loss: 1.0114 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.9501 (2400 samples)\n",
      "Validation Loss: 0.9116 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.8593 (2400 samples)\n",
      "Validation Loss: 0.8314 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.7883 (2400 samples)\n",
      "Validation Loss: 0.7668 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.7322 (2400 samples)\n",
      "Validation Loss: 0.7141 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.6873 (2400 samples)\n",
      "Validation Loss: 0.6706 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.6508 (2400 samples)\n",
      "Validation Loss: 0.6343 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.6206 (2400 samples)\n",
      "Validation Loss: 0.6035 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.5954 (2400 samples)\n",
      "Validation Loss: 0.5771 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.5741 (2400 samples)\n",
      "Validation Loss: 0.5542 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.5559 (2400 samples)\n",
      "Validation Loss: 0.5342 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.5401 (2400 samples)\n",
      "Validation Loss: 0.5165 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.5263 (2400 samples)\n",
      "Validation Loss: 0.5008 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.5142 (2400 samples)\n",
      "Validation Loss: 0.4868 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.5035 (2400 samples)\n",
      "Validation Loss: 0.4741 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.4939 (2400 samples)\n",
      "Validation Loss: 0.4626 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.4852 (2400 samples)\n",
      "Validation Loss: 0.4521 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.4775 (2400 samples)\n",
      "Validation Loss: 0.4424 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.4704 (2400 samples)\n",
      "Validation Loss: 0.4336 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.4639 (2400 samples)\n",
      "Validation Loss: 0.4254 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.4580 (2400 samples)\n",
      "Validation Loss: 0.4178 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.4526 (2400 samples)\n",
      "Validation Loss: 0.4107 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.4475 (2400 samples)\n",
      "Validation Loss: 0.4040 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.4429 (2400 samples)\n",
      "Validation Loss: 0.3978 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.4385 (2400 samples)\n",
      "Validation Loss: 0.3920 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.4345 (2400 samples)\n",
      "Validation Loss: 0.3865 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.4307 (2400 samples)\n",
      "Validation Loss: 0.3813 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.4271 (2400 samples)\n",
      "Validation Loss: 0.3764 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.4238 (2400 samples)\n",
      "Validation Loss: 0.3718 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.4206 (2400 samples)\n",
      "Validation Loss: 0.3673 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.4176 (2400 samples)\n",
      "Validation Loss: 0.3631 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.4148 (2400 samples)\n",
      "Validation Loss: 0.3591 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.4121 (2400 samples)\n",
      "Validation Loss: 0.3553 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.4096 (2400 samples)\n",
      "Validation Loss: 0.3516 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.4072 (2400 samples)\n",
      "Validation Loss: 0.3481 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.4049 (2400 samples)\n",
      "Validation Loss: 0.3448 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.4027 (2400 samples)\n",
      "Validation Loss: 0.3415 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.4006 (2400 samples)\n",
      "Validation Loss: 0.3384 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.3986 (2400 samples)\n",
      "Validation Loss: 0.3355 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.3966 (2400 samples)\n",
      "Validation Loss: 0.3326 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.3948 (2400 samples)\n",
      "Validation Loss: 0.3298 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.3930 (2400 samples)\n",
      "Validation Loss: 0.3272 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.3913 (2400 samples)\n",
      "Validation Loss: 0.3246 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.3897 (2400 samples)\n",
      "Validation Loss: 0.3221 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.3881 (2400 samples)\n",
      "Validation Loss: 0.3197 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.3865 (2400 samples)\n",
      "Validation Loss: 0.3173 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.3851 (2400 samples)\n",
      "Validation Loss: 0.3151 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.3836 (2400 samples)\n",
      "Validation Loss: 0.3129 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.3823 (2400 samples)\n",
      "Validation Loss: 0.3108 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.3809 (2400 samples)\n",
      "Validation Loss: 0.3087 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.3796 (2400 samples)\n",
      "Validation Loss: 0.3067 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.3784 (2400 samples)\n",
      "Validation Loss: 0.3048 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.3771 (2400 samples)\n",
      "Validation Loss: 0.3029 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.3760 (2400 samples)\n",
      "Validation Loss: 0.3010 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.3748 (2400 samples)\n",
      "Validation Loss: 0.2993 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.3737 (2400 samples)\n",
      "Validation Loss: 0.2975 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.3726 (2400 samples)\n",
      "Validation Loss: 0.2958 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.3715 (2400 samples)\n",
      "Validation Loss: 0.2942 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.3705 (2400 samples)\n",
      "Validation Loss: 0.2926 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.3695 (2400 samples)\n",
      "Validation Loss: 0.2910 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.3685 (2400 samples)\n",
      "Validation Loss: 0.2895 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.3676 (2400 samples)\n",
      "Validation Loss: 0.2880 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.3666 (2400 samples)\n",
      "Validation Loss: 0.2865 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.3657 (2400 samples)\n",
      "Validation Loss: 0.2851 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.3648 (2400 samples)\n",
      "Validation Loss: 0.2837 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.3640 (2400 samples)\n",
      "Validation Loss: 0.2823 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.3631 (2400 samples)\n",
      "Validation Loss: 0.2810 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.3623 (2400 samples)\n",
      "Validation Loss: 0.2797 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.3615 (2400 samples)\n",
      "Validation Loss: 0.2784 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.3607 (2400 samples)\n",
      "Validation Loss: 0.2772 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.3599 (2400 samples)\n",
      "Validation Loss: 0.2759 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.3592 (2400 samples)\n",
      "Validation Loss: 0.2747 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.3584 (2400 samples)\n",
      "Validation Loss: 0.2736 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.3577 (2400 samples)\n",
      "Validation Loss: 0.2724 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.3570 (2400 samples)\n",
      "Validation Loss: 0.2713 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.3563 (2400 samples)\n",
      "Validation Loss: 0.2702 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.3556 (2400 samples)\n",
      "Validation Loss: 0.2691 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.3549 (2400 samples)\n",
      "Validation Loss: 0.2681 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.3542 (2400 samples)\n",
      "Validation Loss: 0.2670 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.3536 (2400 samples)\n",
      "Validation Loss: 0.2660 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.3529 (2400 samples)\n",
      "Validation Loss: 0.2650 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.3523 (2400 samples)\n",
      "Validation Loss: 0.2640 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.3517 (2400 samples)\n",
      "Validation Loss: 0.2630 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.3511 (2400 samples)\n",
      "Validation Loss: 0.2621 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.3505 (2400 samples)\n",
      "Validation Loss: 0.2612 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.3499 (2400 samples)\n",
      "Validation Loss: 0.2602 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.3493 (2400 samples)\n",
      "Validation Loss: 0.2593 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.3487 (2400 samples)\n",
      "Validation Loss: 0.2585 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.3482 (2400 samples)\n",
      "Validation Loss: 0.2576 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.3476 (2400 samples)\n",
      "Validation Loss: 0.2567 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.3471 (2400 samples)\n",
      "Validation Loss: 0.2559 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.3465 (2400 samples)\n",
      "Validation Loss: 0.2551 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.3460 (2400 samples)\n",
      "Validation Loss: 0.2542 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.3455 (2400 samples)\n",
      "Validation Loss: 0.2534 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.3450 (2400 samples)\n",
      "Validation Loss: 0.2527 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.3445 (2400 samples)\n",
      "Validation Loss: 0.2519 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.3440 (2400 samples)\n",
      "Validation Loss: 0.2511 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.3435 (2400 samples)\n",
      "Validation Loss: 0.2504 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.3430 (2400 samples)\n",
      "Validation Loss: 0.2496 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.3425 (2400 samples)\n",
      "Validation Loss: 0.2489 (150 samples)\n",
      "Training Loss: [1.1070415115148897, 0.9500802240702964, 0.8593045006295665, 0.7883415357531376, 0.7322487643623462, 0.6873009046723955, 0.6507568807116607, 0.6206188697769726, 0.5954277415091923, 0.5741091416287273, 0.5558630382152692, 0.5400861542426492, 0.5263175675808156, 0.5142007454493034, 0.5034564908145143, 0.4938634492944139, 0.4852444493919377, 0.4774562071900656, 0.4703817512620979, 0.4639247875479525, 0.45800526732983066, 0.4525563525363883, 0.4475217133983154, 0.44285358786805556, 0.43851120756565054, 0.43445972998048515, 0.4306689828893201, 0.4271128736344132, 0.4237686709406773, 0.4206165388744679, 0.4176390542893778, 0.41482079630749397, 0.41214816439480845, 0.40960913082659545, 0.4071929084683667, 0.4048898563871437, 0.4026913629387125, 0.40058967301681775, 0.39857777247161974, 0.3966493773617007, 0.39479873098547896, 0.39302068914422866, 0.3913104572400993, 0.38966372460789694, 0.38807659153284985, 0.38654536683765045, 0.3850667767014707, 0.38363773884001345, 0.38225542737708007, 0.3809173021427446, 0.37962089083711703, 0.37836398865108306, 0.37714455135995256, 0.3759606771305644, 0.3748105325090196, 0.3736924800404277, 0.3726049416439831, 0.37154648914363764, 0.37051578005535957, 0.36951154300946876, 0.36853258877005934, 0.36757779284835507, 0.36664611665376123, 0.3657365672642817, 0.3648482102120022, 0.36398018530238313, 0.36313167327704915, 0.36230188433506055, 0.36149008699400953, 0.3606955784470811, 0.35991769185928074, 0.35915580484435433, 0.35840934910704325, 0.35767773044958867, 0.3569603961321517, 0.3562568809918779, 0.35556670962703074, 0.35488938436335055, 0.35422447252772066, 0.35357156559652353, 0.3529302686238082, 0.35230019399924967, 0.3516809819934354, 0.351072278630984, 0.3504737594395413, 0.3498851140055246, 0.3493060128526166, 0.3487362014340677, 0.3481753711405864, 0.3476232555363239, 0.34707957124895067, 0.34654414416068885, 0.3460167092776274, 0.3454970056769438, 0.3449848616328942, 0.3444800077771928, 0.34398231358791587, 0.343491544097636, 0.34300752520895583, 0.3425300879963003]\n",
      "Validation Losses: [1.011430744778455, 0.9116352876732297, 0.8314176224866846, 0.7667927170307285, 0.7141280545375283, 0.6706495344549985, 0.6342807995170966, 0.6034751864633493, 0.5770746082688187, 0.5542052236506959, 0.534200612344194, 0.5165467270895654, 0.5008422591197508, 0.48677058049563937, 0.47407902664386825, 0.4625639755599903, 0.4520598485766943, 0.44243036838278355, 0.4335630457207652, 0.4253639550772068, 0.4177541050724764, 0.4106664057644836, 0.404044254709774, 0.3978385213568647, 0.3920072633280516, 0.38651419021513306, 0.38132717053877613, 0.3764187945231232, 0.37176455894906935, 0.36734307610201733, 0.36313523504962875, 0.3591238054009115, 0.35529419496626113, 0.35163269885550225, 0.3481269826283049, 0.3447662553410035, 0.3415404986099914, 0.33844125995393637, 0.3354595615788791, 0.3325885084074602, 0.32982128967249746, 0.32715181395400966, 0.32457415026644126, 0.3220830192729829, 0.3196740687744959, 0.3173424962749886, 0.31508425585567024, 0.31289556306243055, 0.3107729266470402, 0.3087130926326152, 0.30671304781464964, 0.304769869489999, 0.3028809950239179, 0.30104392770178195, 0.29925609137586784, 0.2975158548880613, 0.2958206890723116, 0.29416880587877386, 0.29255836304903315, 0.2909878661814969, 0.28945554204464197, 0.28796008432296316, 0.2864998090515375, 0.2850733369714643, 0.2836794312041556, 0.282317141787064, 0.2809850280028943, 0.27968215217879683, 0.2784074318850175, 0.27715996772429713, 0.2759386036190057, 0.2747423919844758, 0.2735708911596186, 0.27242294268547573, 0.27129792332357483, 0.27019492438110226, 0.26911337059062884, 0.2680527109066677, 0.26701215407733303, 0.26599096688134055, 0.26498874459617366, 0.26400486209337953, 0.2630387515779692, 0.26208988576534076, 0.2611577329145141, 0.260241873146038, 0.25934174304744245, 0.25845709597135735, 0.25758729052934204, 0.2567320138192517, 0.25589084600017503, 0.25506344167580003, 0.25424939622526277, 0.2534483194721926, 0.25265995458324014, 0.2518838923229464, 0.25111989775354693, 0.2503675634774849, 0.24962664176124463, 0.2488969480386219]\n",
      "Hyperparameters: lr=0.001, dropout=0.3\n",
      "Testing with learning rate: 0.001, dropout rate: 0.5\n",
      "Epoch: 1/100, Train Loss: 1.1070 (2400 samples)\n",
      "Validation Loss: 1.0114 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.9501 (2400 samples)\n",
      "Validation Loss: 0.9116 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.8593 (2400 samples)\n",
      "Validation Loss: 0.8314 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.7883 (2400 samples)\n",
      "Validation Loss: 0.7668 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.7322 (2400 samples)\n",
      "Validation Loss: 0.7141 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.6873 (2400 samples)\n",
      "Validation Loss: 0.6706 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.6508 (2400 samples)\n",
      "Validation Loss: 0.6343 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.6206 (2400 samples)\n",
      "Validation Loss: 0.6035 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.5954 (2400 samples)\n",
      "Validation Loss: 0.5771 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.5741 (2400 samples)\n",
      "Validation Loss: 0.5542 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.5559 (2400 samples)\n",
      "Validation Loss: 0.5342 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.5401 (2400 samples)\n",
      "Validation Loss: 0.5165 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.5263 (2400 samples)\n",
      "Validation Loss: 0.5008 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.5142 (2400 samples)\n",
      "Validation Loss: 0.4868 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.5035 (2400 samples)\n",
      "Validation Loss: 0.4741 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.4939 (2400 samples)\n",
      "Validation Loss: 0.4626 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.4852 (2400 samples)\n",
      "Validation Loss: 0.4521 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.4775 (2400 samples)\n",
      "Validation Loss: 0.4424 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.4704 (2400 samples)\n",
      "Validation Loss: 0.4336 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.4639 (2400 samples)\n",
      "Validation Loss: 0.4254 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.4580 (2400 samples)\n",
      "Validation Loss: 0.4178 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.4526 (2400 samples)\n",
      "Validation Loss: 0.4107 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.4475 (2400 samples)\n",
      "Validation Loss: 0.4040 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.4429 (2400 samples)\n",
      "Validation Loss: 0.3978 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.4385 (2400 samples)\n",
      "Validation Loss: 0.3920 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.4345 (2400 samples)\n",
      "Validation Loss: 0.3865 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.4307 (2400 samples)\n",
      "Validation Loss: 0.3813 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.4271 (2400 samples)\n",
      "Validation Loss: 0.3764 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.4238 (2400 samples)\n",
      "Validation Loss: 0.3718 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.4206 (2400 samples)\n",
      "Validation Loss: 0.3673 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.4176 (2400 samples)\n",
      "Validation Loss: 0.3631 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.4148 (2400 samples)\n",
      "Validation Loss: 0.3591 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.4121 (2400 samples)\n",
      "Validation Loss: 0.3553 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.4096 (2400 samples)\n",
      "Validation Loss: 0.3516 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.4072 (2400 samples)\n",
      "Validation Loss: 0.3481 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.4049 (2400 samples)\n",
      "Validation Loss: 0.3448 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.4027 (2400 samples)\n",
      "Validation Loss: 0.3415 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.4006 (2400 samples)\n",
      "Validation Loss: 0.3384 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.3986 (2400 samples)\n",
      "Validation Loss: 0.3355 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.3966 (2400 samples)\n",
      "Validation Loss: 0.3326 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.3948 (2400 samples)\n",
      "Validation Loss: 0.3298 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.3930 (2400 samples)\n",
      "Validation Loss: 0.3272 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.3913 (2400 samples)\n",
      "Validation Loss: 0.3246 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.3897 (2400 samples)\n",
      "Validation Loss: 0.3221 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.3881 (2400 samples)\n",
      "Validation Loss: 0.3197 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.3865 (2400 samples)\n",
      "Validation Loss: 0.3173 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.3851 (2400 samples)\n",
      "Validation Loss: 0.3151 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.3836 (2400 samples)\n",
      "Validation Loss: 0.3129 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.3823 (2400 samples)\n",
      "Validation Loss: 0.3108 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.3809 (2400 samples)\n",
      "Validation Loss: 0.3087 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.3796 (2400 samples)\n",
      "Validation Loss: 0.3067 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.3784 (2400 samples)\n",
      "Validation Loss: 0.3048 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.3771 (2400 samples)\n",
      "Validation Loss: 0.3029 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.3760 (2400 samples)\n",
      "Validation Loss: 0.3010 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.3748 (2400 samples)\n",
      "Validation Loss: 0.2993 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.3737 (2400 samples)\n",
      "Validation Loss: 0.2975 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.3726 (2400 samples)\n",
      "Validation Loss: 0.2958 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.3715 (2400 samples)\n",
      "Validation Loss: 0.2942 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.3705 (2400 samples)\n",
      "Validation Loss: 0.2926 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.3695 (2400 samples)\n",
      "Validation Loss: 0.2910 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.3685 (2400 samples)\n",
      "Validation Loss: 0.2895 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.3676 (2400 samples)\n",
      "Validation Loss: 0.2880 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.3666 (2400 samples)\n",
      "Validation Loss: 0.2865 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.3657 (2400 samples)\n",
      "Validation Loss: 0.2851 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.3648 (2400 samples)\n",
      "Validation Loss: 0.2837 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.3640 (2400 samples)\n",
      "Validation Loss: 0.2823 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.3631 (2400 samples)\n",
      "Validation Loss: 0.2810 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.3623 (2400 samples)\n",
      "Validation Loss: 0.2797 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.3615 (2400 samples)\n",
      "Validation Loss: 0.2784 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.3607 (2400 samples)\n",
      "Validation Loss: 0.2772 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.3599 (2400 samples)\n",
      "Validation Loss: 0.2759 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.3592 (2400 samples)\n",
      "Validation Loss: 0.2747 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.3584 (2400 samples)\n",
      "Validation Loss: 0.2736 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.3577 (2400 samples)\n",
      "Validation Loss: 0.2724 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.3570 (2400 samples)\n",
      "Validation Loss: 0.2713 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.3563 (2400 samples)\n",
      "Validation Loss: 0.2702 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.3556 (2400 samples)\n",
      "Validation Loss: 0.2691 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.3549 (2400 samples)\n",
      "Validation Loss: 0.2681 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.3542 (2400 samples)\n",
      "Validation Loss: 0.2670 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.3536 (2400 samples)\n",
      "Validation Loss: 0.2660 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.3529 (2400 samples)\n",
      "Validation Loss: 0.2650 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.3523 (2400 samples)\n",
      "Validation Loss: 0.2640 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.3517 (2400 samples)\n",
      "Validation Loss: 0.2630 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.3511 (2400 samples)\n",
      "Validation Loss: 0.2621 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.3505 (2400 samples)\n",
      "Validation Loss: 0.2612 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.3499 (2400 samples)\n",
      "Validation Loss: 0.2602 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.3493 (2400 samples)\n",
      "Validation Loss: 0.2593 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.3487 (2400 samples)\n",
      "Validation Loss: 0.2585 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.3482 (2400 samples)\n",
      "Validation Loss: 0.2576 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.3476 (2400 samples)\n",
      "Validation Loss: 0.2567 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.3471 (2400 samples)\n",
      "Validation Loss: 0.2559 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.3465 (2400 samples)\n",
      "Validation Loss: 0.2551 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.3460 (2400 samples)\n",
      "Validation Loss: 0.2542 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.3455 (2400 samples)\n",
      "Validation Loss: 0.2534 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.3450 (2400 samples)\n",
      "Validation Loss: 0.2527 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.3445 (2400 samples)\n",
      "Validation Loss: 0.2519 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.3440 (2400 samples)\n",
      "Validation Loss: 0.2511 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.3435 (2400 samples)\n",
      "Validation Loss: 0.2504 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.3430 (2400 samples)\n",
      "Validation Loss: 0.2496 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.3425 (2400 samples)\n",
      "Validation Loss: 0.2489 (150 samples)\n",
      "Training Loss: [1.1070415115148897, 0.9500802240702964, 0.8593045006295665, 0.7883415357531376, 0.7322487643623462, 0.6873009046723955, 0.6507568807116607, 0.6206188697769726, 0.5954277415091923, 0.5741091416287273, 0.5558630382152692, 0.5400861542426492, 0.5263175675808156, 0.5142007454493034, 0.5034564908145143, 0.4938634492944139, 0.4852444493919377, 0.4774562071900656, 0.4703817512620979, 0.4639247875479525, 0.45800526732983066, 0.4525563525363883, 0.4475217133983154, 0.44285358786805556, 0.43851120756565054, 0.43445972998048515, 0.4306689828893201, 0.4271128736344132, 0.4237686709406773, 0.4206165388744679, 0.4176390542893778, 0.41482079630749397, 0.41214816439480845, 0.40960913082659545, 0.4071929084683667, 0.4048898563871437, 0.4026913629387125, 0.40058967301681775, 0.39857777247161974, 0.3966493773617007, 0.39479873098547896, 0.39302068914422866, 0.3913104572400993, 0.38966372460789694, 0.38807659153284985, 0.38654536683765045, 0.3850667767014707, 0.38363773884001345, 0.38225542737708007, 0.3809173021427446, 0.37962089083711703, 0.37836398865108306, 0.37714455135995256, 0.3759606771305644, 0.3748105325090196, 0.3736924800404277, 0.3726049416439831, 0.37154648914363764, 0.37051578005535957, 0.36951154300946876, 0.36853258877005934, 0.36757779284835507, 0.36664611665376123, 0.3657365672642817, 0.3648482102120022, 0.36398018530238313, 0.36313167327704915, 0.36230188433506055, 0.36149008699400953, 0.3606955784470811, 0.35991769185928074, 0.35915580484435433, 0.35840934910704325, 0.35767773044958867, 0.3569603961321517, 0.3562568809918779, 0.35556670962703074, 0.35488938436335055, 0.35422447252772066, 0.35357156559652353, 0.3529302686238082, 0.35230019399924967, 0.3516809819934354, 0.351072278630984, 0.3504737594395413, 0.3498851140055246, 0.3493060128526166, 0.3487362014340677, 0.3481753711405864, 0.3476232555363239, 0.34707957124895067, 0.34654414416068885, 0.3460167092776274, 0.3454970056769438, 0.3449848616328942, 0.3444800077771928, 0.34398231358791587, 0.343491544097636, 0.34300752520895583, 0.3425300879963003]\n",
      "Validation Losses: [1.011430744778455, 0.9116352876732297, 0.8314176224866846, 0.7667927170307285, 0.7141280545375283, 0.6706495344549985, 0.6342807995170966, 0.6034751864633493, 0.5770746082688187, 0.5542052236506959, 0.534200612344194, 0.5165467270895654, 0.5008422591197508, 0.48677058049563937, 0.47407902664386825, 0.4625639755599903, 0.4520598485766943, 0.44243036838278355, 0.4335630457207652, 0.4253639550772068, 0.4177541050724764, 0.4106664057644836, 0.404044254709774, 0.3978385213568647, 0.3920072633280516, 0.38651419021513306, 0.38132717053877613, 0.3764187945231232, 0.37176455894906935, 0.36734307610201733, 0.36313523504962875, 0.3591238054009115, 0.35529419496626113, 0.35163269885550225, 0.3481269826283049, 0.3447662553410035, 0.3415404986099914, 0.33844125995393637, 0.3354595615788791, 0.3325885084074602, 0.32982128967249746, 0.32715181395400966, 0.32457415026644126, 0.3220830192729829, 0.3196740687744959, 0.3173424962749886, 0.31508425585567024, 0.31289556306243055, 0.3107729266470402, 0.3087130926326152, 0.30671304781464964, 0.304769869489999, 0.3028809950239179, 0.30104392770178195, 0.29925609137586784, 0.2975158548880613, 0.2958206890723116, 0.29416880587877386, 0.29255836304903315, 0.2909878661814969, 0.28945554204464197, 0.28796008432296316, 0.2864998090515375, 0.2850733369714643, 0.2836794312041556, 0.282317141787064, 0.2809850280028943, 0.27968215217879683, 0.2784074318850175, 0.27715996772429713, 0.2759386036190057, 0.2747423919844758, 0.2735708911596186, 0.27242294268547573, 0.27129792332357483, 0.27019492438110226, 0.26911337059062884, 0.2680527109066677, 0.26701215407733303, 0.26599096688134055, 0.26498874459617366, 0.26400486209337953, 0.2630387515779692, 0.26208988576534076, 0.2611577329145141, 0.260241873146038, 0.25934174304744245, 0.25845709597135735, 0.25758729052934204, 0.2567320138192517, 0.25589084600017503, 0.25506344167580003, 0.25424939622526277, 0.2534483194721926, 0.25265995458324014, 0.2518838923229464, 0.25111989775354693, 0.2503675634774849, 0.24962664176124463, 0.2488969480386219]\n",
      "Hyperparameters: lr=0.001, dropout=0.5\n",
      "Testing with learning rate: 0.001, dropout rate: 0.7\n",
      "Epoch: 1/100, Train Loss: 1.1070 (2400 samples)\n",
      "Validation Loss: 1.0114 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.9501 (2400 samples)\n",
      "Validation Loss: 0.9116 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.8593 (2400 samples)\n",
      "Validation Loss: 0.8314 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.7883 (2400 samples)\n",
      "Validation Loss: 0.7668 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.7322 (2400 samples)\n",
      "Validation Loss: 0.7141 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.6873 (2400 samples)\n",
      "Validation Loss: 0.6706 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.6508 (2400 samples)\n",
      "Validation Loss: 0.6343 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.6206 (2400 samples)\n",
      "Validation Loss: 0.6035 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.5954 (2400 samples)\n",
      "Validation Loss: 0.5771 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.5741 (2400 samples)\n",
      "Validation Loss: 0.5542 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.5559 (2400 samples)\n",
      "Validation Loss: 0.5342 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.5401 (2400 samples)\n",
      "Validation Loss: 0.5165 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.5263 (2400 samples)\n",
      "Validation Loss: 0.5008 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.5142 (2400 samples)\n",
      "Validation Loss: 0.4868 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.5035 (2400 samples)\n",
      "Validation Loss: 0.4741 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.4939 (2400 samples)\n",
      "Validation Loss: 0.4626 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.4852 (2400 samples)\n",
      "Validation Loss: 0.4521 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.4775 (2400 samples)\n",
      "Validation Loss: 0.4424 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.4704 (2400 samples)\n",
      "Validation Loss: 0.4336 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.4639 (2400 samples)\n",
      "Validation Loss: 0.4254 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.4580 (2400 samples)\n",
      "Validation Loss: 0.4178 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.4526 (2400 samples)\n",
      "Validation Loss: 0.4107 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.4475 (2400 samples)\n",
      "Validation Loss: 0.4040 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.4429 (2400 samples)\n",
      "Validation Loss: 0.3978 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.4385 (2400 samples)\n",
      "Validation Loss: 0.3920 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.4345 (2400 samples)\n",
      "Validation Loss: 0.3865 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.4307 (2400 samples)\n",
      "Validation Loss: 0.3813 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.4271 (2400 samples)\n",
      "Validation Loss: 0.3764 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.4238 (2400 samples)\n",
      "Validation Loss: 0.3718 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.4206 (2400 samples)\n",
      "Validation Loss: 0.3673 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.4176 (2400 samples)\n",
      "Validation Loss: 0.3631 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.4148 (2400 samples)\n",
      "Validation Loss: 0.3591 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.4121 (2400 samples)\n",
      "Validation Loss: 0.3553 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.4096 (2400 samples)\n",
      "Validation Loss: 0.3516 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.4072 (2400 samples)\n",
      "Validation Loss: 0.3481 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.4049 (2400 samples)\n",
      "Validation Loss: 0.3448 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.4027 (2400 samples)\n",
      "Validation Loss: 0.3415 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.4006 (2400 samples)\n",
      "Validation Loss: 0.3384 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.3986 (2400 samples)\n",
      "Validation Loss: 0.3355 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.3966 (2400 samples)\n",
      "Validation Loss: 0.3326 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.3948 (2400 samples)\n",
      "Validation Loss: 0.3298 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.3930 (2400 samples)\n",
      "Validation Loss: 0.3272 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.3913 (2400 samples)\n",
      "Validation Loss: 0.3246 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.3897 (2400 samples)\n",
      "Validation Loss: 0.3221 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.3881 (2400 samples)\n",
      "Validation Loss: 0.3197 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.3865 (2400 samples)\n",
      "Validation Loss: 0.3173 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.3851 (2400 samples)\n",
      "Validation Loss: 0.3151 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.3836 (2400 samples)\n",
      "Validation Loss: 0.3129 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.3823 (2400 samples)\n",
      "Validation Loss: 0.3108 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.3809 (2400 samples)\n",
      "Validation Loss: 0.3087 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.3796 (2400 samples)\n",
      "Validation Loss: 0.3067 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.3784 (2400 samples)\n",
      "Validation Loss: 0.3048 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.3771 (2400 samples)\n",
      "Validation Loss: 0.3029 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.3760 (2400 samples)\n",
      "Validation Loss: 0.3010 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.3748 (2400 samples)\n",
      "Validation Loss: 0.2993 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.3737 (2400 samples)\n",
      "Validation Loss: 0.2975 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.3726 (2400 samples)\n",
      "Validation Loss: 0.2958 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.3715 (2400 samples)\n",
      "Validation Loss: 0.2942 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.3705 (2400 samples)\n",
      "Validation Loss: 0.2926 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.3695 (2400 samples)\n",
      "Validation Loss: 0.2910 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.3685 (2400 samples)\n",
      "Validation Loss: 0.2895 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.3676 (2400 samples)\n",
      "Validation Loss: 0.2880 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.3666 (2400 samples)\n",
      "Validation Loss: 0.2865 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.3657 (2400 samples)\n",
      "Validation Loss: 0.2851 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.3648 (2400 samples)\n",
      "Validation Loss: 0.2837 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.3640 (2400 samples)\n",
      "Validation Loss: 0.2823 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.3631 (2400 samples)\n",
      "Validation Loss: 0.2810 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.3623 (2400 samples)\n",
      "Validation Loss: 0.2797 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.3615 (2400 samples)\n",
      "Validation Loss: 0.2784 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.3607 (2400 samples)\n",
      "Validation Loss: 0.2772 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.3599 (2400 samples)\n",
      "Validation Loss: 0.2759 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.3592 (2400 samples)\n",
      "Validation Loss: 0.2747 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.3584 (2400 samples)\n",
      "Validation Loss: 0.2736 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.3577 (2400 samples)\n",
      "Validation Loss: 0.2724 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.3570 (2400 samples)\n",
      "Validation Loss: 0.2713 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.3563 (2400 samples)\n",
      "Validation Loss: 0.2702 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.3556 (2400 samples)\n",
      "Validation Loss: 0.2691 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.3549 (2400 samples)\n",
      "Validation Loss: 0.2681 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.3542 (2400 samples)\n",
      "Validation Loss: 0.2670 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.3536 (2400 samples)\n",
      "Validation Loss: 0.2660 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.3529 (2400 samples)\n",
      "Validation Loss: 0.2650 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.3523 (2400 samples)\n",
      "Validation Loss: 0.2640 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.3517 (2400 samples)\n",
      "Validation Loss: 0.2630 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.3511 (2400 samples)\n",
      "Validation Loss: 0.2621 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.3505 (2400 samples)\n",
      "Validation Loss: 0.2612 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.3499 (2400 samples)\n",
      "Validation Loss: 0.2602 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.3493 (2400 samples)\n",
      "Validation Loss: 0.2593 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.3487 (2400 samples)\n",
      "Validation Loss: 0.2585 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.3482 (2400 samples)\n",
      "Validation Loss: 0.2576 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.3476 (2400 samples)\n",
      "Validation Loss: 0.2567 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.3471 (2400 samples)\n",
      "Validation Loss: 0.2559 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.3465 (2400 samples)\n",
      "Validation Loss: 0.2551 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.3460 (2400 samples)\n",
      "Validation Loss: 0.2542 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.3455 (2400 samples)\n",
      "Validation Loss: 0.2534 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.3450 (2400 samples)\n",
      "Validation Loss: 0.2527 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.3445 (2400 samples)\n",
      "Validation Loss: 0.2519 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.3440 (2400 samples)\n",
      "Validation Loss: 0.2511 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.3435 (2400 samples)\n",
      "Validation Loss: 0.2504 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.3430 (2400 samples)\n",
      "Validation Loss: 0.2496 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.3425 (2400 samples)\n",
      "Validation Loss: 0.2489 (150 samples)\n",
      "Training Loss: [1.1070415115148897, 0.9500802240702964, 0.8593045006295665, 0.7883415357531376, 0.7322487643623462, 0.6873009046723955, 0.6507568807116607, 0.6206188697769726, 0.5954277415091923, 0.5741091416287273, 0.5558630382152692, 0.5400861542426492, 0.5263175675808156, 0.5142007454493034, 0.5034564908145143, 0.4938634492944139, 0.4852444493919377, 0.4774562071900656, 0.4703817512620979, 0.4639247875479525, 0.45800526732983066, 0.4525563525363883, 0.4475217133983154, 0.44285358786805556, 0.43851120756565054, 0.43445972998048515, 0.4306689828893201, 0.4271128736344132, 0.4237686709406773, 0.4206165388744679, 0.4176390542893778, 0.41482079630749397, 0.41214816439480845, 0.40960913082659545, 0.4071929084683667, 0.4048898563871437, 0.4026913629387125, 0.40058967301681775, 0.39857777247161974, 0.3966493773617007, 0.39479873098547896, 0.39302068914422866, 0.3913104572400993, 0.38966372460789694, 0.38807659153284985, 0.38654536683765045, 0.3850667767014707, 0.38363773884001345, 0.38225542737708007, 0.3809173021427446, 0.37962089083711703, 0.37836398865108306, 0.37714455135995256, 0.3759606771305644, 0.3748105325090196, 0.3736924800404277, 0.3726049416439831, 0.37154648914363764, 0.37051578005535957, 0.36951154300946876, 0.36853258877005934, 0.36757779284835507, 0.36664611665376123, 0.3657365672642817, 0.3648482102120022, 0.36398018530238313, 0.36313167327704915, 0.36230188433506055, 0.36149008699400953, 0.3606955784470811, 0.35991769185928074, 0.35915580484435433, 0.35840934910704325, 0.35767773044958867, 0.3569603961321517, 0.3562568809918779, 0.35556670962703074, 0.35488938436335055, 0.35422447252772066, 0.35357156559652353, 0.3529302686238082, 0.35230019399924967, 0.3516809819934354, 0.351072278630984, 0.3504737594395413, 0.3498851140055246, 0.3493060128526166, 0.3487362014340677, 0.3481753711405864, 0.3476232555363239, 0.34707957124895067, 0.34654414416068885, 0.3460167092776274, 0.3454970056769438, 0.3449848616328942, 0.3444800077771928, 0.34398231358791587, 0.343491544097636, 0.34300752520895583, 0.3425300879963003]\n",
      "Validation Losses: [1.011430744778455, 0.9116352876732297, 0.8314176224866846, 0.7667927170307285, 0.7141280545375283, 0.6706495344549985, 0.6342807995170966, 0.6034751864633493, 0.5770746082688187, 0.5542052236506959, 0.534200612344194, 0.5165467270895654, 0.5008422591197508, 0.48677058049563937, 0.47407902664386825, 0.4625639755599903, 0.4520598485766943, 0.44243036838278355, 0.4335630457207652, 0.4253639550772068, 0.4177541050724764, 0.4106664057644836, 0.404044254709774, 0.3978385213568647, 0.3920072633280516, 0.38651419021513306, 0.38132717053877613, 0.3764187945231232, 0.37176455894906935, 0.36734307610201733, 0.36313523504962875, 0.3591238054009115, 0.35529419496626113, 0.35163269885550225, 0.3481269826283049, 0.3447662553410035, 0.3415404986099914, 0.33844125995393637, 0.3354595615788791, 0.3325885084074602, 0.32982128967249746, 0.32715181395400966, 0.32457415026644126, 0.3220830192729829, 0.3196740687744959, 0.3173424962749886, 0.31508425585567024, 0.31289556306243055, 0.3107729266470402, 0.3087130926326152, 0.30671304781464964, 0.304769869489999, 0.3028809950239179, 0.30104392770178195, 0.29925609137586784, 0.2975158548880613, 0.2958206890723116, 0.29416880587877386, 0.29255836304903315, 0.2909878661814969, 0.28945554204464197, 0.28796008432296316, 0.2864998090515375, 0.2850733369714643, 0.2836794312041556, 0.282317141787064, 0.2809850280028943, 0.27968215217879683, 0.2784074318850175, 0.27715996772429713, 0.2759386036190057, 0.2747423919844758, 0.2735708911596186, 0.27242294268547573, 0.27129792332357483, 0.27019492438110226, 0.26911337059062884, 0.2680527109066677, 0.26701215407733303, 0.26599096688134055, 0.26498874459617366, 0.26400486209337953, 0.2630387515779692, 0.26208988576534076, 0.2611577329145141, 0.260241873146038, 0.25934174304744245, 0.25845709597135735, 0.25758729052934204, 0.2567320138192517, 0.25589084600017503, 0.25506344167580003, 0.25424939622526277, 0.2534483194721926, 0.25265995458324014, 0.2518838923229464, 0.25111989775354693, 0.2503675634774849, 0.24962664176124463, 0.2488969480386219]\n",
      "Hyperparameters: lr=0.001, dropout=0.7\n",
      "Testing with learning rate: 0.01, dropout rate: 0.3\n",
      "Epoch: 1/100, Train Loss: 0.7750 (2400 samples)\n",
      "Validation Loss: 0.4919 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.5110 (2400 samples)\n",
      "Validation Loss: 0.3895 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.4428 (2400 samples)\n",
      "Validation Loss: 0.3416 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.4106 (2400 samples)\n",
      "Validation Loss: 0.3122 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.3911 (2400 samples)\n",
      "Validation Loss: 0.2916 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.3777 (2400 samples)\n",
      "Validation Loss: 0.2762 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.3676 (2400 samples)\n",
      "Validation Loss: 0.2640 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.3597 (2400 samples)\n",
      "Validation Loss: 0.2541 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.3532 (2400 samples)\n",
      "Validation Loss: 0.2458 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.3476 (2400 samples)\n",
      "Validation Loss: 0.2388 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.3429 (2400 samples)\n",
      "Validation Loss: 0.2327 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.3387 (2400 samples)\n",
      "Validation Loss: 0.2274 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.3349 (2400 samples)\n",
      "Validation Loss: 0.2226 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.3315 (2400 samples)\n",
      "Validation Loss: 0.2184 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.3284 (2400 samples)\n",
      "Validation Loss: 0.2145 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.3255 (2400 samples)\n",
      "Validation Loss: 0.2110 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.3229 (2400 samples)\n",
      "Validation Loss: 0.2078 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.3204 (2400 samples)\n",
      "Validation Loss: 0.2049 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.3180 (2400 samples)\n",
      "Validation Loss: 0.2021 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.3158 (2400 samples)\n",
      "Validation Loss: 0.1996 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.3137 (2400 samples)\n",
      "Validation Loss: 0.1972 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.3118 (2400 samples)\n",
      "Validation Loss: 0.1950 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.3099 (2400 samples)\n",
      "Validation Loss: 0.1929 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.3080 (2400 samples)\n",
      "Validation Loss: 0.1909 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.3063 (2400 samples)\n",
      "Validation Loss: 0.1890 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.3046 (2400 samples)\n",
      "Validation Loss: 0.1873 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.3030 (2400 samples)\n",
      "Validation Loss: 0.1856 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.3015 (2400 samples)\n",
      "Validation Loss: 0.1840 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.3000 (2400 samples)\n",
      "Validation Loss: 0.1825 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.2985 (2400 samples)\n",
      "Validation Loss: 0.1810 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.2971 (2400 samples)\n",
      "Validation Loss: 0.1796 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.2957 (2400 samples)\n",
      "Validation Loss: 0.1783 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.2944 (2400 samples)\n",
      "Validation Loss: 0.1770 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.2931 (2400 samples)\n",
      "Validation Loss: 0.1758 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.2919 (2400 samples)\n",
      "Validation Loss: 0.1746 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.2907 (2400 samples)\n",
      "Validation Loss: 0.1735 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.2895 (2400 samples)\n",
      "Validation Loss: 0.1724 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.2883 (2400 samples)\n",
      "Validation Loss: 0.1713 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.2872 (2400 samples)\n",
      "Validation Loss: 0.1703 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.2861 (2400 samples)\n",
      "Validation Loss: 0.1693 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.2850 (2400 samples)\n",
      "Validation Loss: 0.1684 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.2840 (2400 samples)\n",
      "Validation Loss: 0.1675 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.2830 (2400 samples)\n",
      "Validation Loss: 0.1666 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.2820 (2400 samples)\n",
      "Validation Loss: 0.1657 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.2810 (2400 samples)\n",
      "Validation Loss: 0.1649 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.2800 (2400 samples)\n",
      "Validation Loss: 0.1641 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.2791 (2400 samples)\n",
      "Validation Loss: 0.1633 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.2782 (2400 samples)\n",
      "Validation Loss: 0.1625 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.2773 (2400 samples)\n",
      "Validation Loss: 0.1618 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.2764 (2400 samples)\n",
      "Validation Loss: 0.1611 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.2755 (2400 samples)\n",
      "Validation Loss: 0.1604 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.2747 (2400 samples)\n",
      "Validation Loss: 0.1597 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.2738 (2400 samples)\n",
      "Validation Loss: 0.1590 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.2730 (2400 samples)\n",
      "Validation Loss: 0.1584 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.2722 (2400 samples)\n",
      "Validation Loss: 0.1577 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.2714 (2400 samples)\n",
      "Validation Loss: 0.1571 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.2706 (2400 samples)\n",
      "Validation Loss: 0.1565 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.2698 (2400 samples)\n",
      "Validation Loss: 0.1559 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.2691 (2400 samples)\n",
      "Validation Loss: 0.1554 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.2683 (2400 samples)\n",
      "Validation Loss: 0.1548 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.2676 (2400 samples)\n",
      "Validation Loss: 0.1542 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.2669 (2400 samples)\n",
      "Validation Loss: 0.1537 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.2662 (2400 samples)\n",
      "Validation Loss: 0.1532 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.2655 (2400 samples)\n",
      "Validation Loss: 0.1527 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.2648 (2400 samples)\n",
      "Validation Loss: 0.1522 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.2641 (2400 samples)\n",
      "Validation Loss: 0.1517 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.2635 (2400 samples)\n",
      "Validation Loss: 0.1512 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.2628 (2400 samples)\n",
      "Validation Loss: 0.1507 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.2622 (2400 samples)\n",
      "Validation Loss: 0.1503 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.2615 (2400 samples)\n",
      "Validation Loss: 0.1498 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.2609 (2400 samples)\n",
      "Validation Loss: 0.1494 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.2603 (2400 samples)\n",
      "Validation Loss: 0.1490 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.2597 (2400 samples)\n",
      "Validation Loss: 0.1485 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.2591 (2400 samples)\n",
      "Validation Loss: 0.1481 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.2585 (2400 samples)\n",
      "Validation Loss: 0.1477 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.2579 (2400 samples)\n",
      "Validation Loss: 0.1473 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.2573 (2400 samples)\n",
      "Validation Loss: 0.1469 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.2567 (2400 samples)\n",
      "Validation Loss: 0.1465 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.2562 (2400 samples)\n",
      "Validation Loss: 0.1462 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.2556 (2400 samples)\n",
      "Validation Loss: 0.1458 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.2551 (2400 samples)\n",
      "Validation Loss: 0.1454 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.2545 (2400 samples)\n",
      "Validation Loss: 0.1451 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.2540 (2400 samples)\n",
      "Validation Loss: 0.1447 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.2534 (2400 samples)\n",
      "Validation Loss: 0.1444 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.2529 (2400 samples)\n",
      "Validation Loss: 0.1440 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.2524 (2400 samples)\n",
      "Validation Loss: 0.1437 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.2519 (2400 samples)\n",
      "Validation Loss: 0.1434 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.2514 (2400 samples)\n",
      "Validation Loss: 0.1430 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.2509 (2400 samples)\n",
      "Validation Loss: 0.1427 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.2504 (2400 samples)\n",
      "Validation Loss: 0.1424 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.2499 (2400 samples)\n",
      "Validation Loss: 0.1421 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.2494 (2400 samples)\n",
      "Validation Loss: 0.1418 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.2489 (2400 samples)\n",
      "Validation Loss: 0.1415 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.2484 (2400 samples)\n",
      "Validation Loss: 0.1412 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.2480 (2400 samples)\n",
      "Validation Loss: 0.1409 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.2475 (2400 samples)\n",
      "Validation Loss: 0.1406 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.2470 (2400 samples)\n",
      "Validation Loss: 0.1403 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.2466 (2400 samples)\n",
      "Validation Loss: 0.1401 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.2461 (2400 samples)\n",
      "Validation Loss: 0.1398 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.2457 (2400 samples)\n",
      "Validation Loss: 0.1395 (150 samples)\n",
      "Training Loss: [0.7750445316829154, 0.5110399488830398, 0.44277666656248393, 0.4105999293510022, 0.39114248000747126, 0.3777067940067848, 0.3676468925605038, 0.359696401181783, 0.35316589774950835, 0.34764407564123206, 0.34286841281544217, 0.33866264173004157, 0.33490357904654067, 0.3315024089706418, 0.32839342645650793, 0.3255269155791484, 0.32286457406056446, 0.3203763806490599, 0.31803846744068337, 0.3158316068515362, 0.313740064408659, 0.31175088125953593, 0.30985326306075106, 0.30803810301922785, 0.30629766269433234, 0.3046253301018349, 0.3030153729248331, 0.30146282273238045, 0.2999633226344244, 0.2985130316956536, 0.297108562254098, 0.29574688504034824, 0.2944252869199091, 0.2931413154567943, 0.29189276837128425, 0.2906776498287731, 0.2894941414338227, 0.2883405787326043, 0.28721541494550357, 0.2861172748855966, 0.28504484491539495, 0.2839969353356257, 0.28297243188748855, 0.28197029477016333, 0.28098957807561836, 0.2800293621231016, 0.27908882383167527, 0.2781671834518677, 0.2772636697565247, 0.27637761222153917, 0.27550836033817167, 0.2746552818580824, 0.2738178030345794, 0.27299537859501155, 0.2721874902521047, 0.2713936248866086, 0.27061332550002415, 0.2698461492890861, 0.2690916659640477, 0.2683494744825987, 0.2676191874710135, 0.26690044834275367, 0.26619291264353784, 0.2654962346119994, 0.264810095965597, 0.2641341932988635, 0.263468244732762, 0.262811968124447, 0.2621650716311687, 0.2615273204698599, 0.26089846885385026, 0.26027827249007934, 0.25966649920442114, 0.25906293379270634, 0.25846737560921645, 0.2578796039883719, 0.25729945093276574, 0.25672669508980306, 0.2561611791915926, 0.25560272833988523, 0.25505115699441816, 0.25450631839366084, 0.25396804698880576, 0.25343619459965905, 0.2529106201803663, 0.25239117070151307, 0.251877716020161, 0.2513701253121285, 0.2508682608497489, 0.2503720095828681, 0.24988125303896577, 0.24939586869735314, 0.24891574550597734, 0.2484407746870738, 0.2479708525090013, 0.24750587444871353, 0.24704573750297504, 0.24659035566701823, 0.24613961950605018, 0.24569346503128747]\n",
      "Validation Losses: [0.4918901380232855, 0.38947427405553486, 0.34159508347736617, 0.31218357147472614, 0.2916337482498718, 0.27618776339121776, 0.2640202963493375, 0.25411306163789554, 0.24584207008929346, 0.23879913791234056, 0.23270384751192136, 0.22735678796902875, 0.22261204687951755, 0.21836000986087478, 0.21451713189107782, 0.21101839752108553, 0.20781239588829428, 0.20485798546466252, 0.2021219079732207, 0.1995767816676267, 0.19719987853960935, 0.19497244149115917, 0.19287838372853963, 0.19090411393198525, 0.18903793012434722, 0.18726975294681203, 0.18559084761310848, 0.18399357204571284, 0.18247112622182882, 0.18101769148136893, 0.17962782858390774, 0.17829690549853236, 0.17702071795952615, 0.1757954550029556, 0.17461773338625292, 0.17348446375568763, 0.1723927215607439, 0.17134013334538759, 0.1703241657848875, 0.16934283664272842, 0.16839405783708072, 0.1674761749778065, 0.1665874291658992, 0.1657263244945524, 0.1648914170268119, 0.16408134925549606, 0.16329498699347048, 0.16253117036372286, 0.16178873050027684, 0.1610667978780933, 0.16036441579574845, 0.1596807195437008, 0.15901492828805747, 0.15836616148811405, 0.15773387000433606, 0.1571172619191429, 0.15651568817911163, 0.15592862921824377, 0.15535545100341994, 0.1547956117011509, 0.1542486788353408, 0.1537141520188635, 0.15319151624411775, 0.15268043322963934, 0.15218046294800536, 0.1516911173372917, 0.15121216472734458, 0.15074309053780816, 0.15028376323327153, 0.14983361693790515, 0.1493925996525887, 0.14896030271086333, 0.14853645552322697, 0.14812071467400442, 0.14771297331465705, 0.14731288023594524, 0.14692023292775225, 0.14653482365450496, 0.1461563588498146, 0.14578472894506983, 0.1454196757017806, 0.14506106584195416, 0.14470864980815892, 0.14436230117043045, 0.1440217521430425, 0.14368701980400125, 0.1433577848995972, 0.1430339400668031, 0.1427154229518622, 0.1424019842588454, 0.14209350212465, 0.14178994175834989, 0.1414910572443484, 0.1411967431293978, 0.14090692572558414, 0.14062154221373824, 0.14034037490975265, 0.14006340509829607, 0.13979042742893094, 0.1395214621824809]\n",
      "Hyperparameters: lr=0.01, dropout=0.3\n",
      "Testing with learning rate: 0.01, dropout rate: 0.5\n",
      "Epoch: 1/100, Train Loss: 0.7750 (2400 samples)\n",
      "Validation Loss: 0.4919 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.5110 (2400 samples)\n",
      "Validation Loss: 0.3895 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.4428 (2400 samples)\n",
      "Validation Loss: 0.3416 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.4106 (2400 samples)\n",
      "Validation Loss: 0.3122 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.3911 (2400 samples)\n",
      "Validation Loss: 0.2916 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.3777 (2400 samples)\n",
      "Validation Loss: 0.2762 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.3676 (2400 samples)\n",
      "Validation Loss: 0.2640 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.3597 (2400 samples)\n",
      "Validation Loss: 0.2541 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.3532 (2400 samples)\n",
      "Validation Loss: 0.2458 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.3476 (2400 samples)\n",
      "Validation Loss: 0.2388 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.3429 (2400 samples)\n",
      "Validation Loss: 0.2327 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.3387 (2400 samples)\n",
      "Validation Loss: 0.2274 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.3349 (2400 samples)\n",
      "Validation Loss: 0.2226 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.3315 (2400 samples)\n",
      "Validation Loss: 0.2184 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.3284 (2400 samples)\n",
      "Validation Loss: 0.2145 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.3255 (2400 samples)\n",
      "Validation Loss: 0.2110 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.3229 (2400 samples)\n",
      "Validation Loss: 0.2078 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.3204 (2400 samples)\n",
      "Validation Loss: 0.2049 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.3180 (2400 samples)\n",
      "Validation Loss: 0.2021 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.3158 (2400 samples)\n",
      "Validation Loss: 0.1996 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.3137 (2400 samples)\n",
      "Validation Loss: 0.1972 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.3118 (2400 samples)\n",
      "Validation Loss: 0.1950 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.3099 (2400 samples)\n",
      "Validation Loss: 0.1929 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.3080 (2400 samples)\n",
      "Validation Loss: 0.1909 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.3063 (2400 samples)\n",
      "Validation Loss: 0.1890 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.3046 (2400 samples)\n",
      "Validation Loss: 0.1873 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.3030 (2400 samples)\n",
      "Validation Loss: 0.1856 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.3015 (2400 samples)\n",
      "Validation Loss: 0.1840 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.3000 (2400 samples)\n",
      "Validation Loss: 0.1825 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.2985 (2400 samples)\n",
      "Validation Loss: 0.1810 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.2971 (2400 samples)\n",
      "Validation Loss: 0.1796 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.2957 (2400 samples)\n",
      "Validation Loss: 0.1783 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.2944 (2400 samples)\n",
      "Validation Loss: 0.1770 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.2931 (2400 samples)\n",
      "Validation Loss: 0.1758 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.2919 (2400 samples)\n",
      "Validation Loss: 0.1746 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.2907 (2400 samples)\n",
      "Validation Loss: 0.1735 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.2895 (2400 samples)\n",
      "Validation Loss: 0.1724 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.2883 (2400 samples)\n",
      "Validation Loss: 0.1713 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.2872 (2400 samples)\n",
      "Validation Loss: 0.1703 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.2861 (2400 samples)\n",
      "Validation Loss: 0.1693 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.2850 (2400 samples)\n",
      "Validation Loss: 0.1684 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.2840 (2400 samples)\n",
      "Validation Loss: 0.1675 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.2830 (2400 samples)\n",
      "Validation Loss: 0.1666 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.2820 (2400 samples)\n",
      "Validation Loss: 0.1657 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.2810 (2400 samples)\n",
      "Validation Loss: 0.1649 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.2800 (2400 samples)\n",
      "Validation Loss: 0.1641 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.2791 (2400 samples)\n",
      "Validation Loss: 0.1633 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.2782 (2400 samples)\n",
      "Validation Loss: 0.1625 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.2773 (2400 samples)\n",
      "Validation Loss: 0.1618 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.2764 (2400 samples)\n",
      "Validation Loss: 0.1611 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.2755 (2400 samples)\n",
      "Validation Loss: 0.1604 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.2747 (2400 samples)\n",
      "Validation Loss: 0.1597 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.2738 (2400 samples)\n",
      "Validation Loss: 0.1590 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.2730 (2400 samples)\n",
      "Validation Loss: 0.1584 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.2722 (2400 samples)\n",
      "Validation Loss: 0.1577 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.2714 (2400 samples)\n",
      "Validation Loss: 0.1571 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.2706 (2400 samples)\n",
      "Validation Loss: 0.1565 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.2698 (2400 samples)\n",
      "Validation Loss: 0.1559 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.2691 (2400 samples)\n",
      "Validation Loss: 0.1554 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.2683 (2400 samples)\n",
      "Validation Loss: 0.1548 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.2676 (2400 samples)\n",
      "Validation Loss: 0.1542 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.2669 (2400 samples)\n",
      "Validation Loss: 0.1537 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.2662 (2400 samples)\n",
      "Validation Loss: 0.1532 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.2655 (2400 samples)\n",
      "Validation Loss: 0.1527 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.2648 (2400 samples)\n",
      "Validation Loss: 0.1522 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.2641 (2400 samples)\n",
      "Validation Loss: 0.1517 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.2635 (2400 samples)\n",
      "Validation Loss: 0.1512 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.2628 (2400 samples)\n",
      "Validation Loss: 0.1507 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.2622 (2400 samples)\n",
      "Validation Loss: 0.1503 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.2615 (2400 samples)\n",
      "Validation Loss: 0.1498 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.2609 (2400 samples)\n",
      "Validation Loss: 0.1494 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.2603 (2400 samples)\n",
      "Validation Loss: 0.1490 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.2597 (2400 samples)\n",
      "Validation Loss: 0.1485 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.2591 (2400 samples)\n",
      "Validation Loss: 0.1481 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.2585 (2400 samples)\n",
      "Validation Loss: 0.1477 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.2579 (2400 samples)\n",
      "Validation Loss: 0.1473 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.2573 (2400 samples)\n",
      "Validation Loss: 0.1469 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.2567 (2400 samples)\n",
      "Validation Loss: 0.1465 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.2562 (2400 samples)\n",
      "Validation Loss: 0.1462 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.2556 (2400 samples)\n",
      "Validation Loss: 0.1458 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.2551 (2400 samples)\n",
      "Validation Loss: 0.1454 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.2545 (2400 samples)\n",
      "Validation Loss: 0.1451 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.2540 (2400 samples)\n",
      "Validation Loss: 0.1447 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.2534 (2400 samples)\n",
      "Validation Loss: 0.1444 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.2529 (2400 samples)\n",
      "Validation Loss: 0.1440 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.2524 (2400 samples)\n",
      "Validation Loss: 0.1437 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.2519 (2400 samples)\n",
      "Validation Loss: 0.1434 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.2514 (2400 samples)\n",
      "Validation Loss: 0.1430 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.2509 (2400 samples)\n",
      "Validation Loss: 0.1427 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.2504 (2400 samples)\n",
      "Validation Loss: 0.1424 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.2499 (2400 samples)\n",
      "Validation Loss: 0.1421 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.2494 (2400 samples)\n",
      "Validation Loss: 0.1418 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.2489 (2400 samples)\n",
      "Validation Loss: 0.1415 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.2484 (2400 samples)\n",
      "Validation Loss: 0.1412 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.2480 (2400 samples)\n",
      "Validation Loss: 0.1409 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.2475 (2400 samples)\n",
      "Validation Loss: 0.1406 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.2470 (2400 samples)\n",
      "Validation Loss: 0.1403 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.2466 (2400 samples)\n",
      "Validation Loss: 0.1401 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.2461 (2400 samples)\n",
      "Validation Loss: 0.1398 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.2457 (2400 samples)\n",
      "Validation Loss: 0.1395 (150 samples)\n",
      "Training Loss: [0.7750445316829154, 0.5110399488830398, 0.44277666656248393, 0.4105999293510022, 0.39114248000747126, 0.3777067940067848, 0.3676468925605038, 0.359696401181783, 0.35316589774950835, 0.34764407564123206, 0.34286841281544217, 0.33866264173004157, 0.33490357904654067, 0.3315024089706418, 0.32839342645650793, 0.3255269155791484, 0.32286457406056446, 0.3203763806490599, 0.31803846744068337, 0.3158316068515362, 0.313740064408659, 0.31175088125953593, 0.30985326306075106, 0.30803810301922785, 0.30629766269433234, 0.3046253301018349, 0.3030153729248331, 0.30146282273238045, 0.2999633226344244, 0.2985130316956536, 0.297108562254098, 0.29574688504034824, 0.2944252869199091, 0.2931413154567943, 0.29189276837128425, 0.2906776498287731, 0.2894941414338227, 0.2883405787326043, 0.28721541494550357, 0.2861172748855966, 0.28504484491539495, 0.2839969353356257, 0.28297243188748855, 0.28197029477016333, 0.28098957807561836, 0.2800293621231016, 0.27908882383167527, 0.2781671834518677, 0.2772636697565247, 0.27637761222153917, 0.27550836033817167, 0.2746552818580824, 0.2738178030345794, 0.27299537859501155, 0.2721874902521047, 0.2713936248866086, 0.27061332550002415, 0.2698461492890861, 0.2690916659640477, 0.2683494744825987, 0.2676191874710135, 0.26690044834275367, 0.26619291264353784, 0.2654962346119994, 0.264810095965597, 0.2641341932988635, 0.263468244732762, 0.262811968124447, 0.2621650716311687, 0.2615273204698599, 0.26089846885385026, 0.26027827249007934, 0.25966649920442114, 0.25906293379270634, 0.25846737560921645, 0.2578796039883719, 0.25729945093276574, 0.25672669508980306, 0.2561611791915926, 0.25560272833988523, 0.25505115699441816, 0.25450631839366084, 0.25396804698880576, 0.25343619459965905, 0.2529106201803663, 0.25239117070151307, 0.251877716020161, 0.2513701253121285, 0.2508682608497489, 0.2503720095828681, 0.24988125303896577, 0.24939586869735314, 0.24891574550597734, 0.2484407746870738, 0.2479708525090013, 0.24750587444871353, 0.24704573750297504, 0.24659035566701823, 0.24613961950605018, 0.24569346503128747]\n",
      "Validation Losses: [0.4918901380232855, 0.38947427405553486, 0.34159508347736617, 0.31218357147472614, 0.2916337482498718, 0.27618776339121776, 0.2640202963493375, 0.25411306163789554, 0.24584207008929346, 0.23879913791234056, 0.23270384751192136, 0.22735678796902875, 0.22261204687951755, 0.21836000986087478, 0.21451713189107782, 0.21101839752108553, 0.20781239588829428, 0.20485798546466252, 0.2021219079732207, 0.1995767816676267, 0.19719987853960935, 0.19497244149115917, 0.19287838372853963, 0.19090411393198525, 0.18903793012434722, 0.18726975294681203, 0.18559084761310848, 0.18399357204571284, 0.18247112622182882, 0.18101769148136893, 0.17962782858390774, 0.17829690549853236, 0.17702071795952615, 0.1757954550029556, 0.17461773338625292, 0.17348446375568763, 0.1723927215607439, 0.17134013334538759, 0.1703241657848875, 0.16934283664272842, 0.16839405783708072, 0.1674761749778065, 0.1665874291658992, 0.1657263244945524, 0.1648914170268119, 0.16408134925549606, 0.16329498699347048, 0.16253117036372286, 0.16178873050027684, 0.1610667978780933, 0.16036441579574845, 0.1596807195437008, 0.15901492828805747, 0.15836616148811405, 0.15773387000433606, 0.1571172619191429, 0.15651568817911163, 0.15592862921824377, 0.15535545100341994, 0.1547956117011509, 0.1542486788353408, 0.1537141520188635, 0.15319151624411775, 0.15268043322963934, 0.15218046294800536, 0.1516911173372917, 0.15121216472734458, 0.15074309053780816, 0.15028376323327153, 0.14983361693790515, 0.1493925996525887, 0.14896030271086333, 0.14853645552322697, 0.14812071467400442, 0.14771297331465705, 0.14731288023594524, 0.14692023292775225, 0.14653482365450496, 0.1461563588498146, 0.14578472894506983, 0.1454196757017806, 0.14506106584195416, 0.14470864980815892, 0.14436230117043045, 0.1440217521430425, 0.14368701980400125, 0.1433577848995972, 0.1430339400668031, 0.1427154229518622, 0.1424019842588454, 0.14209350212465, 0.14178994175834989, 0.1414910572443484, 0.1411967431293978, 0.14090692572558414, 0.14062154221373824, 0.14034037490975265, 0.14006340509829607, 0.13979042742893094, 0.1395214621824809]\n",
      "Hyperparameters: lr=0.01, dropout=0.5\n",
      "Testing with learning rate: 0.01, dropout rate: 0.7\n",
      "Epoch: 1/100, Train Loss: 0.7750 (2400 samples)\n",
      "Validation Loss: 0.4919 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.5110 (2400 samples)\n",
      "Validation Loss: 0.3895 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.4428 (2400 samples)\n",
      "Validation Loss: 0.3416 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.4106 (2400 samples)\n",
      "Validation Loss: 0.3122 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.3911 (2400 samples)\n",
      "Validation Loss: 0.2916 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.3777 (2400 samples)\n",
      "Validation Loss: 0.2762 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.3676 (2400 samples)\n",
      "Validation Loss: 0.2640 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.3597 (2400 samples)\n",
      "Validation Loss: 0.2541 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.3532 (2400 samples)\n",
      "Validation Loss: 0.2458 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.3476 (2400 samples)\n",
      "Validation Loss: 0.2388 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.3429 (2400 samples)\n",
      "Validation Loss: 0.2327 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.3387 (2400 samples)\n",
      "Validation Loss: 0.2274 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.3349 (2400 samples)\n",
      "Validation Loss: 0.2226 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.3315 (2400 samples)\n",
      "Validation Loss: 0.2184 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.3284 (2400 samples)\n",
      "Validation Loss: 0.2145 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.3255 (2400 samples)\n",
      "Validation Loss: 0.2110 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.3229 (2400 samples)\n",
      "Validation Loss: 0.2078 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.3204 (2400 samples)\n",
      "Validation Loss: 0.2049 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.3180 (2400 samples)\n",
      "Validation Loss: 0.2021 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.3158 (2400 samples)\n",
      "Validation Loss: 0.1996 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.3137 (2400 samples)\n",
      "Validation Loss: 0.1972 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.3118 (2400 samples)\n",
      "Validation Loss: 0.1950 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.3099 (2400 samples)\n",
      "Validation Loss: 0.1929 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.3080 (2400 samples)\n",
      "Validation Loss: 0.1909 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.3063 (2400 samples)\n",
      "Validation Loss: 0.1890 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.3046 (2400 samples)\n",
      "Validation Loss: 0.1873 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.3030 (2400 samples)\n",
      "Validation Loss: 0.1856 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.3015 (2400 samples)\n",
      "Validation Loss: 0.1840 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.3000 (2400 samples)\n",
      "Validation Loss: 0.1825 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.2985 (2400 samples)\n",
      "Validation Loss: 0.1810 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.2971 (2400 samples)\n",
      "Validation Loss: 0.1796 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.2957 (2400 samples)\n",
      "Validation Loss: 0.1783 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.2944 (2400 samples)\n",
      "Validation Loss: 0.1770 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.2931 (2400 samples)\n",
      "Validation Loss: 0.1758 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.2919 (2400 samples)\n",
      "Validation Loss: 0.1746 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.2907 (2400 samples)\n",
      "Validation Loss: 0.1735 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.2895 (2400 samples)\n",
      "Validation Loss: 0.1724 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.2883 (2400 samples)\n",
      "Validation Loss: 0.1713 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.2872 (2400 samples)\n",
      "Validation Loss: 0.1703 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.2861 (2400 samples)\n",
      "Validation Loss: 0.1693 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.2850 (2400 samples)\n",
      "Validation Loss: 0.1684 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.2840 (2400 samples)\n",
      "Validation Loss: 0.1675 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.2830 (2400 samples)\n",
      "Validation Loss: 0.1666 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.2820 (2400 samples)\n",
      "Validation Loss: 0.1657 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.2810 (2400 samples)\n",
      "Validation Loss: 0.1649 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.2800 (2400 samples)\n",
      "Validation Loss: 0.1641 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.2791 (2400 samples)\n",
      "Validation Loss: 0.1633 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.2782 (2400 samples)\n",
      "Validation Loss: 0.1625 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.2773 (2400 samples)\n",
      "Validation Loss: 0.1618 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.2764 (2400 samples)\n",
      "Validation Loss: 0.1611 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.2755 (2400 samples)\n",
      "Validation Loss: 0.1604 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.2747 (2400 samples)\n",
      "Validation Loss: 0.1597 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.2738 (2400 samples)\n",
      "Validation Loss: 0.1590 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.2730 (2400 samples)\n",
      "Validation Loss: 0.1584 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.2722 (2400 samples)\n",
      "Validation Loss: 0.1577 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.2714 (2400 samples)\n",
      "Validation Loss: 0.1571 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.2706 (2400 samples)\n",
      "Validation Loss: 0.1565 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.2698 (2400 samples)\n",
      "Validation Loss: 0.1559 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.2691 (2400 samples)\n",
      "Validation Loss: 0.1554 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.2683 (2400 samples)\n",
      "Validation Loss: 0.1548 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.2676 (2400 samples)\n",
      "Validation Loss: 0.1542 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.2669 (2400 samples)\n",
      "Validation Loss: 0.1537 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.2662 (2400 samples)\n",
      "Validation Loss: 0.1532 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.2655 (2400 samples)\n",
      "Validation Loss: 0.1527 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.2648 (2400 samples)\n",
      "Validation Loss: 0.1522 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.2641 (2400 samples)\n",
      "Validation Loss: 0.1517 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.2635 (2400 samples)\n",
      "Validation Loss: 0.1512 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.2628 (2400 samples)\n",
      "Validation Loss: 0.1507 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.2622 (2400 samples)\n",
      "Validation Loss: 0.1503 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.2615 (2400 samples)\n",
      "Validation Loss: 0.1498 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.2609 (2400 samples)\n",
      "Validation Loss: 0.1494 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.2603 (2400 samples)\n",
      "Validation Loss: 0.1490 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.2597 (2400 samples)\n",
      "Validation Loss: 0.1485 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.2591 (2400 samples)\n",
      "Validation Loss: 0.1481 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.2585 (2400 samples)\n",
      "Validation Loss: 0.1477 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.2579 (2400 samples)\n",
      "Validation Loss: 0.1473 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.2573 (2400 samples)\n",
      "Validation Loss: 0.1469 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.2567 (2400 samples)\n",
      "Validation Loss: 0.1465 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.2562 (2400 samples)\n",
      "Validation Loss: 0.1462 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.2556 (2400 samples)\n",
      "Validation Loss: 0.1458 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.2551 (2400 samples)\n",
      "Validation Loss: 0.1454 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.2545 (2400 samples)\n",
      "Validation Loss: 0.1451 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.2540 (2400 samples)\n",
      "Validation Loss: 0.1447 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.2534 (2400 samples)\n",
      "Validation Loss: 0.1444 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.2529 (2400 samples)\n",
      "Validation Loss: 0.1440 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.2524 (2400 samples)\n",
      "Validation Loss: 0.1437 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.2519 (2400 samples)\n",
      "Validation Loss: 0.1434 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.2514 (2400 samples)\n",
      "Validation Loss: 0.1430 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.2509 (2400 samples)\n",
      "Validation Loss: 0.1427 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.2504 (2400 samples)\n",
      "Validation Loss: 0.1424 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.2499 (2400 samples)\n",
      "Validation Loss: 0.1421 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.2494 (2400 samples)\n",
      "Validation Loss: 0.1418 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.2489 (2400 samples)\n",
      "Validation Loss: 0.1415 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.2484 (2400 samples)\n",
      "Validation Loss: 0.1412 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.2480 (2400 samples)\n",
      "Validation Loss: 0.1409 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.2475 (2400 samples)\n",
      "Validation Loss: 0.1406 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.2470 (2400 samples)\n",
      "Validation Loss: 0.1403 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.2466 (2400 samples)\n",
      "Validation Loss: 0.1401 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.2461 (2400 samples)\n",
      "Validation Loss: 0.1398 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.2457 (2400 samples)\n",
      "Validation Loss: 0.1395 (150 samples)\n",
      "Training Loss: [0.7750445316829154, 0.5110399488830398, 0.44277666656248393, 0.4105999293510022, 0.39114248000747126, 0.3777067940067848, 0.3676468925605038, 0.359696401181783, 0.35316589774950835, 0.34764407564123206, 0.34286841281544217, 0.33866264173004157, 0.33490357904654067, 0.3315024089706418, 0.32839342645650793, 0.3255269155791484, 0.32286457406056446, 0.3203763806490599, 0.31803846744068337, 0.3158316068515362, 0.313740064408659, 0.31175088125953593, 0.30985326306075106, 0.30803810301922785, 0.30629766269433234, 0.3046253301018349, 0.3030153729248331, 0.30146282273238045, 0.2999633226344244, 0.2985130316956536, 0.297108562254098, 0.29574688504034824, 0.2944252869199091, 0.2931413154567943, 0.29189276837128425, 0.2906776498287731, 0.2894941414338227, 0.2883405787326043, 0.28721541494550357, 0.2861172748855966, 0.28504484491539495, 0.2839969353356257, 0.28297243188748855, 0.28197029477016333, 0.28098957807561836, 0.2800293621231016, 0.27908882383167527, 0.2781671834518677, 0.2772636697565247, 0.27637761222153917, 0.27550836033817167, 0.2746552818580824, 0.2738178030345794, 0.27299537859501155, 0.2721874902521047, 0.2713936248866086, 0.27061332550002415, 0.2698461492890861, 0.2690916659640477, 0.2683494744825987, 0.2676191874710135, 0.26690044834275367, 0.26619291264353784, 0.2654962346119994, 0.264810095965597, 0.2641341932988635, 0.263468244732762, 0.262811968124447, 0.2621650716311687, 0.2615273204698599, 0.26089846885385026, 0.26027827249007934, 0.25966649920442114, 0.25906293379270634, 0.25846737560921645, 0.2578796039883719, 0.25729945093276574, 0.25672669508980306, 0.2561611791915926, 0.25560272833988523, 0.25505115699441816, 0.25450631839366084, 0.25396804698880576, 0.25343619459965905, 0.2529106201803663, 0.25239117070151307, 0.251877716020161, 0.2513701253121285, 0.2508682608497489, 0.2503720095828681, 0.24988125303896577, 0.24939586869735314, 0.24891574550597734, 0.2484407746870738, 0.2479708525090013, 0.24750587444871353, 0.24704573750297504, 0.24659035566701823, 0.24613961950605018, 0.24569346503128747]\n",
      "Validation Losses: [0.4918901380232855, 0.38947427405553486, 0.34159508347736617, 0.31218357147472614, 0.2916337482498718, 0.27618776339121776, 0.2640202963493375, 0.25411306163789554, 0.24584207008929346, 0.23879913791234056, 0.23270384751192136, 0.22735678796902875, 0.22261204687951755, 0.21836000986087478, 0.21451713189107782, 0.21101839752108553, 0.20781239588829428, 0.20485798546466252, 0.2021219079732207, 0.1995767816676267, 0.19719987853960935, 0.19497244149115917, 0.19287838372853963, 0.19090411393198525, 0.18903793012434722, 0.18726975294681203, 0.18559084761310848, 0.18399357204571284, 0.18247112622182882, 0.18101769148136893, 0.17962782858390774, 0.17829690549853236, 0.17702071795952615, 0.1757954550029556, 0.17461773338625292, 0.17348446375568763, 0.1723927215607439, 0.17134013334538759, 0.1703241657848875, 0.16934283664272842, 0.16839405783708072, 0.1674761749778065, 0.1665874291658992, 0.1657263244945524, 0.1648914170268119, 0.16408134925549606, 0.16329498699347048, 0.16253117036372286, 0.16178873050027684, 0.1610667978780933, 0.16036441579574845, 0.1596807195437008, 0.15901492828805747, 0.15836616148811405, 0.15773387000433606, 0.1571172619191429, 0.15651568817911163, 0.15592862921824377, 0.15535545100341994, 0.1547956117011509, 0.1542486788353408, 0.1537141520188635, 0.15319151624411775, 0.15268043322963934, 0.15218046294800536, 0.1516911173372917, 0.15121216472734458, 0.15074309053780816, 0.15028376323327153, 0.14983361693790515, 0.1493925996525887, 0.14896030271086333, 0.14853645552322697, 0.14812071467400442, 0.14771297331465705, 0.14731288023594524, 0.14692023292775225, 0.14653482365450496, 0.1461563588498146, 0.14578472894506983, 0.1454196757017806, 0.14506106584195416, 0.14470864980815892, 0.14436230117043045, 0.1440217521430425, 0.14368701980400125, 0.1433577848995972, 0.1430339400668031, 0.1427154229518622, 0.1424019842588454, 0.14209350212465, 0.14178994175834989, 0.1414910572443484, 0.1411967431293978, 0.14090692572558414, 0.14062154221373824, 0.14034037490975265, 0.14006340509829607, 0.13979042742893094, 0.1395214621824809]\n",
      "Hyperparameters: lr=0.01, dropout=0.7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define lists of hyperparameters to test\n",
    "learning_rates = [0.001, 0.01]\n",
    "dropout_rates = [0.3, 0.5, 0.7]\n",
    "\n",
    "# Initialize dictionaries to store losses for each hyperparameter combination\n",
    "train_loss_dict = {}\n",
    "dev_loss_dict = {}\n",
    "\n",
    "\n",
    "# Iterate over different combinations of hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(f\"Testing with learning rate: {lr}, dropout rate: {dropout_rate}\")\n",
    "        \n",
    "        #Initialize network weights\n",
    "        W = network_weights(vocab_size=len(word2id), embedding_dim=300, hidden_dim=[], num_classes=3)\n",
    "\n",
    "        #Replace embedding matrix weights with w_glove\n",
    "        W[0] = w_glove\n",
    "\n",
    "        # Train the network\n",
    "        W, loss_tr, dev_losses = SGD(X_tr=train_sequences, Y_tr=train_y,\n",
    "                                    W=W,\n",
    "                                    X_dev=dev_sequences,\n",
    "                                    Y_dev=dev_y,\n",
    "                                    lr=lr,\n",
    "                                    dropout=dropout_rate,\n",
    "                                    freeze_emb=True,\n",
    "                                    tolerance=1e-12,\n",
    "                                    epochs=100)\n",
    "\n",
    "        # Store losses for this hyperparameter combination\n",
    "        key = (lr, dropout_rate)\n",
    "        train_loss_dict[key] = loss_tr\n",
    "        dev_loss_dict[key] = dev_losses\n",
    "\n",
    "        # Optionally, you can print or store the results for each combination\n",
    "        print(\"Training Loss:\", loss_tr)\n",
    "        print(\"Validation Losses:\", dev_losses)\n",
    "        print(\"Hyperparameters:\", f\"lr={lr}, dropout={dropout_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:30:11.121198Z",
     "start_time": "2020-04-02T14:29:24.946124Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 1.1070 (2400 samples)\n",
      "Validation Loss: 1.0114 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.9501 (2400 samples)\n",
      "Validation Loss: 0.9116 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.8593 (2400 samples)\n",
      "Validation Loss: 0.8314 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.7883 (2400 samples)\n",
      "Validation Loss: 0.7668 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.7322 (2400 samples)\n",
      "Validation Loss: 0.7141 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.6873 (2400 samples)\n",
      "Validation Loss: 0.6706 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.6508 (2400 samples)\n",
      "Validation Loss: 0.6343 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.6206 (2400 samples)\n",
      "Validation Loss: 0.6035 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.5954 (2400 samples)\n",
      "Validation Loss: 0.5771 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.5741 (2400 samples)\n",
      "Validation Loss: 0.5542 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.5559 (2400 samples)\n",
      "Validation Loss: 0.5342 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.5401 (2400 samples)\n",
      "Validation Loss: 0.5165 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.5263 (2400 samples)\n",
      "Validation Loss: 0.5008 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.5142 (2400 samples)\n",
      "Validation Loss: 0.4868 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.5035 (2400 samples)\n",
      "Validation Loss: 0.4741 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.4939 (2400 samples)\n",
      "Validation Loss: 0.4626 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.4852 (2400 samples)\n",
      "Validation Loss: 0.4521 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.4775 (2400 samples)\n",
      "Validation Loss: 0.4424 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.4704 (2400 samples)\n",
      "Validation Loss: 0.4336 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.4639 (2400 samples)\n",
      "Validation Loss: 0.4254 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.4580 (2400 samples)\n",
      "Validation Loss: 0.4178 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.4526 (2400 samples)\n",
      "Validation Loss: 0.4107 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.4475 (2400 samples)\n",
      "Validation Loss: 0.4040 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.4429 (2400 samples)\n",
      "Validation Loss: 0.3978 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.4385 (2400 samples)\n",
      "Validation Loss: 0.3920 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.4345 (2400 samples)\n",
      "Validation Loss: 0.3865 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.4307 (2400 samples)\n",
      "Validation Loss: 0.3813 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.4271 (2400 samples)\n",
      "Validation Loss: 0.3764 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.4238 (2400 samples)\n",
      "Validation Loss: 0.3718 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.4206 (2400 samples)\n",
      "Validation Loss: 0.3673 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.4176 (2400 samples)\n",
      "Validation Loss: 0.3631 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.4148 (2400 samples)\n",
      "Validation Loss: 0.3591 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.4121 (2400 samples)\n",
      "Validation Loss: 0.3553 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.4096 (2400 samples)\n",
      "Validation Loss: 0.3516 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.4072 (2400 samples)\n",
      "Validation Loss: 0.3481 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.4049 (2400 samples)\n",
      "Validation Loss: 0.3448 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.4027 (2400 samples)\n",
      "Validation Loss: 0.3415 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.4006 (2400 samples)\n",
      "Validation Loss: 0.3384 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.3986 (2400 samples)\n",
      "Validation Loss: 0.3355 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.3966 (2400 samples)\n",
      "Validation Loss: 0.3326 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.3948 (2400 samples)\n",
      "Validation Loss: 0.3298 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.3930 (2400 samples)\n",
      "Validation Loss: 0.3272 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.3913 (2400 samples)\n",
      "Validation Loss: 0.3246 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.3897 (2400 samples)\n",
      "Validation Loss: 0.3221 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.3881 (2400 samples)\n",
      "Validation Loss: 0.3197 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.3865 (2400 samples)\n",
      "Validation Loss: 0.3173 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.3851 (2400 samples)\n",
      "Validation Loss: 0.3151 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.3836 (2400 samples)\n",
      "Validation Loss: 0.3129 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.3823 (2400 samples)\n",
      "Validation Loss: 0.3108 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.3809 (2400 samples)\n",
      "Validation Loss: 0.3087 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.3796 (2400 samples)\n",
      "Validation Loss: 0.3067 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.3784 (2400 samples)\n",
      "Validation Loss: 0.3048 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.3771 (2400 samples)\n",
      "Validation Loss: 0.3029 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.3760 (2400 samples)\n",
      "Validation Loss: 0.3010 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.3748 (2400 samples)\n",
      "Validation Loss: 0.2993 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.3737 (2400 samples)\n",
      "Validation Loss: 0.2975 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.3726 (2400 samples)\n",
      "Validation Loss: 0.2958 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.3715 (2400 samples)\n",
      "Validation Loss: 0.2942 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.3705 (2400 samples)\n",
      "Validation Loss: 0.2926 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.3695 (2400 samples)\n",
      "Validation Loss: 0.2910 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.3685 (2400 samples)\n",
      "Validation Loss: 0.2895 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.3676 (2400 samples)\n",
      "Validation Loss: 0.2880 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.3666 (2400 samples)\n",
      "Validation Loss: 0.2865 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.3657 (2400 samples)\n",
      "Validation Loss: 0.2851 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.3648 (2400 samples)\n",
      "Validation Loss: 0.2837 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.3640 (2400 samples)\n",
      "Validation Loss: 0.2823 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.3631 (2400 samples)\n",
      "Validation Loss: 0.2810 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.3623 (2400 samples)\n",
      "Validation Loss: 0.2797 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.3615 (2400 samples)\n",
      "Validation Loss: 0.2784 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.3607 (2400 samples)\n",
      "Validation Loss: 0.2772 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.3599 (2400 samples)\n",
      "Validation Loss: 0.2759 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.3592 (2400 samples)\n",
      "Validation Loss: 0.2747 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.3584 (2400 samples)\n",
      "Validation Loss: 0.2736 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.3577 (2400 samples)\n",
      "Validation Loss: 0.2724 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.3570 (2400 samples)\n",
      "Validation Loss: 0.2713 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.3563 (2400 samples)\n",
      "Validation Loss: 0.2702 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.3556 (2400 samples)\n",
      "Validation Loss: 0.2691 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.3549 (2400 samples)\n",
      "Validation Loss: 0.2681 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.3542 (2400 samples)\n",
      "Validation Loss: 0.2670 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.3536 (2400 samples)\n",
      "Validation Loss: 0.2660 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.3529 (2400 samples)\n",
      "Validation Loss: 0.2650 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.3523 (2400 samples)\n",
      "Validation Loss: 0.2640 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.3517 (2400 samples)\n",
      "Validation Loss: 0.2630 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.3511 (2400 samples)\n",
      "Validation Loss: 0.2621 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.3505 (2400 samples)\n",
      "Validation Loss: 0.2612 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.3499 (2400 samples)\n",
      "Validation Loss: 0.2602 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.3493 (2400 samples)\n",
      "Validation Loss: 0.2593 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.3487 (2400 samples)\n",
      "Validation Loss: 0.2585 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.3482 (2400 samples)\n",
      "Validation Loss: 0.2576 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.3476 (2400 samples)\n",
      "Validation Loss: 0.2567 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.3471 (2400 samples)\n",
      "Validation Loss: 0.2559 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.3465 (2400 samples)\n",
      "Validation Loss: 0.2551 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.3460 (2400 samples)\n",
      "Validation Loss: 0.2542 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.3455 (2400 samples)\n",
      "Validation Loss: 0.2534 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.3450 (2400 samples)\n",
      "Validation Loss: 0.2527 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.3445 (2400 samples)\n",
      "Validation Loss: 0.2519 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.3440 (2400 samples)\n",
      "Validation Loss: 0.2511 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.3435 (2400 samples)\n",
      "Validation Loss: 0.2504 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.3430 (2400 samples)\n",
      "Validation Loss: 0.2496 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.3425 (2400 samples)\n",
      "Validation Loss: 0.2489 (150 samples)\n",
      "[1.1070415115148897, 0.9500802240702964, 0.8593045006295665, 0.7883415357531376, 0.7322487643623462, 0.6873009046723955, 0.6507568807116607, 0.6206188697769726, 0.5954277415091923, 0.5741091416287273, 0.5558630382152692, 0.5400861542426492, 0.5263175675808156, 0.5142007454493034, 0.5034564908145143, 0.4938634492944139, 0.4852444493919377, 0.4774562071900656, 0.4703817512620979, 0.4639247875479525, 0.45800526732983066, 0.4525563525363883, 0.4475217133983154, 0.44285358786805556, 0.43851120756565054, 0.43445972998048515, 0.4306689828893201, 0.4271128736344132, 0.4237686709406773, 0.4206165388744679, 0.4176390542893778, 0.41482079630749397, 0.41214816439480845, 0.40960913082659545, 0.4071929084683667, 0.4048898563871437, 0.4026913629387125, 0.40058967301681775, 0.39857777247161974, 0.3966493773617007, 0.39479873098547896, 0.39302068914422866, 0.3913104572400993, 0.38966372460789694, 0.38807659153284985, 0.38654536683765045, 0.3850667767014707, 0.38363773884001345, 0.38225542737708007, 0.3809173021427446, 0.37962089083711703, 0.37836398865108306, 0.37714455135995256, 0.3759606771305644, 0.3748105325090196, 0.3736924800404277, 0.3726049416439831, 0.37154648914363764, 0.37051578005535957, 0.36951154300946876, 0.36853258877005934, 0.36757779284835507, 0.36664611665376123, 0.3657365672642817, 0.3648482102120022, 0.36398018530238313, 0.36313167327704915, 0.36230188433506055, 0.36149008699400953, 0.3606955784470811, 0.35991769185928074, 0.35915580484435433, 0.35840934910704325, 0.35767773044958867, 0.3569603961321517, 0.3562568809918779, 0.35556670962703074, 0.35488938436335055, 0.35422447252772066, 0.35357156559652353, 0.3529302686238082, 0.35230019399924967, 0.3516809819934354, 0.351072278630984, 0.3504737594395413, 0.3498851140055246, 0.3493060128526166, 0.3487362014340677, 0.3481753711405864, 0.3476232555363239, 0.34707957124895067, 0.34654414416068885, 0.3460167092776274, 0.3454970056769438, 0.3449848616328942, 0.3444800077771928, 0.34398231358791587, 0.343491544097636, 0.34300752520895583, 0.3425300879963003]\n",
      "[1.011430744778455, 0.9116352876732297, 0.8314176224866846, 0.7667927170307285, 0.7141280545375283, 0.6706495344549985, 0.6342807995170966, 0.6034751864633493, 0.5770746082688187, 0.5542052236506959, 0.534200612344194, 0.5165467270895654, 0.5008422591197508, 0.48677058049563937, 0.47407902664386825, 0.4625639755599903, 0.4520598485766943, 0.44243036838278355, 0.4335630457207652, 0.4253639550772068, 0.4177541050724764, 0.4106664057644836, 0.404044254709774, 0.3978385213568647, 0.3920072633280516, 0.38651419021513306, 0.38132717053877613, 0.3764187945231232, 0.37176455894906935, 0.36734307610201733, 0.36313523504962875, 0.3591238054009115, 0.35529419496626113, 0.35163269885550225, 0.3481269826283049, 0.3447662553410035, 0.3415404986099914, 0.33844125995393637, 0.3354595615788791, 0.3325885084074602, 0.32982128967249746, 0.32715181395400966, 0.32457415026644126, 0.3220830192729829, 0.3196740687744959, 0.3173424962749886, 0.31508425585567024, 0.31289556306243055, 0.3107729266470402, 0.3087130926326152, 0.30671304781464964, 0.304769869489999, 0.3028809950239179, 0.30104392770178195, 0.29925609137586784, 0.2975158548880613, 0.2958206890723116, 0.29416880587877386, 0.29255836304903315, 0.2909878661814969, 0.28945554204464197, 0.28796008432296316, 0.2864998090515375, 0.2850733369714643, 0.2836794312041556, 0.282317141787064, 0.2809850280028943, 0.27968215217879683, 0.2784074318850175, 0.27715996772429713, 0.2759386036190057, 0.2747423919844758, 0.2735708911596186, 0.27242294268547573, 0.27129792332357483, 0.27019492438110226, 0.26911337059062884, 0.2680527109066677, 0.26701215407733303, 0.26599096688134055, 0.26498874459617366, 0.26400486209337953, 0.2630387515779692, 0.26208988576534076, 0.2611577329145141, 0.260241873146038, 0.25934174304744245, 0.25845709597135735, 0.25758729052934204, 0.2567320138192517, 0.25589084600017503, 0.25506344167580003, 0.25424939622526277, 0.2534483194721926, 0.25265995458324014, 0.2518838923229464, 0.25111989775354693, 0.2503675634774849, 0.24962664176124463, 0.2488969480386219]\n"
     ]
    }
   ],
   "source": [
    "#Initialize network weights\n",
    "W = network_weights(vocab_size=len(word2id), embedding_dim=300, hidden_dim=[], num_classes=3)\n",
    "\n",
    "#Replace embedding matrix weights with w_glove\n",
    "W[0] = w_glove\n",
    "\n",
    "#Train the network with frozen embedding weights\n",
    "W, loss_tr, dev_loss = SGD(X_tr=train_sequences, Y_tr=train_y,\n",
    "                            W=W,\n",
    "                            X_dev=dev_sequences,\n",
    "                            Y_dev=dev_y,\n",
    "                            lr=0.001,\n",
    "                            dropout=0.7,\n",
    "                            freeze_emb=True,  # Set freeze_emb to True to freeze embedding weights\n",
    "                            tolerance=1e-12,\n",
    "                            epochs=100)\n",
    "\n",
    "# Print or plot training and validation losses\n",
    "print(loss_tr)\n",
    "print(dev_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8766666666666667\n",
      "Precision: 0.877113226062642\n",
      "Recall: 0.8766666666666666\n",
      "F1-Score: 0.8765231120343734\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['pred']) \n",
    "            for x,y in zip(test_sequences,test_y)]\n",
    "\n",
    "preds_te_adjusted = [pred + 1 for pred in preds_te]\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_y,preds_te_adjusted))\n",
    "print('Precision:', precision_score(test_y,preds_te_adjusted,average='macro'))\n",
    "print('Recall:', recall_score(test_y,preds_te_adjusted,average='macro'))\n",
    "print('F1-Score:', f1_score(test_y,preds_te_adjusted,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning process\n",
    "plt.plot(range(1, len(loss_tr) + 1), loss_tr, label='Training Loss')\n",
    "plt.plot(range(1, len(dev_loss) + 1), dev_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Process')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "- Learning Rate: This parameter controls the magnitude of updates to the model's weights during training. We investigated learning rates of 0.001 and 0.01.\n",
    "- Dropout Rate: Dropout helps prevent overfitting by randomly deactivating neurons during training. Dropout rates of 0.3, 0.5, and 0.7 were examined.\n",
    "\n",
    "Tuning Process\n",
    "- Grid Definition: A grid encompassing all combinations of the chosen learning rates and dropout rates was established.\n",
    "- Model Training: For each combination in the grid, a separate instance of the neural network was trained using the SGD optimizer with frozen embedding weights (using the freeze_emb=True argument).\n",
    "- Performance Evaluation: The training loss and validation loss were monitored throughout training for each model configuration. The validation loss serves as a crucial metric for assessing model generalization to unseen data.\n",
    "\n",
    "By employing grid search for hyperparameter tuning and analyzing the performance of models with varying architectures (without including hidden layers), we were able to identify a configuration that resulted in a well-performing model on the validation set, achieving an accuracy of 87.67%, precision of 87.71%, recall of 87.67%, and F1-score of 87.65%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures \n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Train Loss: 1.1770 (2400 samples)\n",
      "Validation Loss: 1.1406 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 1.1269 (2400 samples)\n",
      "Validation Loss: 1.1040 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 1.0956 (2400 samples)\n",
      "Validation Loss: 1.0812 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 1.0734 (2400 samples)\n",
      "Validation Loss: 1.0659 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 1.0558 (2400 samples)\n",
      "Validation Loss: 1.0534 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 1.0407 (2400 samples)\n",
      "Validation Loss: 1.0427 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 1.0269 (2400 samples)\n",
      "Validation Loss: 1.0323 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 1.0140 (2400 samples)\n",
      "Validation Loss: 1.0223 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 1.0017 (2400 samples)\n",
      "Validation Loss: 1.0127 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.9896 (2400 samples)\n",
      "Validation Loss: 1.0029 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.9778 (2400 samples)\n",
      "Validation Loss: 0.9931 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.9660 (2400 samples)\n",
      "Validation Loss: 0.9831 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.9542 (2400 samples)\n",
      "Validation Loss: 0.9730 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.9425 (2400 samples)\n",
      "Validation Loss: 0.9626 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.9306 (2400 samples)\n",
      "Validation Loss: 0.9521 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.9188 (2400 samples)\n",
      "Validation Loss: 0.9414 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.9069 (2400 samples)\n",
      "Validation Loss: 0.9307 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.8951 (2400 samples)\n",
      "Validation Loss: 0.9198 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.8832 (2400 samples)\n",
      "Validation Loss: 0.9089 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.8714 (2400 samples)\n",
      "Validation Loss: 0.8981 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.8596 (2400 samples)\n",
      "Validation Loss: 0.8872 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.8479 (2400 samples)\n",
      "Validation Loss: 0.8761 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.8362 (2400 samples)\n",
      "Validation Loss: 0.8652 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.8247 (2400 samples)\n",
      "Validation Loss: 0.8543 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.8133 (2400 samples)\n",
      "Validation Loss: 0.8435 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.8020 (2400 samples)\n",
      "Validation Loss: 0.8329 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.7908 (2400 samples)\n",
      "Validation Loss: 0.8223 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.7798 (2400 samples)\n",
      "Validation Loss: 0.8117 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.7690 (2400 samples)\n",
      "Validation Loss: 0.8013 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.7584 (2400 samples)\n",
      "Validation Loss: 0.7910 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.7480 (2400 samples)\n",
      "Validation Loss: 0.7809 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.7378 (2400 samples)\n",
      "Validation Loss: 0.7710 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.7278 (2400 samples)\n",
      "Validation Loss: 0.7613 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.7181 (2400 samples)\n",
      "Validation Loss: 0.7518 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.7085 (2400 samples)\n",
      "Validation Loss: 0.7425 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.6993 (2400 samples)\n",
      "Validation Loss: 0.7333 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.6902 (2400 samples)\n",
      "Validation Loss: 0.7244 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.6814 (2400 samples)\n",
      "Validation Loss: 0.7156 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.6728 (2400 samples)\n",
      "Validation Loss: 0.7070 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.6645 (2400 samples)\n",
      "Validation Loss: 0.6986 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.6564 (2400 samples)\n",
      "Validation Loss: 0.6903 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.6486 (2400 samples)\n",
      "Validation Loss: 0.6823 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.6410 (2400 samples)\n",
      "Validation Loss: 0.6743 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.6336 (2400 samples)\n",
      "Validation Loss: 0.6665 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.6265 (2400 samples)\n",
      "Validation Loss: 0.6589 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.6196 (2400 samples)\n",
      "Validation Loss: 0.6515 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.6129 (2400 samples)\n",
      "Validation Loss: 0.6442 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.6064 (2400 samples)\n",
      "Validation Loss: 0.6371 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.6002 (2400 samples)\n",
      "Validation Loss: 0.6301 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.5941 (2400 samples)\n",
      "Validation Loss: 0.6233 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.5882 (2400 samples)\n",
      "Validation Loss: 0.6167 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.5825 (2400 samples)\n",
      "Validation Loss: 0.6103 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.5770 (2400 samples)\n",
      "Validation Loss: 0.6040 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.5717 (2400 samples)\n",
      "Validation Loss: 0.5978 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.5666 (2400 samples)\n",
      "Validation Loss: 0.5919 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.5616 (2400 samples)\n",
      "Validation Loss: 0.5860 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.5568 (2400 samples)\n",
      "Validation Loss: 0.5803 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.5521 (2400 samples)\n",
      "Validation Loss: 0.5747 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.5476 (2400 samples)\n",
      "Validation Loss: 0.5693 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.5432 (2400 samples)\n",
      "Validation Loss: 0.5640 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.5390 (2400 samples)\n",
      "Validation Loss: 0.5588 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.5349 (2400 samples)\n",
      "Validation Loss: 0.5538 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.5309 (2400 samples)\n",
      "Validation Loss: 0.5489 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.5271 (2400 samples)\n",
      "Validation Loss: 0.5441 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.5233 (2400 samples)\n",
      "Validation Loss: 0.5393 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.5197 (2400 samples)\n",
      "Validation Loss: 0.5348 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.5162 (2400 samples)\n",
      "Validation Loss: 0.5303 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.5128 (2400 samples)\n",
      "Validation Loss: 0.5259 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.5095 (2400 samples)\n",
      "Validation Loss: 0.5216 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.5062 (2400 samples)\n",
      "Validation Loss: 0.5175 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.5031 (2400 samples)\n",
      "Validation Loss: 0.5134 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.5001 (2400 samples)\n",
      "Validation Loss: 0.5094 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.4971 (2400 samples)\n",
      "Validation Loss: 0.5055 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.4943 (2400 samples)\n",
      "Validation Loss: 0.5018 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.4915 (2400 samples)\n",
      "Validation Loss: 0.4980 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.4888 (2400 samples)\n",
      "Validation Loss: 0.4944 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.4861 (2400 samples)\n",
      "Validation Loss: 0.4908 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.4836 (2400 samples)\n",
      "Validation Loss: 0.4873 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.4811 (2400 samples)\n",
      "Validation Loss: 0.4839 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.4787 (2400 samples)\n",
      "Validation Loss: 0.4806 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.4763 (2400 samples)\n",
      "Validation Loss: 0.4773 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.4740 (2400 samples)\n",
      "Validation Loss: 0.4741 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.4718 (2400 samples)\n",
      "Validation Loss: 0.4709 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.4696 (2400 samples)\n",
      "Validation Loss: 0.4678 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.4674 (2400 samples)\n",
      "Validation Loss: 0.4648 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.4654 (2400 samples)\n",
      "Validation Loss: 0.4618 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.4633 (2400 samples)\n",
      "Validation Loss: 0.4589 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.4614 (2400 samples)\n",
      "Validation Loss: 0.4560 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.4594 (2400 samples)\n",
      "Validation Loss: 0.4532 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.4575 (2400 samples)\n",
      "Validation Loss: 0.4505 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.4557 (2400 samples)\n",
      "Validation Loss: 0.4478 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.4539 (2400 samples)\n",
      "Validation Loss: 0.4451 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.4521 (2400 samples)\n",
      "Validation Loss: 0.4425 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.4504 (2400 samples)\n",
      "Validation Loss: 0.4399 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.4487 (2400 samples)\n",
      "Validation Loss: 0.4374 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.4471 (2400 samples)\n",
      "Validation Loss: 0.4350 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.4454 (2400 samples)\n",
      "Validation Loss: 0.4325 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.4439 (2400 samples)\n",
      "Validation Loss: 0.4301 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.4423 (2400 samples)\n",
      "Validation Loss: 0.4278 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.4408 (2400 samples)\n",
      "Validation Loss: 0.4255 (150 samples)\n",
      "Epoch: 1/100, Train Loss: 1.0777 (2400 samples)\n",
      "Validation Loss: 1.0867 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 1.0689 (2400 samples)\n",
      "Validation Loss: 1.0768 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 1.0612 (2400 samples)\n",
      "Validation Loss: 1.0679 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 1.0540 (2400 samples)\n",
      "Validation Loss: 1.0598 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 1.0468 (2400 samples)\n",
      "Validation Loss: 1.0520 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 1.0394 (2400 samples)\n",
      "Validation Loss: 1.0446 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 1.0317 (2400 samples)\n",
      "Validation Loss: 1.0367 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 1.0233 (2400 samples)\n",
      "Validation Loss: 1.0282 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 1.0143 (2400 samples)\n",
      "Validation Loss: 1.0191 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 1.0042 (2400 samples)\n",
      "Validation Loss: 1.0091 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.9930 (2400 samples)\n",
      "Validation Loss: 0.9979 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.9804 (2400 samples)\n",
      "Validation Loss: 0.9851 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.9664 (2400 samples)\n",
      "Validation Loss: 0.9707 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.9508 (2400 samples)\n",
      "Validation Loss: 0.9546 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.9337 (2400 samples)\n",
      "Validation Loss: 0.9370 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.9151 (2400 samples)\n",
      "Validation Loss: 0.9183 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.8951 (2400 samples)\n",
      "Validation Loss: 0.8983 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.8737 (2400 samples)\n",
      "Validation Loss: 0.8773 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.8511 (2400 samples)\n",
      "Validation Loss: 0.8544 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.8275 (2400 samples)\n",
      "Validation Loss: 0.8305 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.8032 (2400 samples)\n",
      "Validation Loss: 0.8057 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.7787 (2400 samples)\n",
      "Validation Loss: 0.7808 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.7542 (2400 samples)\n",
      "Validation Loss: 0.7564 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.7303 (2400 samples)\n",
      "Validation Loss: 0.7327 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.7070 (2400 samples)\n",
      "Validation Loss: 0.7096 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.6845 (2400 samples)\n",
      "Validation Loss: 0.6875 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.6631 (2400 samples)\n",
      "Validation Loss: 0.6662 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.6428 (2400 samples)\n",
      "Validation Loss: 0.6459 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.6237 (2400 samples)\n",
      "Validation Loss: 0.6269 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.6058 (2400 samples)\n",
      "Validation Loss: 0.6091 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.5893 (2400 samples)\n",
      "Validation Loss: 0.5925 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.5741 (2400 samples)\n",
      "Validation Loss: 0.5771 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.5602 (2400 samples)\n",
      "Validation Loss: 0.5630 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.5474 (2400 samples)\n",
      "Validation Loss: 0.5501 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.5357 (2400 samples)\n",
      "Validation Loss: 0.5380 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.5250 (2400 samples)\n",
      "Validation Loss: 0.5269 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.5153 (2400 samples)\n",
      "Validation Loss: 0.5166 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.5063 (2400 samples)\n",
      "Validation Loss: 0.5071 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.4982 (2400 samples)\n",
      "Validation Loss: 0.4983 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.4908 (2400 samples)\n",
      "Validation Loss: 0.4901 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.4840 (2400 samples)\n",
      "Validation Loss: 0.4826 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.4778 (2400 samples)\n",
      "Validation Loss: 0.4755 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.4721 (2400 samples)\n",
      "Validation Loss: 0.4689 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.4669 (2400 samples)\n",
      "Validation Loss: 0.4628 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.4621 (2400 samples)\n",
      "Validation Loss: 0.4572 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.4576 (2400 samples)\n",
      "Validation Loss: 0.4518 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.4535 (2400 samples)\n",
      "Validation Loss: 0.4467 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.4497 (2400 samples)\n",
      "Validation Loss: 0.4420 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.4461 (2400 samples)\n",
      "Validation Loss: 0.4374 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.4427 (2400 samples)\n",
      "Validation Loss: 0.4331 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.4395 (2400 samples)\n",
      "Validation Loss: 0.4289 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.4365 (2400 samples)\n",
      "Validation Loss: 0.4250 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.4336 (2400 samples)\n",
      "Validation Loss: 0.4212 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.4309 (2400 samples)\n",
      "Validation Loss: 0.4175 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.4283 (2400 samples)\n",
      "Validation Loss: 0.4140 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.4259 (2400 samples)\n",
      "Validation Loss: 0.4105 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.4235 (2400 samples)\n",
      "Validation Loss: 0.4072 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.4212 (2400 samples)\n",
      "Validation Loss: 0.4039 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.4190 (2400 samples)\n",
      "Validation Loss: 0.4008 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.4169 (2400 samples)\n",
      "Validation Loss: 0.3978 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.4149 (2400 samples)\n",
      "Validation Loss: 0.3949 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.4129 (2400 samples)\n",
      "Validation Loss: 0.3920 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.4110 (2400 samples)\n",
      "Validation Loss: 0.3893 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.4092 (2400 samples)\n",
      "Validation Loss: 0.3867 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.4074 (2400 samples)\n",
      "Validation Loss: 0.3841 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.4057 (2400 samples)\n",
      "Validation Loss: 0.3816 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.4040 (2400 samples)\n",
      "Validation Loss: 0.3791 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.4023 (2400 samples)\n",
      "Validation Loss: 0.3766 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.4007 (2400 samples)\n",
      "Validation Loss: 0.3742 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.3990 (2400 samples)\n",
      "Validation Loss: 0.3719 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.3975 (2400 samples)\n",
      "Validation Loss: 0.3696 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.3959 (2400 samples)\n",
      "Validation Loss: 0.3675 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.3944 (2400 samples)\n",
      "Validation Loss: 0.3654 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.3930 (2400 samples)\n",
      "Validation Loss: 0.3632 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.3916 (2400 samples)\n",
      "Validation Loss: 0.3612 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.3902 (2400 samples)\n",
      "Validation Loss: 0.3591 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.3889 (2400 samples)\n",
      "Validation Loss: 0.3572 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.3876 (2400 samples)\n",
      "Validation Loss: 0.3552 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.3863 (2400 samples)\n",
      "Validation Loss: 0.3533 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.3850 (2400 samples)\n",
      "Validation Loss: 0.3514 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.3838 (2400 samples)\n",
      "Validation Loss: 0.3495 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.3825 (2400 samples)\n",
      "Validation Loss: 0.3476 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.3813 (2400 samples)\n",
      "Validation Loss: 0.3457 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.3801 (2400 samples)\n",
      "Validation Loss: 0.3439 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.3789 (2400 samples)\n",
      "Validation Loss: 0.3422 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.3777 (2400 samples)\n",
      "Validation Loss: 0.3404 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.3765 (2400 samples)\n",
      "Validation Loss: 0.3387 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.3753 (2400 samples)\n",
      "Validation Loss: 0.3370 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.3742 (2400 samples)\n",
      "Validation Loss: 0.3353 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.3731 (2400 samples)\n",
      "Validation Loss: 0.3337 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.3720 (2400 samples)\n",
      "Validation Loss: 0.3320 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.3708 (2400 samples)\n",
      "Validation Loss: 0.3304 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.3698 (2400 samples)\n",
      "Validation Loss: 0.3288 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.3686 (2400 samples)\n",
      "Validation Loss: 0.3272 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.3675 (2400 samples)\n",
      "Validation Loss: 0.3256 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.3664 (2400 samples)\n",
      "Validation Loss: 0.3241 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.3654 (2400 samples)\n",
      "Validation Loss: 0.3226 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.3644 (2400 samples)\n",
      "Validation Loss: 0.3211 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.3634 (2400 samples)\n",
      "Validation Loss: 0.3197 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.3624 (2400 samples)\n",
      "Validation Loss: 0.3183 (150 samples)\n",
      "Epoch: 1/100, Train Loss: 1.0652 (2400 samples)\n",
      "Validation Loss: 1.0033 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.9293 (2400 samples)\n",
      "Validation Loss: 0.8949 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.8130 (2400 samples)\n",
      "Validation Loss: 0.7841 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.7096 (2400 samples)\n",
      "Validation Loss: 0.6880 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.6282 (2400 samples)\n",
      "Validation Loss: 0.6103 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.5684 (2400 samples)\n",
      "Validation Loss: 0.5496 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.5251 (2400 samples)\n",
      "Validation Loss: 0.5029 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.4932 (2400 samples)\n",
      "Validation Loss: 0.4663 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.4691 (2400 samples)\n",
      "Validation Loss: 0.4364 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.4504 (2400 samples)\n",
      "Validation Loss: 0.4118 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.4353 (2400 samples)\n",
      "Validation Loss: 0.3911 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.4229 (2400 samples)\n",
      "Validation Loss: 0.3735 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.4125 (2400 samples)\n",
      "Validation Loss: 0.3586 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.4037 (2400 samples)\n",
      "Validation Loss: 0.3456 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.3961 (2400 samples)\n",
      "Validation Loss: 0.3341 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.3894 (2400 samples)\n",
      "Validation Loss: 0.3238 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.3834 (2400 samples)\n",
      "Validation Loss: 0.3146 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.3781 (2400 samples)\n",
      "Validation Loss: 0.3064 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.3732 (2400 samples)\n",
      "Validation Loss: 0.2992 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.3686 (2400 samples)\n",
      "Validation Loss: 0.2926 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.3644 (2400 samples)\n",
      "Validation Loss: 0.2864 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.3605 (2400 samples)\n",
      "Validation Loss: 0.2805 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.3569 (2400 samples)\n",
      "Validation Loss: 0.2751 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.3536 (2400 samples)\n",
      "Validation Loss: 0.2701 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.3504 (2400 samples)\n",
      "Validation Loss: 0.2653 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.3475 (2400 samples)\n",
      "Validation Loss: 0.2610 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.3447 (2400 samples)\n",
      "Validation Loss: 0.2569 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.3421 (2400 samples)\n",
      "Validation Loss: 0.2532 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.3396 (2400 samples)\n",
      "Validation Loss: 0.2496 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.3372 (2400 samples)\n",
      "Validation Loss: 0.2462 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.3350 (2400 samples)\n",
      "Validation Loss: 0.2429 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.3328 (2400 samples)\n",
      "Validation Loss: 0.2397 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.3307 (2400 samples)\n",
      "Validation Loss: 0.2366 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.3287 (2400 samples)\n",
      "Validation Loss: 0.2338 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.3267 (2400 samples)\n",
      "Validation Loss: 0.2310 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.3249 (2400 samples)\n",
      "Validation Loss: 0.2282 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.3231 (2400 samples)\n",
      "Validation Loss: 0.2256 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.3214 (2400 samples)\n",
      "Validation Loss: 0.2232 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.3196 (2400 samples)\n",
      "Validation Loss: 0.2208 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.3180 (2400 samples)\n",
      "Validation Loss: 0.2185 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.3164 (2400 samples)\n",
      "Validation Loss: 0.2162 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.3148 (2400 samples)\n",
      "Validation Loss: 0.2141 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.3133 (2400 samples)\n",
      "Validation Loss: 0.2121 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.3118 (2400 samples)\n",
      "Validation Loss: 0.2101 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.3103 (2400 samples)\n",
      "Validation Loss: 0.2082 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.3089 (2400 samples)\n",
      "Validation Loss: 0.2063 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.3075 (2400 samples)\n",
      "Validation Loss: 0.2044 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.3061 (2400 samples)\n",
      "Validation Loss: 0.2027 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.3048 (2400 samples)\n",
      "Validation Loss: 0.2010 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.3034 (2400 samples)\n",
      "Validation Loss: 0.1994 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.3021 (2400 samples)\n",
      "Validation Loss: 0.1977 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.3009 (2400 samples)\n",
      "Validation Loss: 0.1961 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.2996 (2400 samples)\n",
      "Validation Loss: 0.1944 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.2984 (2400 samples)\n",
      "Validation Loss: 0.1930 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.2972 (2400 samples)\n",
      "Validation Loss: 0.1915 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.2961 (2400 samples)\n",
      "Validation Loss: 0.1900 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.2949 (2400 samples)\n",
      "Validation Loss: 0.1886 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.2938 (2400 samples)\n",
      "Validation Loss: 0.1871 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.2927 (2400 samples)\n",
      "Validation Loss: 0.1857 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.2916 (2400 samples)\n",
      "Validation Loss: 0.1843 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.2905 (2400 samples)\n",
      "Validation Loss: 0.1830 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.2894 (2400 samples)\n",
      "Validation Loss: 0.1816 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.2884 (2400 samples)\n",
      "Validation Loss: 0.1804 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.2873 (2400 samples)\n",
      "Validation Loss: 0.1791 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.2863 (2400 samples)\n",
      "Validation Loss: 0.1778 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.2852 (2400 samples)\n",
      "Validation Loss: 0.1766 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.2842 (2400 samples)\n",
      "Validation Loss: 0.1754 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.2832 (2400 samples)\n",
      "Validation Loss: 0.1742 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.2822 (2400 samples)\n",
      "Validation Loss: 0.1731 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.2812 (2400 samples)\n",
      "Validation Loss: 0.1720 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.2803 (2400 samples)\n",
      "Validation Loss: 0.1708 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.2793 (2400 samples)\n",
      "Validation Loss: 0.1697 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.2783 (2400 samples)\n",
      "Validation Loss: 0.1687 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.2774 (2400 samples)\n",
      "Validation Loss: 0.1676 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.2764 (2400 samples)\n",
      "Validation Loss: 0.1666 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.2755 (2400 samples)\n",
      "Validation Loss: 0.1656 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.2746 (2400 samples)\n",
      "Validation Loss: 0.1646 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.2737 (2400 samples)\n",
      "Validation Loss: 0.1636 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.2727 (2400 samples)\n",
      "Validation Loss: 0.1627 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.2718 (2400 samples)\n",
      "Validation Loss: 0.1617 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.2709 (2400 samples)\n",
      "Validation Loss: 0.1609 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.2700 (2400 samples)\n",
      "Validation Loss: 0.1599 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.2691 (2400 samples)\n",
      "Validation Loss: 0.1590 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.2682 (2400 samples)\n",
      "Validation Loss: 0.1581 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.2674 (2400 samples)\n",
      "Validation Loss: 0.1572 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.2665 (2400 samples)\n",
      "Validation Loss: 0.1564 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.2656 (2400 samples)\n",
      "Validation Loss: 0.1555 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.2647 (2400 samples)\n",
      "Validation Loss: 0.1547 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.2639 (2400 samples)\n",
      "Validation Loss: 0.1539 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.2630 (2400 samples)\n",
      "Validation Loss: 0.1531 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.2621 (2400 samples)\n",
      "Validation Loss: 0.1523 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.2613 (2400 samples)\n",
      "Validation Loss: 0.1515 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.2604 (2400 samples)\n",
      "Validation Loss: 0.1507 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.2596 (2400 samples)\n",
      "Validation Loss: 0.1500 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.2588 (2400 samples)\n",
      "Validation Loss: 0.1492 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.2580 (2400 samples)\n",
      "Validation Loss: 0.1484 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.2571 (2400 samples)\n",
      "Validation Loss: 0.1477 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.2563 (2400 samples)\n",
      "Validation Loss: 0.1470 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.2555 (2400 samples)\n",
      "Validation Loss: 0.1463 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.2547 (2400 samples)\n",
      "Validation Loss: 0.1456 (150 samples)\n",
      "Epoch: 1/100, Train Loss: 1.0450 (2400 samples)\n",
      "Validation Loss: 1.0184 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 0.9253 (2400 samples)\n",
      "Validation Loss: 0.8450 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 0.7110 (2400 samples)\n",
      "Validation Loss: 0.5939 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 0.5441 (2400 samples)\n",
      "Validation Loss: 0.4566 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 0.4693 (2400 samples)\n",
      "Validation Loss: 0.3975 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 0.4348 (2400 samples)\n",
      "Validation Loss: 0.3664 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 0.4135 (2400 samples)\n",
      "Validation Loss: 0.3454 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 0.3975 (2400 samples)\n",
      "Validation Loss: 0.3298 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 0.3842 (2400 samples)\n",
      "Validation Loss: 0.3172 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 0.3730 (2400 samples)\n",
      "Validation Loss: 0.3044 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.3632 (2400 samples)\n",
      "Validation Loss: 0.2944 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.3550 (2400 samples)\n",
      "Validation Loss: 0.2859 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.3483 (2400 samples)\n",
      "Validation Loss: 0.2768 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.3424 (2400 samples)\n",
      "Validation Loss: 0.2693 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.3371 (2400 samples)\n",
      "Validation Loss: 0.2620 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.3324 (2400 samples)\n",
      "Validation Loss: 0.2552 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.3282 (2400 samples)\n",
      "Validation Loss: 0.2492 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.3242 (2400 samples)\n",
      "Validation Loss: 0.2437 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.3206 (2400 samples)\n",
      "Validation Loss: 0.2394 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.3171 (2400 samples)\n",
      "Validation Loss: 0.2350 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.3139 (2400 samples)\n",
      "Validation Loss: 0.2300 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.3107 (2400 samples)\n",
      "Validation Loss: 0.2257 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.3077 (2400 samples)\n",
      "Validation Loss: 0.2216 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.3049 (2400 samples)\n",
      "Validation Loss: 0.2176 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.3022 (2400 samples)\n",
      "Validation Loss: 0.2143 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.2996 (2400 samples)\n",
      "Validation Loss: 0.2109 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.2970 (2400 samples)\n",
      "Validation Loss: 0.2078 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.2946 (2400 samples)\n",
      "Validation Loss: 0.2045 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.2922 (2400 samples)\n",
      "Validation Loss: 0.2016 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.2899 (2400 samples)\n",
      "Validation Loss: 0.1989 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.2876 (2400 samples)\n",
      "Validation Loss: 0.1962 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.2854 (2400 samples)\n",
      "Validation Loss: 0.1936 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.2832 (2400 samples)\n",
      "Validation Loss: 0.1910 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.2811 (2400 samples)\n",
      "Validation Loss: 0.1884 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.2790 (2400 samples)\n",
      "Validation Loss: 0.1861 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.2769 (2400 samples)\n",
      "Validation Loss: 0.1836 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.2749 (2400 samples)\n",
      "Validation Loss: 0.1808 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.2729 (2400 samples)\n",
      "Validation Loss: 0.1784 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.2709 (2400 samples)\n",
      "Validation Loss: 0.1762 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.2689 (2400 samples)\n",
      "Validation Loss: 0.1739 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.2670 (2400 samples)\n",
      "Validation Loss: 0.1719 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.2652 (2400 samples)\n",
      "Validation Loss: 0.1702 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.2634 (2400 samples)\n",
      "Validation Loss: 0.1684 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.2615 (2400 samples)\n",
      "Validation Loss: 0.1665 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.2597 (2400 samples)\n",
      "Validation Loss: 0.1645 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.2579 (2400 samples)\n",
      "Validation Loss: 0.1629 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.2561 (2400 samples)\n",
      "Validation Loss: 0.1610 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.2543 (2400 samples)\n",
      "Validation Loss: 0.1590 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.2525 (2400 samples)\n",
      "Validation Loss: 0.1573 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.2508 (2400 samples)\n",
      "Validation Loss: 0.1557 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.2490 (2400 samples)\n",
      "Validation Loss: 0.1541 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.2473 (2400 samples)\n",
      "Validation Loss: 0.1525 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.2456 (2400 samples)\n",
      "Validation Loss: 0.1511 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.2438 (2400 samples)\n",
      "Validation Loss: 0.1492 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.2422 (2400 samples)\n",
      "Validation Loss: 0.1475 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.2404 (2400 samples)\n",
      "Validation Loss: 0.1462 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.2388 (2400 samples)\n",
      "Validation Loss: 0.1448 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.2372 (2400 samples)\n",
      "Validation Loss: 0.1433 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.2355 (2400 samples)\n",
      "Validation Loss: 0.1420 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.2339 (2400 samples)\n",
      "Validation Loss: 0.1406 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.2322 (2400 samples)\n",
      "Validation Loss: 0.1391 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.2305 (2400 samples)\n",
      "Validation Loss: 0.1377 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.2289 (2400 samples)\n",
      "Validation Loss: 0.1364 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.2272 (2400 samples)\n",
      "Validation Loss: 0.1350 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.2255 (2400 samples)\n",
      "Validation Loss: 0.1336 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.2238 (2400 samples)\n",
      "Validation Loss: 0.1322 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.2221 (2400 samples)\n",
      "Validation Loss: 0.1309 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.2205 (2400 samples)\n",
      "Validation Loss: 0.1298 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.2187 (2400 samples)\n",
      "Validation Loss: 0.1284 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.2171 (2400 samples)\n",
      "Validation Loss: 0.1273 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.2154 (2400 samples)\n",
      "Validation Loss: 0.1253 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.2138 (2400 samples)\n",
      "Validation Loss: 0.1241 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.2122 (2400 samples)\n",
      "Validation Loss: 0.1234 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.2106 (2400 samples)\n",
      "Validation Loss: 0.1219 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.2090 (2400 samples)\n",
      "Validation Loss: 0.1206 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.2074 (2400 samples)\n",
      "Validation Loss: 0.1193 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.2057 (2400 samples)\n",
      "Validation Loss: 0.1177 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.2042 (2400 samples)\n",
      "Validation Loss: 0.1171 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.2025 (2400 samples)\n",
      "Validation Loss: 0.1153 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.2009 (2400 samples)\n",
      "Validation Loss: 0.1140 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.1993 (2400 samples)\n",
      "Validation Loss: 0.1136 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.1978 (2400 samples)\n",
      "Validation Loss: 0.1118 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.1961 (2400 samples)\n",
      "Validation Loss: 0.1106 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.1946 (2400 samples)\n",
      "Validation Loss: 0.1094 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.1930 (2400 samples)\n",
      "Validation Loss: 0.1081 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.1914 (2400 samples)\n",
      "Validation Loss: 0.1071 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.1898 (2400 samples)\n",
      "Validation Loss: 0.1056 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.1884 (2400 samples)\n",
      "Validation Loss: 0.1046 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.1868 (2400 samples)\n",
      "Validation Loss: 0.1034 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.1853 (2400 samples)\n",
      "Validation Loss: 0.1019 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.1838 (2400 samples)\n",
      "Validation Loss: 0.1009 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.1823 (2400 samples)\n",
      "Validation Loss: 0.1000 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.1807 (2400 samples)\n",
      "Validation Loss: 0.0986 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.1792 (2400 samples)\n",
      "Validation Loss: 0.0977 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.1777 (2400 samples)\n",
      "Validation Loss: 0.0966 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.1762 (2400 samples)\n",
      "Validation Loss: 0.0956 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.1746 (2400 samples)\n",
      "Validation Loss: 0.0945 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.1732 (2400 samples)\n",
      "Validation Loss: 0.0935 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.1717 (2400 samples)\n",
      "Validation Loss: 0.0925 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.1702 (2400 samples)\n",
      "Validation Loss: 0.0915 (150 samples)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.0001, 0.001]\n",
    "dropouts = [0.3]\n",
    "hidden_layers = [[50],[50, 50]]\n",
    "\n",
    "# Store losses for each parameter combination\n",
    "losses = {}\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for dropout in dropouts:\n",
    "        for hidden_dim in hidden_layers:\n",
    "            # Reset weights for each combination\n",
    "            W = network_weights(vocab_size=len(word2id), embedding_dim=300, hidden_dim=hidden_dim, num_classes=3)\n",
    "            np.random.seed(0)\n",
    "            # Replace embedding matrix weights with w_glove\n",
    "            W[0] = w_glove\n",
    "\n",
    "            # Run SGD with current hyperparameters\n",
    "            W, loss_tr, dev_loss = SGD(X_tr=train_sequences, Y_tr=train_y,\n",
    "                                        W=W,\n",
    "                                        X_dev=dev_sequences,\n",
    "                                        Y_dev=dev_y,\n",
    "                                        lr=lr,\n",
    "                                        dropout=dropout,\n",
    "                                        freeze_emb=True,\n",
    "                                        tolerance=1e-20,\n",
    "                                        epochs=100)\n",
    "\n",
    "            # Store losses for this combination\n",
    "            losses[(lr, dropout, tuple(hidden_dim))] = (loss_tr, dev_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (5000, 300)\n",
      "Shape W1 (300, 50)\n",
      "Shape W2 (50, 50)\n",
      "Shape W3 (50, 3)\n",
      "Epoch: 1/100, Train Loss: 1.0777 (2400 samples)\n",
      "Validation Loss: 1.0867 (150 samples)\n",
      "Epoch: 2/100, Train Loss: 1.0689 (2400 samples)\n",
      "Validation Loss: 1.0768 (150 samples)\n",
      "Epoch: 3/100, Train Loss: 1.0612 (2400 samples)\n",
      "Validation Loss: 1.0679 (150 samples)\n",
      "Epoch: 4/100, Train Loss: 1.0540 (2400 samples)\n",
      "Validation Loss: 1.0598 (150 samples)\n",
      "Epoch: 5/100, Train Loss: 1.0468 (2400 samples)\n",
      "Validation Loss: 1.0520 (150 samples)\n",
      "Epoch: 6/100, Train Loss: 1.0394 (2400 samples)\n",
      "Validation Loss: 1.0446 (150 samples)\n",
      "Epoch: 7/100, Train Loss: 1.0317 (2400 samples)\n",
      "Validation Loss: 1.0367 (150 samples)\n",
      "Epoch: 8/100, Train Loss: 1.0233 (2400 samples)\n",
      "Validation Loss: 1.0282 (150 samples)\n",
      "Epoch: 9/100, Train Loss: 1.0143 (2400 samples)\n",
      "Validation Loss: 1.0191 (150 samples)\n",
      "Epoch: 10/100, Train Loss: 1.0042 (2400 samples)\n",
      "Validation Loss: 1.0091 (150 samples)\n",
      "Epoch: 11/100, Train Loss: 0.9930 (2400 samples)\n",
      "Validation Loss: 0.9979 (150 samples)\n",
      "Epoch: 12/100, Train Loss: 0.9804 (2400 samples)\n",
      "Validation Loss: 0.9851 (150 samples)\n",
      "Epoch: 13/100, Train Loss: 0.9664 (2400 samples)\n",
      "Validation Loss: 0.9707 (150 samples)\n",
      "Epoch: 14/100, Train Loss: 0.9508 (2400 samples)\n",
      "Validation Loss: 0.9546 (150 samples)\n",
      "Epoch: 15/100, Train Loss: 0.9337 (2400 samples)\n",
      "Validation Loss: 0.9370 (150 samples)\n",
      "Epoch: 16/100, Train Loss: 0.9151 (2400 samples)\n",
      "Validation Loss: 0.9183 (150 samples)\n",
      "Epoch: 17/100, Train Loss: 0.8951 (2400 samples)\n",
      "Validation Loss: 0.8983 (150 samples)\n",
      "Epoch: 18/100, Train Loss: 0.8737 (2400 samples)\n",
      "Validation Loss: 0.8773 (150 samples)\n",
      "Epoch: 19/100, Train Loss: 0.8511 (2400 samples)\n",
      "Validation Loss: 0.8544 (150 samples)\n",
      "Epoch: 20/100, Train Loss: 0.8275 (2400 samples)\n",
      "Validation Loss: 0.8305 (150 samples)\n",
      "Epoch: 21/100, Train Loss: 0.8032 (2400 samples)\n",
      "Validation Loss: 0.8057 (150 samples)\n",
      "Epoch: 22/100, Train Loss: 0.7787 (2400 samples)\n",
      "Validation Loss: 0.7808 (150 samples)\n",
      "Epoch: 23/100, Train Loss: 0.7542 (2400 samples)\n",
      "Validation Loss: 0.7564 (150 samples)\n",
      "Epoch: 24/100, Train Loss: 0.7303 (2400 samples)\n",
      "Validation Loss: 0.7327 (150 samples)\n",
      "Epoch: 25/100, Train Loss: 0.7070 (2400 samples)\n",
      "Validation Loss: 0.7096 (150 samples)\n",
      "Epoch: 26/100, Train Loss: 0.6845 (2400 samples)\n",
      "Validation Loss: 0.6875 (150 samples)\n",
      "Epoch: 27/100, Train Loss: 0.6631 (2400 samples)\n",
      "Validation Loss: 0.6662 (150 samples)\n",
      "Epoch: 28/100, Train Loss: 0.6428 (2400 samples)\n",
      "Validation Loss: 0.6459 (150 samples)\n",
      "Epoch: 29/100, Train Loss: 0.6237 (2400 samples)\n",
      "Validation Loss: 0.6269 (150 samples)\n",
      "Epoch: 30/100, Train Loss: 0.6058 (2400 samples)\n",
      "Validation Loss: 0.6091 (150 samples)\n",
      "Epoch: 31/100, Train Loss: 0.5893 (2400 samples)\n",
      "Validation Loss: 0.5925 (150 samples)\n",
      "Epoch: 32/100, Train Loss: 0.5741 (2400 samples)\n",
      "Validation Loss: 0.5771 (150 samples)\n",
      "Epoch: 33/100, Train Loss: 0.5602 (2400 samples)\n",
      "Validation Loss: 0.5630 (150 samples)\n",
      "Epoch: 34/100, Train Loss: 0.5474 (2400 samples)\n",
      "Validation Loss: 0.5501 (150 samples)\n",
      "Epoch: 35/100, Train Loss: 0.5357 (2400 samples)\n",
      "Validation Loss: 0.5380 (150 samples)\n",
      "Epoch: 36/100, Train Loss: 0.5250 (2400 samples)\n",
      "Validation Loss: 0.5269 (150 samples)\n",
      "Epoch: 37/100, Train Loss: 0.5153 (2400 samples)\n",
      "Validation Loss: 0.5166 (150 samples)\n",
      "Epoch: 38/100, Train Loss: 0.5063 (2400 samples)\n",
      "Validation Loss: 0.5071 (150 samples)\n",
      "Epoch: 39/100, Train Loss: 0.4982 (2400 samples)\n",
      "Validation Loss: 0.4983 (150 samples)\n",
      "Epoch: 40/100, Train Loss: 0.4908 (2400 samples)\n",
      "Validation Loss: 0.4901 (150 samples)\n",
      "Epoch: 41/100, Train Loss: 0.4840 (2400 samples)\n",
      "Validation Loss: 0.4826 (150 samples)\n",
      "Epoch: 42/100, Train Loss: 0.4778 (2400 samples)\n",
      "Validation Loss: 0.4755 (150 samples)\n",
      "Epoch: 43/100, Train Loss: 0.4721 (2400 samples)\n",
      "Validation Loss: 0.4689 (150 samples)\n",
      "Epoch: 44/100, Train Loss: 0.4669 (2400 samples)\n",
      "Validation Loss: 0.4628 (150 samples)\n",
      "Epoch: 45/100, Train Loss: 0.4621 (2400 samples)\n",
      "Validation Loss: 0.4572 (150 samples)\n",
      "Epoch: 46/100, Train Loss: 0.4576 (2400 samples)\n",
      "Validation Loss: 0.4518 (150 samples)\n",
      "Epoch: 47/100, Train Loss: 0.4535 (2400 samples)\n",
      "Validation Loss: 0.4467 (150 samples)\n",
      "Epoch: 48/100, Train Loss: 0.4497 (2400 samples)\n",
      "Validation Loss: 0.4420 (150 samples)\n",
      "Epoch: 49/100, Train Loss: 0.4461 (2400 samples)\n",
      "Validation Loss: 0.4374 (150 samples)\n",
      "Epoch: 50/100, Train Loss: 0.4427 (2400 samples)\n",
      "Validation Loss: 0.4331 (150 samples)\n",
      "Epoch: 51/100, Train Loss: 0.4395 (2400 samples)\n",
      "Validation Loss: 0.4289 (150 samples)\n",
      "Epoch: 52/100, Train Loss: 0.4365 (2400 samples)\n",
      "Validation Loss: 0.4250 (150 samples)\n",
      "Epoch: 53/100, Train Loss: 0.4336 (2400 samples)\n",
      "Validation Loss: 0.4212 (150 samples)\n",
      "Epoch: 54/100, Train Loss: 0.4309 (2400 samples)\n",
      "Validation Loss: 0.4175 (150 samples)\n",
      "Epoch: 55/100, Train Loss: 0.4283 (2400 samples)\n",
      "Validation Loss: 0.4140 (150 samples)\n",
      "Epoch: 56/100, Train Loss: 0.4259 (2400 samples)\n",
      "Validation Loss: 0.4105 (150 samples)\n",
      "Epoch: 57/100, Train Loss: 0.4235 (2400 samples)\n",
      "Validation Loss: 0.4072 (150 samples)\n",
      "Epoch: 58/100, Train Loss: 0.4212 (2400 samples)\n",
      "Validation Loss: 0.4039 (150 samples)\n",
      "Epoch: 59/100, Train Loss: 0.4190 (2400 samples)\n",
      "Validation Loss: 0.4008 (150 samples)\n",
      "Epoch: 60/100, Train Loss: 0.4169 (2400 samples)\n",
      "Validation Loss: 0.3978 (150 samples)\n",
      "Epoch: 61/100, Train Loss: 0.4149 (2400 samples)\n",
      "Validation Loss: 0.3949 (150 samples)\n",
      "Epoch: 62/100, Train Loss: 0.4129 (2400 samples)\n",
      "Validation Loss: 0.3920 (150 samples)\n",
      "Epoch: 63/100, Train Loss: 0.4110 (2400 samples)\n",
      "Validation Loss: 0.3893 (150 samples)\n",
      "Epoch: 64/100, Train Loss: 0.4092 (2400 samples)\n",
      "Validation Loss: 0.3867 (150 samples)\n",
      "Epoch: 65/100, Train Loss: 0.4074 (2400 samples)\n",
      "Validation Loss: 0.3841 (150 samples)\n",
      "Epoch: 66/100, Train Loss: 0.4057 (2400 samples)\n",
      "Validation Loss: 0.3816 (150 samples)\n",
      "Epoch: 67/100, Train Loss: 0.4040 (2400 samples)\n",
      "Validation Loss: 0.3791 (150 samples)\n",
      "Epoch: 68/100, Train Loss: 0.4023 (2400 samples)\n",
      "Validation Loss: 0.3766 (150 samples)\n",
      "Epoch: 69/100, Train Loss: 0.4007 (2400 samples)\n",
      "Validation Loss: 0.3742 (150 samples)\n",
      "Epoch: 70/100, Train Loss: 0.3990 (2400 samples)\n",
      "Validation Loss: 0.3719 (150 samples)\n",
      "Epoch: 71/100, Train Loss: 0.3975 (2400 samples)\n",
      "Validation Loss: 0.3696 (150 samples)\n",
      "Epoch: 72/100, Train Loss: 0.3959 (2400 samples)\n",
      "Validation Loss: 0.3675 (150 samples)\n",
      "Epoch: 73/100, Train Loss: 0.3944 (2400 samples)\n",
      "Validation Loss: 0.3654 (150 samples)\n",
      "Epoch: 74/100, Train Loss: 0.3930 (2400 samples)\n",
      "Validation Loss: 0.3632 (150 samples)\n",
      "Epoch: 75/100, Train Loss: 0.3916 (2400 samples)\n",
      "Validation Loss: 0.3612 (150 samples)\n",
      "Epoch: 76/100, Train Loss: 0.3902 (2400 samples)\n",
      "Validation Loss: 0.3591 (150 samples)\n",
      "Epoch: 77/100, Train Loss: 0.3889 (2400 samples)\n",
      "Validation Loss: 0.3572 (150 samples)\n",
      "Epoch: 78/100, Train Loss: 0.3876 (2400 samples)\n",
      "Validation Loss: 0.3552 (150 samples)\n",
      "Epoch: 79/100, Train Loss: 0.3863 (2400 samples)\n",
      "Validation Loss: 0.3533 (150 samples)\n",
      "Epoch: 80/100, Train Loss: 0.3850 (2400 samples)\n",
      "Validation Loss: 0.3514 (150 samples)\n",
      "Epoch: 81/100, Train Loss: 0.3838 (2400 samples)\n",
      "Validation Loss: 0.3495 (150 samples)\n",
      "Epoch: 82/100, Train Loss: 0.3825 (2400 samples)\n",
      "Validation Loss: 0.3476 (150 samples)\n",
      "Epoch: 83/100, Train Loss: 0.3813 (2400 samples)\n",
      "Validation Loss: 0.3457 (150 samples)\n",
      "Epoch: 84/100, Train Loss: 0.3801 (2400 samples)\n",
      "Validation Loss: 0.3439 (150 samples)\n",
      "Epoch: 85/100, Train Loss: 0.3789 (2400 samples)\n",
      "Validation Loss: 0.3422 (150 samples)\n",
      "Epoch: 86/100, Train Loss: 0.3777 (2400 samples)\n",
      "Validation Loss: 0.3404 (150 samples)\n",
      "Epoch: 87/100, Train Loss: 0.3765 (2400 samples)\n",
      "Validation Loss: 0.3387 (150 samples)\n",
      "Epoch: 88/100, Train Loss: 0.3753 (2400 samples)\n",
      "Validation Loss: 0.3370 (150 samples)\n",
      "Epoch: 89/100, Train Loss: 0.3742 (2400 samples)\n",
      "Validation Loss: 0.3353 (150 samples)\n",
      "Epoch: 90/100, Train Loss: 0.3731 (2400 samples)\n",
      "Validation Loss: 0.3337 (150 samples)\n",
      "Epoch: 91/100, Train Loss: 0.3720 (2400 samples)\n",
      "Validation Loss: 0.3320 (150 samples)\n",
      "Epoch: 92/100, Train Loss: 0.3708 (2400 samples)\n",
      "Validation Loss: 0.3304 (150 samples)\n",
      "Epoch: 93/100, Train Loss: 0.3698 (2400 samples)\n",
      "Validation Loss: 0.3288 (150 samples)\n",
      "Epoch: 94/100, Train Loss: 0.3686 (2400 samples)\n",
      "Validation Loss: 0.3272 (150 samples)\n",
      "Epoch: 95/100, Train Loss: 0.3675 (2400 samples)\n",
      "Validation Loss: 0.3256 (150 samples)\n",
      "Epoch: 96/100, Train Loss: 0.3664 (2400 samples)\n",
      "Validation Loss: 0.3241 (150 samples)\n",
      "Epoch: 97/100, Train Loss: 0.3654 (2400 samples)\n",
      "Validation Loss: 0.3226 (150 samples)\n",
      "Epoch: 98/100, Train Loss: 0.3644 (2400 samples)\n",
      "Validation Loss: 0.3211 (150 samples)\n",
      "Epoch: 99/100, Train Loss: 0.3634 (2400 samples)\n",
      "Validation Loss: 0.3197 (150 samples)\n",
      "Epoch: 100/100, Train Loss: 0.3624 (2400 samples)\n",
      "Validation Loss: 0.3183 (150 samples)\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(word2id), embedding_dim=300, hidden_dim=[50,50], num_classes=3)\n",
    "np.random.seed(0)\n",
    "\n",
    "#Replace embedding matrix weights with w_glove\n",
    "W[0] = w_glove\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "    \n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr=train_sequences, Y_tr=train_y,\n",
    "                            W=W,\n",
    "                                X_dev=dev_sequences,\n",
    "                            Y_dev=dev_y,\n",
    "                            lr=0.0001,\n",
    "                            dropout=0.3,\n",
    "                            freeze_emb=True,\n",
    "                            tolerance= 1e-20,\n",
    "                            epochs= 100,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Precision: 0.8553109781836526\n",
      "Recall: 0.8500000000000001\n",
      "F1-Score: 0.8483921852005252\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['pred']) \n",
    "            for x,y in zip(test_sequences,test_y)]\n",
    "preds_te_adjusted = [pred + 1 for pred in preds_te]\n",
    "\n",
    "print('Accuracy:', accuracy_score(test_y,preds_te_adjusted))\n",
    "print('Precision:', precision_score(test_y,preds_te_adjusted,average='macro'))\n",
    "print('Recall:', recall_score(test_y,preds_te_adjusted,average='macro'))\n",
    "print('F1-Score:', f1_score(test_y,preds_te_adjusted,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning process\n",
    "plt.plot(range(1, len(loss_tr) + 1), loss_tr, label='Training Loss')\n",
    "plt.plot(range(1, len(dev_loss) + 1), dev_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Process')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do deeper architectures increase performance?\n",
    "- The selection of network architecture, specifically the inclusion of hidden layers, is influenced by the complexity of the data being modeled. Adding hidden layers may result in a more complex model in simpler datasets where a linear transformation can adequately capture the relationships between input features and output labels. The model may become overfitted as a result, memorising details from the training set instead of identifying patterns that are applicable to a wider range of data.\n",
    "- The neural network may not have been trained on a dataset that was sufficiently complex in our experiment to require hidden layers. To learn the required mappings between input features and output labels, the output layer in conjunction with the pre-trained embedding layer may be enough. This makes sense as to why, in comparison to models with hidden layers, the model without hidden layers performed better on the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following hyperparameters were investigated:\n",
    "I chose the hyperparameters based on the following criteria:  \n",
    "1. Learning Rate: I chose learning rates of 0.0001 and 0.001 because they are common learning rates for training neural networks.\n",
    "2. Dropout Rate: I chose dropout rates of 0.3 because it is a common dropout rate for regularization in neural networks.\n",
    "3. Hidden Layers: I chose hidden layer sizes of [50] and [50, 50] because they are common sizes for hidden layers in neural networks.\n",
    "4. Embedding Dimension: I chose an embedding dimension of 300 because it is a common embedding dimension for word embeddings like GloVe.\n",
    "\n",
    "Tuning Process\n",
    "- Grid Definition: A grid encompassing all possible combinations of the chosen learning rates, dropout rates, and hidden layer configurations was established.\n",
    "\n",
    "- Model Training: For each combination in the grid, a separate instance of the neural network was trained using the SGD optimizer with frozen embedding weights. This ensured that the pre-trained word representations remained unaltered during training.\n",
    "\n",
    "- Performance Evaluation: The training loss and validation loss were monitored throughout training for each model. The validation loss serves as a crucial metric for assessing model generalization to unseen data.\n",
    "\n",
    "- By employing this hyperparameter tuning strategy, we were able to identify a configuration that resulted in a well-performing model on the validation set, achieving an accuracy of 85.33%, precision of 86.24%, recall of 85.33%, and F1-score of 84.96%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  |84.24%   |84.11%   | 84.04%  | 84.11%  |\n",
    "| Average Embedding (Pre-trained)  | 87.7%  |87.6%   |  87.6% |  87.6% |\n",
    "| Average Embedding (Pre-trained) + X hidden layers    |85.53%| 85%|84.83%|85%|\n",
    "\n",
    "\n",
    "Please discuss why your best performing model is better than the rest and provide a bried error analaysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is \"Average Embedding (Pre-trained)\" with an accuracy of 87.6%. This model outperforms the other two models in terms of precision, recall, F1-score, and accuracy.\n",
    "\n",
    "- The reason why the \"Average Embedding (Pre-trained)\" model performs better can be attributed to the use of pre-trained embeddings. Pre-trained embeddings are word representations that have been trained on a large corpus of text data, capturing semantic relationships between words. By leveraging pre-trained embeddings, the model can benefit from the knowledge encoded in the embeddings, which can improve its ability to understand the meaning of words and make more accurate predictions.\n",
    "\n",
    "- In contrast, the \"Average Embedding\" model does not use pre-trained embeddings, which means it has to learn word representations from scratch during the training process. This can be more challenging, especially if the training data is limited, as the model needs to learn meaningful representations of words solely from the available data.\n",
    "\n",
    "- Additionally, the \"Average Embedding (Pre-trained) + X hidden layers\" model introduces additional hidden layers to the architecture. These hidden layers can provide the model with more capacity to learn complex patterns and relationships in the data. However, in this case, the model's performance is slightly worse compared to the \"Average Embedding (Pre-trained)\" model. This could be due to overfitting, where the model becomes too specialized to the training data and performs poorly on unseen data.\n",
    "\n",
    "In terms of error analysis, it would be helpful to examine the specific errors made by the models. This could involve analyzing misclassified examples and identifying patterns or common characteristics that contribute to the misclassifications. By understanding the types of errors made by the models, we can gain insights into their limitations and potential areas for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
